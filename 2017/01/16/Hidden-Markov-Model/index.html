<!DOCTYPE HTML>
<html>

<head>
  <meta name="google-site-verification" content="6-Grm86DHfS67XSHMuvfO8cIcQcOWPpjGvOBMxbA3Fo" />
  <meta charset="utf-8">
  
  <title>Hidden Markov Model | WatsonYang&#39;s Blog</title>
  
  <meta name="author" content="Watson Yang">
  
  <meta name="description" content="My secret private space.">
  
  
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

  
  <meta property="og:title" content="Hidden Markov Model" />
  <meta property="og:site_name" content="WatsonYang&#39;s Blog" />

  
  <meta property="og:image" content="" />
  

  <link href="/favicon.png" rel="icon">
  <link rel="alternate" href="/atom.xml"
    title="WatsonYang&#39;s Blog" type="application/atom+xml">
  <link rel="stylesheet" href="/css/style.css" media="screen" type="text/css">
  <!--[if lt IE 9]><script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script><![endif]-->
  


  <a href="https://github.com/watsonyanghx" target="_blank">
    <img style="position: absolute; top: 0; left: 0; border: 0;"
      src="https://s3.amazonaws.com/github/ribbons/forkme_left_gray_6d6d6d.png" alt="Fork me on GitHub">
  </a>
  <script src="https://use.fontawesome.com/4eb261e456.js"></script>

  <script>
    MathJax = {
      tex: {
        inlineMath: [
          ['$', '$'],
          ['\\(', '\\)']
        ]
      },
      svg: {
        fontCache: 'global'
      }
    };
  </script>
  <script type="text/javascript" id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js">
  </script>
<meta name="generator" content="Hexo 4.2.0"></head>

<body>
  <header id="header" class="inner"><div class="alignleft">
  <h1><a href="/">WatsonYang&#39;s Blog</a></h1>
  <h2><a href="/">Enrich yourself.</a></h2>
</div>
<nav id="main-nav" class="alignright">
  <ul>
    
      <li><a href="/">Home</a></li>
    
      <li><a href="/archives">Archives</a></li>
    
      <li><a href="/about">About</a></li>
    

    <li> <a href="/atom.xml">RSS</a> </li>
  </ul>
  <div class="clearfix"></div>
</nav>
<div class="clearfix"></div>
</header>
  <div id="content" class="inner">
    <div id="main-col" class="alignleft"><div id="wrapper"><article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2017-01-16T06:39:20.000Z"><a href="/2017/01/16/Hidden-Markov-Model/">2017-01-16</a></time>
      
      
  
    <h1 class="title">Hidden Markov Model</h1>
  

    </header>
    <div class="entry">
      
        <p>这篇只是个人为了加深对<code>Markov Model</code>的理解而做的学习笔记，基本是以易于理解的目的进行组织，所以可能有很多地方不太严禁。如有错误，还请不吝指出。</p>
<h1 id="Markov-Process"><a href="#Markov-Process" class="headerlink" title="Markov Process"></a>Markov Process</h1><ul>
<li><p>一个<code>Markov Process</code>是状态间的转移仅依赖于前<code>n</code>个状态的过程。这个过程被称之为<code>n</code>阶马尔科夫模型，其中<code>n</code>是影响下一个状态选择的（前）<code>n</code>个状态。</p>
<p>最简单的<code>Markov Process</code>是一阶模型，它的状态选择仅与前一个状态有关。</p>
</li>
<li><p><code>Markov Process</code>有两个假设：</p>
<ul>
<li><p>系统在时刻t的状态只与时刻<code>t-1</code>处的状态相关（也称为无后效性）。</p>
<p>用如下公式表示：<code>P(qt=Sj|qt-1=Si,qt-2=Sk,…) =  P(qt=Sj|qt-1=Si)</code>，<code>t</code>为大于<code>1</code>的任意数值，<code>Sk</code>为任意状态</p>
<a id="more"></a>
</li>
<li><p>状态转移概率与时间无关（也称为齐次性或时齐性）。</p>
<p>用如下公式表示：<code>P(qt=Sj|qt-1=Si) =  (qk=Sj|qk-1=Si)</code>，<code>k</code>为任意时刻。</p>
<p>当然，上面的假设跟实际情况都不相符，但是这样做便于模型的分析和生成。</p>
</li>
</ul>
</li>
</ul>
<h1 id="Hidden-Markov-Model"><a href="#Hidden-Markov-Model" class="headerlink" title="Hidden Markov Model"></a>Hidden Markov Model</h1><h2 id="Basics"><a href="#Basics" class="headerlink" title="Basics"></a>Basics</h2><ul>
<li><p><code>HMM</code>有 5 个元素：</p>
<ol>
<li><p>隐藏状态集合，一个系统的（真实）状态，可以由一个<code>Markov Process</code>进行描述。</p>
</li>
<li><p>观察状态集合：在这个过程中‘可视’的状态。</p>
</li>
<li><p>状态转移矩阵（<code>transition matrix</code>）：隐藏状态间的转移概率。</p>
<center>
<img src="https://www.tuchuang001.com/images/2017/05/15/Firefox_Screenshot_2017-05-15T04-12-58.839Z.png" width=40% height=40% alt="图 1 transition matrix" border="0" />
</center>
</li>
<li><p>混淆矩阵（<code>confusion matrix</code>）：隐藏状态到输出状态的概率。</p>
<center>
<img src="https://www.tuchuang001.com/images/2017/05/15/Firefox_Screenshot_2017-05-15T04-15-53.060Z.png" width=60% height=60% alt="图 2 confusion matrix" border="0" />
</center>
</li>
<li><p>初始概率分布：隐藏状态的初始概率分布。</p>
</li>
</ol>
<p>  <strong>注意</strong>：其中，后 3 个经常用<code>X = (pi, A, B)</code>表示，称为<code>HMM</code>的参数。给定一个<code>X</code>，也就决定了一个<code>HMM</code>。</p>
</li>
<li><p><code>HMM</code>解决 3 种不同类型的问题（前两个是模式识别的问题）：</p>
<ul>
<li><p>评估：给定<code>HMM</code>（也就是<code>X = (pi, A, B)</code>已知），求产生某个<code>观察序列</code>的概率（前向算法：对所有能够产生<code>观察序列</code>的概率取<code>sum</code>，<code>DP</code>）。</p>
<p>“评估”有时也指代<strong>筛选</strong><code>HMM</code>。也就是在给定的<strong>一系列</strong>的<code>HMM</code>中，求出<strong>最有可能</strong>产生这个<code>观察序列</code>的<code>HMM</code>。</p>
</li>
<li><p>解码：给定<code>HMM</code>（也就是<code>X = (pi, A, B)</code>已知），搜索<strong>最有可能</strong>生成一个<code>观察序列</code>的<code>隐藏状态序列</code>（维特比算法：对所有能够产生<code>观察序列</code>的概率取<code>max</code>，<code>DP</code>）。</p>
</li>
<li><p>学习：<code>X = (pi, A, B)</code><strong>未知</strong>，此时要做的是：对于给定的<code>观察序列</code>，调整<code>HMM</code>的参数（<code>X = (pi, A, B)</code>），使<code>观察序列</code>出现的概率最大（前向-后向算法）。</p>
<p><strong>注意</strong>：“评估”是从一些<strong>已知</strong>的<code>HMM</code>中，筛选出一个求出<strong>最有可能</strong>产生这个<code>观察序列</code>的<code>HMM</code>。但对于“学习”这个过程，根本就没有现成的<code>HMM</code>，而是要以 from scratch 的方式求解一个<code>HMM</code>（也就是求解出一个<code>X = (pi, A, B)</code>） 。</p>
<p>关于这 3 个问题的 concept/definition 可以参考<a href="http://www.52nlp.cn/hmm-learn-best-practices-four-hidden-markov-models" target="_blank" rel="noopener">这个</a>，更为详细。</p>
</li>
</ul>
</li>
</ul>
<h2 id="问题-1"><a href="#问题-1" class="headerlink" title="问题 1"></a>问题 1</h2><p>下面的算法主要用于：计算<code>观察序列</code>的概率（Finding the probability of an observed sequence），也就是问题 1（评估）。</p>
<h3 id="Brute-Force"><a href="#Brute-Force" class="headerlink" title="Brute Force"></a>Brute Force</h3><p>解决问题 1 的<strong>最直观的办法</strong>：求出每一种可能产生这个<code>观察序列</code>的<code>隐藏状态序列</code>，然后计算对应情况下的概率，最后把它们加起来就是最终的。</p>
<p>但是这种求解概率的方式的时间复杂度是非常高的，如果<code>观察序列</code>的长度是<code>n</code>，<code>隐藏状态</code>的个数是<code>m</code>（这里考虑最简单的情况：任意时刻<code>t</code>，每个<code>隐藏状态</code>都有可能发生），所有可能的情况就有<code>m^n</code>种。</p>
<p>比如下面的图：</p>
  <center>
  <img src="https://www.tuchuang001.com/images/2017/05/15/Firefox_Screenshot_2017-05-15T08-37-00.784Z.png" width=60% height=60% alt="图 3 Observation Sequence" border="0" />
  </center>

<p>按照<code>Brute Force</code>算法，需要计算出所有的27种情况：</p>
  <center>
  <img src="https://www.tuchuang001.com/images/2017/05/15/Firefox_Screenshot_2017-05-15T08-40-58.730Z.png" width=80% height=80% alt="图 4 Total Probability" border="0" />
  </center>

<p>所以，自然就会想有没有更为高效的算法？有，就是<code>前向算法（Forward Algorithm）</code>。</p>
<h3 id="前向算法（Forward-Algorithm）"><a href="#前向算法（Forward-Algorithm）" class="headerlink" title="前向算法（Forward Algorithm）"></a>前向算法（Forward Algorithm）</h3><p>正如上面所说，由于<code>Brute Force</code>算法时间复杂度太大，无法在实际生产中使用，所以有了<code>前向算法</code>。</p>
<p><code>前向算法</code>的<strong>实现方法</strong>是<code>动态规划（Dynamic Programming）</code>（这里有道<a href="https://hihocoder.com/problemset/problem/1506" target="_blank" rel="noopener">OJ题目</a>就是利用<code>DP</code>求解概率，当时没写出来 -_- ），使用<code>DP</code>可以暂存从开始到中间某个状态的概率(局部概率）。</p>
<p>本质上<code>前向算法</code>就是对<code>Brute Force</code>算法的一种<font color='red'>变换</font>，怎么变换？非常简单：数学公式的<strong>结合律</strong>：<code>a * c + b * c = (a + b) * c</code>。利用这个就可以把<code>Brute Force</code>转化为<code>前向算法</code>。记住这个，再去看<code>前向算法</code>，就会发现<code>前向算法</code>非常容易理解，详细看下面。（对于问题2的维特比算法也是利用这种变换，只是把求和换成了取max）</p>
<p>-&gt;-&gt;-&gt;-&gt;-&gt;-&gt;-&gt;-&gt;-&gt;-&gt;-&gt;-&gt;-&gt;-&gt;-&gt;-&gt;</p>
<p><code>前向算法</code>示意图如下图，为了方便，定义（i，j）指代图中的第 i 行第 j 列（下标都从1开始计数）所代表的<code>隐藏状态</code>：</p>
  <center>
  <img src="https://www.tuchuang001.com/images/2017/05/15/Firefox_Screenshot_2017-05-15T08-33-42.338Z.png" width=40% height=40% alt="图 5 Forward Algorithm" border="0" />
  </center>

<p><font color='red'>重点</font>：<strong>结合律</strong>。一直说<strong>结合律</strong>，那到底是怎么用到<strong>结合律</strong>的？和上面的图 3 对比着看。</p>
<p>因为图中（2,2）的<code>隐藏状态</code>可以由前一时刻的任何一种<code>隐藏状态</code>转换而来， 按照<code>Brute Force</code>有：</p>
<p>P(O=damp|t=2,S=Cloudy) = P(O=dry|t=1,S=Sunny) × P((1,1)–&gt;(2,2)) × P((2,2)–&gt;damp) + P(O=dry|t=1,S=Cloudy) × P((1,2)–&gt;(2,2)) × P((2,2)–&gt;damp) + P(O=dry|t=1,S=Rainy) × P((1,3)–&gt;(2,2)) × P((2,2)–&gt;damp) 。</p>
<p>那如果现在要计算：在时刻 t=2（第2列）时的<code>隐藏状态</code>是 Cloudy，时刻 t=3（最后一列）时的<code>隐藏状态</code>是 Sunny 的前提下，<code>观察序列</code>是<code>(dry,damp,soggy)</code>的概率，怎么算？（有点绕，注意这里<strong>固定</strong>住了 t=2（第2列）与 t=3（最后一列）时的<code>隐藏状态</code>）</p>
<p>按照<code>Brute Force</code>：</p>
<p>P(O=soggy|t=3,S=Sunny &amp;&amp; t=2,S=Cloudy) = P(O=dry|t=1,S=Sunny) × P((1,1)–&gt;(2,2)) × P((2,2)–&gt;damp) × P((2,2)–&gt;(3,1)) × P((3,1)–&gt;soggy) + P(O=dry|t=1,S=Cloudy) × P((1,2)–&gt;(2,2)) × P((2,2)–&gt;damp) × P((2,2)–&gt;(3,1)) × P((3,1)–&gt;soggy) + P(O=dry|t=1,S=Rainy) × P((1,3)–&gt;(2,2)) × P((2,2)–&gt;damp) × P((2,2)–&gt;(3,1)) × P((3,1)–&gt;soggy)  = P(O=damp|t=2,S=Cloudy) × P((2,2)–&gt;(3,1)) × P((3,1)–&gt;soggy)。</p>
<p>看到了吧，<strong>结合律</strong>就是这样被用到的。这里提取出了共有的 P((2,2)–&gt;(3,1)) × P((3,1)–&gt;soggy)，同时也证明了<code>前向算法</code>就是对<code>Brute Force</code>的一种变换，自然也证明了<code>前向算法</code>的正确性。</p>
<p>上面就是<code>前向算法</code>的整个过程，对于任意<code>观察序列</code>的出现概率都可以用这种方式计算。</p>
<h2 id="问题-2"><a href="#问题-2" class="headerlink" title="问题 2"></a>问题 2</h2><p>下面的算法主要用于：根据一个给定的HMM模型，根据<code>观察状态序列</code>找到产生这一序列的潜在的<code>隐含状态序列</code>，也就是问题 2（解码）。</p>
<h3 id="Brute-Force-1"><a href="#Brute-Force-1" class="headerlink" title="Brute Force"></a>Brute Force</h3><p>解决问题 2 的<strong>最直观的办法</strong>：列出所有可能的情况，从所有可能的情况中选取最有可能产生这个<code>观察状态序列</code>的<code>隐藏状态序列</code>（问题 1 是把它们加起来）。</p>
<p>对于图中，找出能够以<strong>最大概率</strong>产生这个<code>观察状态序列</code>的<code>隐藏状态序列</code>，需要从下面的概率中取最大的一个。</p>
  <center>
  <img src="https://www.tuchuang001.com/images/2017/05/18/Firefox_Screenshot_2017-05-18T14-14-25.524Z.png" width=80% height=80% alt="图 6 Max" border="0" />
  </center>

<p>和问题 1 的 Brute Force 一样，时间复杂度太大，无法应用到实际情况。不过，这个也是有更为高效的算法的，就是：维特比算法（Viterbi Algorithm）。</p>
<h3 id="维特比算法（Viterbi-Algorithm）"><a href="#维特比算法（Viterbi-Algorithm）" class="headerlink" title="维特比算法（Viterbi Algorithm）"></a>维特比算法（Viterbi Algorithm）</h3><h4 id="部分概率和部分最优路径"><a href="#部分概率和部分最优路径" class="headerlink" title="部分概率和部分最优路径"></a><strong>部分概率和部分最优路径</strong></h4><p>对于网格中的每一个中间及终止状态，都有一个到达该状态的最可能路径。举例来说，在t=3时刻的3个状态中的每一个都有一个到达此状态的最可能路径，或许是这样的：</p>
  <center>
  <img src="https://www.tuchuang001.com/images/2017/05/18/Firefox_Screenshot_2017-05-18T14-22-44.808Z.png" width=60% height=60% alt="图 7 Viterbi Algorithm" border="0" />
  </center>

<p>这些路径被称为<code>局部最佳路径(partial best paths)</code>。其中每个<code>局部最佳路径</code>都有一个相关联的概率，即<code>局部概率</code>（注意<code>局部概率</code>就说明对应的路径是最佳的）。不过这里<strong>不是</strong><code>前向算法</code>中对之前的概率取和，而是对之前的概率取最大值。</p>
<p>比如上图，最终要选择其中能够以<strong>最大概率</strong>产生<code>观察状态序列</code>的<code>隐藏状态序列</code>。</p>
<p>在整个计算过程中，同样要使用<code>动态规划</code>算法：计算中间状态的<code>局部概率</code>，最后就能得到正确的<code>隐藏状态序列</code>。</p>
<h4 id="计算过程"><a href="#计算过程" class="headerlink" title="计算过程"></a><strong>计算过程</strong></h4><p>对应的分为t=1时刻和t&gt;1时刻的局部概率的计算，看这个博客：<a href="http://www.52nlp.cn/hmm-learn-best-practices-six-viterbi-algorithm-2" target="_blank" rel="noopener">HMM学习最佳范例六：维特比算法2</a></p>
<h4 id="优点"><a href="#优点" class="headerlink" title="优点"></a><strong>优点</strong></h4><p>说的是计算的是总体的概率，考虑到了全局的信息，这是当然的，因为计算据局部概率时算的就是考虑到了之前观察状态序列最大化概率（联合概率）。</p>
<p>教程（<a href="http://www.52nlp.cn/hmm-learn-best-practices-six-viterbi-algorithm-3" target="_blank" rel="noopener">看这里</a>）中说的另外一个不好的计算方式，使用的是贪心算法：只考虑某个特定的时刻，从它前一个时刻的状态转换到当前状态的概率 × 状态输出概率。这明显是不对的，当前这一步最好不能使全局最好（没有贪心选择性质）。</p>
<h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a><strong>总结</strong></h4><p>关于维特比算法，需要着重强调的一点是它不是简单的对于某个给定的时间点选择最可能的隐藏状态，而是基于全局序列做决策——因此，如果在观察序列中有一个“非寻常”的事件发生，对于维特比算法的结果也影响不大。</p>
<p>这在语音处理中是特别有价值的，譬如当某个单词发音的一个中间音素出现失真或丢失的情况时，该单词也可以被识别出来。</p>
<h2 id="问题-3"><a href="#问题-3" class="headerlink" title="问题 3"></a>问题 3</h2><p>这里需要用到<code>前向-后向算法(Forward-backward algorithm)</code>，由于目前还没有需要用到这个算法，所以本人并没有去学习这个算法，等到以后再来补充。</p>
<p>想自己学习的可以看这里：<a href="http://www.52nlp.cn/hmm-learn-best-practices-seven-forward-backward-algorithm-1" target="_blank" rel="noopener">HMM学习最佳范例七：前向-后向算法1 | 我爱自然语言处理</a></p>
<p>其实，前两部分的概率计算都是算的联合概率：转移概率 × 状态输出概率，所以这也说明了HMM是生成模型（Generative Model），CRF是判别模型（determistic model）。</p>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ol>
<li><a href="http://blog.csdn.net/stdcoutzyx/article/details/8522078" target="_blank" rel="noopener">隐马尔科夫模型（HMM)及其扩展</a></li>
<li><a href="http://www.52nlp.cn/category/hidden-markov-model" target="_blank" rel="noopener">隐马尔科夫模型 | 我爱自然语言处理</a></li>
</ol>

      
    </div>
    <footer>
      
        
  
  <div class="categories">
    <a href="/categories/Machine-Learning/">Machine Learning</a>
  </div>

        
  
  <div class="tags">
    <a href="/tags/Machine-Learning/">Machine Learning</a>
  </div>

        
  <div class="addthis addthis_toolbox addthis_default_style">
    
      <a class="addthis_button_facebook_like" fb:like:layout="button_count"></a>
    
    
      <a class="addthis_button_tweet"></a>
    
    
      <a class="addthis_button_google_plusone" g:plusone:size="medium"></a>
    
    
      <a class="addthis_button_pinterest_pinit" pi:pinit:layout="horizontal"></a>
    
    <a class="addthis_counter addthis_pill_style"></a>
  </div>
  <script type="text/javascript" src="//s7.addthis.com/js/300/addthis_widget.js"></script>

      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>


<section id="comment">
  <h1 class="title">Comments</h1>

  
  <div id="disqus_thread">
    <noscript>Please enable JavaScript to view the <a href="//disqus.com/?ref_noscript" target="_blank" rel="noopener">comments powered by Disqus.</a></noscript>
  </div>
  
</section>

</div></div>
    <aside id="sidebar" class="alignright">
  <div class="search">
  <form action="//google.com/search" method="get" accept-charset="utf-8">
    <input type="search" name="q" results="0" placeholder="Search">
    <input type="hidden" name="q" value="site:watsonyanghx.github.io">
  </form>
</div>

  
<div class="widget tag">
  <h3 class="title">Categories</h3>
  <ul class="entry">
  
    <li><a href="/categories/Algorithm/">Algorithm</a><small>2</small></li>
  
    <li><a href="/categories/Computer-Vision/">Computer Vision</a><small>3</small></li>
  
    <li><a href="/categories/Deep-Learning/">Deep Learning</a><small>3</small></li>
  
    <li><a href="/categories/Interview/">Interview</a><small>1</small></li>
  
    <li><a href="/categories/Machine-Learning/">Machine Learning</a><small>8</small></li>
  
    <li><a href="/categories/Deep-Learning/Machine-Learning/">Machine Learning</a><small>1</small></li>
  
    <li><a href="/categories/NLP/">NLP</a><small>1</small></li>
  
    <li><a href="/categories/OJ/">OJ</a><small>48</small></li>
  
    <li><a href="/categories/Paper/">Paper</a><small>5</small></li>
  
    <li><a href="/categories/Reinforcement-Learning/">Reinforcement Learning</a><small>1</small></li>
  
    <li><a href="/categories/Technique-Summaries/">Technique Summaries</a><small>2</small></li>
  
    <li><a href="/categories/Tools/">Tools</a><small>1</small></li>
  
    <li><a href="/categories/WhatEver/">WhatEver</a><small>1</small></li>
  
  </ul>
</div>


  
<div class="widget tagcloud">
  <h3 class="title">Tag Cloud</h3>
  <div class="entry">
    <a href="/tags/Algorithm/" style="font-size: 18.18px;">Algorithm</a> <a href="/tags/BFS/" style="font-size: 11.82px;">BFS</a> <a href="/tags/Backtracking/" style="font-size: 12.73px;">Backtracking</a> <a href="/tags/Binary-Tree/" style="font-size: 14.55px;">Binary Tree</a> <a href="/tags/C/" style="font-size: 10px;">C++</a> <a href="/tags/CNN/" style="font-size: 15.45px;">CNN</a> <a href="/tags/CS231n/" style="font-size: 11.82px;">CS231n</a> <a href="/tags/Combinatorial-number/" style="font-size: 10.91px;">Combinatorial number</a> <a href="/tags/DFS/" style="font-size: 14.55px;">DFS</a> <a href="/tags/DP/" style="font-size: 19.09px;">DP</a> <a href="/tags/Data-structure/" style="font-size: 10.91px;">Data structure</a> <a href="/tags/DeconvNet/" style="font-size: 10px;">DeconvNet</a> <a href="/tags/Deep-Learning/" style="font-size: 17.27px;">Deep Learning</a> <a href="/tags/Disjoint-Set/" style="font-size: 11.82px;">Disjoint Set</a> <a href="/tags/Divide-and-Conquer/" style="font-size: 13.64px;">Divide and Conquer</a> <a href="/tags/GAN/" style="font-size: 10.91px;">GAN</a> <a href="/tags/Graph/" style="font-size: 13.64px;">Graph</a> <a href="/tags/Greedy/" style="font-size: 11.82px;">Greedy</a> <a href="/tags/Hexo/" style="font-size: 10px;">Hexo</a> <a href="/tags/HihoCoder/" style="font-size: 16.36px;">HihoCoder</a> <a href="/tags/IPv6/" style="font-size: 10px;">IPv6</a> <a href="/tags/LintCode/" style="font-size: 20px;">LintCode</a> <a href="/tags/Machine-Learning/" style="font-size: 10.91px;">Machine Learning</a> <a href="/tags/Matrix/" style="font-size: 10px;">Matrix</a> <a href="/tags/NLP/" style="font-size: 10px;">NLP</a> <a href="/tags/Normalization/" style="font-size: 10px;">Normalization</a> <a href="/tags/Notes/" style="font-size: 10px;">Notes</a> <a href="/tags/Numpy/" style="font-size: 10px;">Numpy</a> <a href="/tags/POJ/" style="font-size: 10.91px;">POJ</a> <a href="/tags/Permutation/" style="font-size: 10px;">Permutation</a> <a href="/tags/Python/" style="font-size: 10px;">Python</a> <a href="/tags/Queue/" style="font-size: 10.91px;">Queue</a> <a href="/tags/RL/" style="font-size: 10px;">RL</a> <a href="/tags/Recursion/" style="font-size: 12.73px;">Recursion</a> <a href="/tags/ResNet/" style="font-size: 10px;">ResNet</a> <a href="/tags/Shadowsocks/" style="font-size: 10px;">Shadowsocks</a> <a href="/tags/String/" style="font-size: 11.82px;">String</a> <a href="/tags/VPS/" style="font-size: 10px;">VPS</a> <a href="/tags/WhatEver/" style="font-size: 10px;">WhatEver</a>
  </div>
</div>

</aside>
    <div class="clearfix"></div>
  </div>
  <footer id="footer" class="inner"><div class="alignleft">
  
  &copy; 2020 Watson Yang
  
</div>
<div class="clearfix"></div></footer>
  <script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>
<script src="/js/jquery.imagesloaded.min.js"></script>
<script src="/js/gallery.js"></script>


<script type="text/javascript">
var disqus_shortname = 'light-disqus';

(function(){
  var dsq = document.createElement('script');
  dsq.type = 'text/javascript';
  dsq.async = true;
  dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
  (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
}());
</script>



<link rel="stylesheet" href="/fancybox/jquery.fancybox.css" media="screen" type="text/css">
<script src="/fancybox/jquery.fancybox.pack.js"></script>
<script type="text/javascript">
(function($){
  $('.fancybox').fancybox();
})(jQuery);
</script>


<div id="totop" style="position:fixed;bottom:35px;right:20px;cursor: pointer;">
  <a title="返回顶部"><i class="fa fa-angle-double-up fa-3x" aria-hidden="true"></i></a>
</div>
<script src="/js/totop.js"></script>

</body>
</html>
