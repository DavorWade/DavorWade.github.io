<!DOCTYPE HTML>
<html>

<head>
  <meta name="google-site-verification" content="6-Grm86DHfS67XSHMuvfO8cIcQcOWPpjGvOBMxbA3Fo" />
  <meta charset="utf-8">
  
  <title>CS231n Notes 2 | WatsonYang&#39;s Blog</title>
  
  <meta name="author" content="Watson Yang">
  
  <meta name="description" content="My secret private space.">
  
  
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

  
  <meta property="og:title" content="CS231n Notes 2" />
  <meta property="og:site_name" content="WatsonYang&#39;s Blog" />

  
  <meta property="og:image" content="" />
  

  <link href="/favicon.png" rel="icon">
  <link rel="alternate" href="/atom.xml"
    title="WatsonYang&#39;s Blog" type="application/atom+xml">
  <link rel="stylesheet" href="/css/style.css" media="screen" type="text/css">
  <!--[if lt IE 9]><script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script><![endif]-->
  


  <a href="https://github.com/watsonyanghx" target="_blank">
    <img style="position: absolute; top: 0; left: 0; border: 0;"
      src="https://s3.amazonaws.com/github/ribbons/forkme_left_gray_6d6d6d.png" alt="Fork me on GitHub">
  </a>
  <script src="https://use.fontawesome.com/4eb261e456.js"></script>

  <script>
    MathJax = {
      tex: {
        inlineMath: [
          ['$', '$'],
          ['\\(', '\\)']
        ]
      },
      svg: {
        fontCache: 'global'
      }
    };
  </script>
  <script type="text/javascript" id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js">
  </script>
<meta name="generator" content="Hexo 4.2.0"></head>

<body>
  <header id="header" class="inner"><div class="alignleft">
  <h1><a href="/">WatsonYang&#39;s Blog</a></h1>
  <h2><a href="/">Enrich yourself.</a></h2>
</div>
<nav id="main-nav" class="alignright">
  <ul>
    
      <li><a href="/">Home</a></li>
    
      <li><a href="/archives">Archives</a></li>
    
      <li><a href="/about">About</a></li>
    

    <li> <a href="/atom.xml">RSS</a> </li>
  </ul>
  <div class="clearfix"></div>
</nav>
<div class="clearfix"></div>
</header>
  <div id="content" class="inner">
    <div id="main-col" class="alignleft"><div id="wrapper"><article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2017-02-13T16:51:14.000Z"><a href="/2017/02/14/CS231n-Notes-2/">2017-02-14</a></time>
      
      
  
    <h1 class="title">CS231n Notes 2</h1>
  

    </header>
    <div class="entry">
      
        <p>CS231n课程笔记，记录自己看懂和没看懂的地方。这里是<a href="https://watsonyanghx.github.io/2017/01/15/CS231n-Notes-1/">CS231n Notes 1</a>。</p>
<h2 id="Lecture-9"><a href="#Lecture-9" class="headerlink" title="Lecture 9"></a><strong>Lecture 9</strong></h2><h2 id="Lecture-10"><a href="#Lecture-10" class="headerlink" title="Lecture 10"></a><strong>Lecture 10</strong></h2><h3 id="RNN"><a href="#RNN" class="headerlink" title="RNN"></a><strong>RNN</strong></h3><ol>
<li><p>输出层的输出只与当前的隐藏状态有关，而隐藏层可以认为是网络的记忆单元，一般只记忆前面若干步的隐藏状态。</p>
</li>
<li><p><code>RNN</code>的权重是共享的，也就是<code>输入--&gt;隐层，隐层--&gt;隐层，隐层--&gt;输出</code>的权重矩阵（分别是<code>U</code>、<code>W</code> 与 <code>V</code>），在每一层都是对同一个进行更新。</p>
</li>
<li><p><code>RNN</code>并不是每一步都必须有输入与输出，但必须有隐层，用于捕捉序列的信息。</p>
</li>
<li><p><code>RNN</code>训练用<code>BPTT</code>，但<code>BPTT</code>无法解决长时以来问题（即当前输出与前面一段较长的序列相关，可能会带来梯度消失）。</p>
</li>
</ol>
<a id="more"></a>

<ol start="5">
<li><p><code>RNN</code>中不常使用正则化，是一个<code>hyper parameter</code>。</p>
</li>
<li><p><code>LSTM</code>之所以要forget一些东西，是为了防止时间轴上的梯度消失。<code>LSTM</code>的结果类似<code>ResNet</code>，但它还有个<code>forget gate</code>。不同结构的效果基本相同，有些在某些情况可能会更坏也可能更好。</p>
</li>
</ol>
<h3 id="Image-Caption"><a href="#Image-Caption" class="headerlink" title="Image Caption"></a><strong>Image Caption</strong></h3><ol>
<li><p>用<code>CNN</code> + <code>RNN</code>组合来做。<code>CNN</code>用来获取图像的特征。</p>
<p> 具体就是把<code>CNN</code>最后一层的 classifier 去掉，然后把<code>fc</code>的特征作为<code>RNN</code>的输入数据（有很多方式把 features 输入<code>RNN</code>中）。前向传播时的计算公式也随之发生了变化（这也只是很多种计算方式中的一种，可见视频33分钟处）。</p>
<p> 一般只把<code>CNN</code>得到的 features plug into <code>RNN</code>的第一层，也可以 plug into 到它几层，但是效果会变差。</p>
</li>
<li><p>34：34出的<code>sample</code>没看懂。</p>
<p> 就是把上一个的输出中最有可能正确的数据，作为紧接着的下一个的输入。输出<code>y</code>的维度就是 vocabulary 的大小+1（因为需要一个<end> token）</p>
</li>
<li><p>在训练的时候<code>CNN</code>的部分还需要继续训练吗？</p>
<p> 可以训练，也可以不训练。训练的的话会更好，因为可以让<code>CNN</code>学习应该学习图片的哪些 features。</p>
</li>
</ol>
<h3 id="LSTM"><a href="#LSTM" class="headerlink" title="LSTM"></a><strong>LSTM</strong></h3><ol>
<li><p>让<code>RNN</code>更为复杂的一个方法是<code>stack RNN</code>，就是把一个<code>RNN</code>隐藏层的输出作为另一个<code>RNN</code>隐藏层的输入。</p>
</li>
<li><p><code>LSTM</code>与<code>RNN</code>的不同在于更新公式更为复杂。另外，每个隐藏层都有两个参数（都是<code>vector</code>）：<code>C</code>与<code>H</code>，比<code>RNN</code>多了一个<code>C</code>（48：24）。</p>
</li>
<li><p><code>GRU</code></p>
</li>
</ol>
<!--more-->


<h2 id="Lecture-11"><a href="#Lecture-11" class="headerlink" title="Lecture 11"></a><strong>Lecture 11</strong></h2><h3 id="Data-Augumentation"><a href="#Data-Augumentation" class="headerlink" title="Data Augumentation"></a><strong>Data Augumentation</strong></h3><ol>
<li><p>通常的方法有以下几种：</p>
<ol>
<li><p><strong>Change all pixels</strong></p>
<ul>
<li>Transmision to gray image</li>
<li>Shift</li>
<li>Horizontal flipping(mirror image)</li>
</ul>
</li>
<li><p><strong>Random crops</strong></p>
<ul>
<li>Take a patch of image at a random scale and location (need to resize for training)</li>
</ul>
</li>
<li><p><strong>Color jitter</strong></p>
<ul>
<li>Change contrast</li>
<li>Complex method (9:28)</li>
</ul>
</li>
<li><p><strong>General theme</strong></p>
<ul>
<li>batch normalization</li>
</ul>
</li>
</ol>
</li>
<li><p>Very useful for small datasets, should be used at any time</p>
</li>
<li><p>通常在 training time 进行 <code>Data Augumentation</code>，因为如果把处理的图片存到 disk 中，会占用大量的 disk space。</p>
</li>
</ol>
<h3 id="Transfer-Learning-with-CNNs"><a href="#Transfer-Learning-with-CNNs" class="headerlink" title="Transfer Learning with CNNs"></a><strong>Transfer Learning with CNNs</strong></h3><ol>
<li><p>用训练好的网络提取待 training dataset 的 features，存入 disk，然后载入这些 features 训练自己的新模型。</p>
<ul>
<li><p>Small dataset: treat classifier as a fixed feature extractor. </p>
<p>  一般的做法： take away last layer, replace with liner calssifier with you care about, freeze other layerand retrain only that top layer (14:20)。就相当于在通过<code>CNN</code>得到的 feature 上直接训练一个 liner classifier（在这之前需要把 feature 存到 disk 上）。</p>
</li>
<li><p>More data: 可以多训练最后几层。</p>
</li>
</ul>
</li>
<li><p>training dataset 较少的时候可以少训练几层，较多的时候可以多训练几层。训练的时候可以把要训练的层同时训练，也可以先训练最后的 classifier 层（因为这里的参数是随机初始化的），当它收敛后，在训练前面的 intermediate 层。</p>
</li>
<li><p>底层得到的特征，比如边缘、颜色等可以直接用到许多 CV task 中。</p>
</li>
<li><p>如果数据比较多可以多训练几层</p>
</li>
<li><p>13分钟第一个问题没有听懂，训练时<code>dump to disk</code>是什么意思？</p>
<p> 把训练好的 features（<code>W</code>、<code>b</code>等参数）存到磁盘中，然后利用这些 features 去训练</p>
</li>
<li><p>how to tackle convolutions: 35:50</p>
</li>
</ol>
<h3 id="All-About-Convolutions"><a href="#All-About-Convolutions" class="headerlink" title="All About Convolutions"></a><strong>All About Convolutions</strong></h3><ol>
<li><p>前两个问题没看懂 :(</p>
</li>
<li><p>尽量用较小的<code>filter</code>进行<code>stack</code>的方式，不要用 size 较大的<code>filter</code></p>
<ul>
<li><p>参数数量少</p>
</li>
<li><p>非线性好</p>
</li>
<li><p>计算量少</p>
</li>
</ul>
</li>
</ol>
<h3 id="Implementing-Convolutions"><a href="#Implementing-Convolutions" class="headerlink" title="Implementing Convolutions"></a><strong>Implementing Convolutions</strong></h3><ol>
<li><p><strong>im2col</strong></p>
<p> 可能会占用较多内存，但实际应用中还可以，且效率比较快。推荐。</p>
</li>
<li><p><strong>FFT：Fast Fourier Transformation</strong></p>
<ul>
<li>实际使用中 doesn’t work well（对3 x 3的 convolution）</li>
<li>stride 处理不好</li>
</ul>
</li>
<li><p><strong>Fast Algorithm</strong></p>
<ul>
<li>Strasens’s Algorithm</li>
</ul>
</li>
</ol>
<h3 id="Implementing-Details"><a href="#Implementing-Details" class="headerlink" title="Implementing Details"></a><strong>Implementing Details</strong></h3><p>Use GPU、use existed library: TF、Tourch、Caffe 都可以玩一玩，当使用不懂时，看源码。</p>
<ol>
<li><p>GPU - GPU communication is bottleneck (1:04:20有个 slide)</p>
</li>
<li><p>disk bottleneck (1:05:50有个 slide)</p>
</li>
</ol>
<h2 id="Lecture-13"><a href="#Lecture-13" class="headerlink" title="Lecture 13"></a><strong>Lecture 13</strong></h2><p><code>Semantic Segmentation</code>与<code>Instance Segmentation</code>一般都是分开做。</p>
<h3 id="Semantic-Segmentation"><a href="#Semantic-Segmentation" class="headerlink" title="Semantic Segmentation"></a><strong>Semantic Segmentation</strong></h3><ol>
<li><p><strong>Don’t care about instance</strong></p>
<p> 取patch –&gt; feed to CNN –&gt; classify the center pixel of the patch。</p>
<p> 对整个图片进行上面的操作。是一种比较 expensive operation，采用与<code>object detection</code>相类似的 trick：run FCNN–&gt;get pixel heatmap（如果有downsampling，pad等操作，可能结果的图片尺寸会变小，需要额外的工作）。</p>
</li>
<li><p><strong>Multi-scale</strong></p>
<ol>
<li>resize input image to different -&gt; run fcnn -&gt; upsample and stack them</li>
<li>merge similar pixels to form a segmentation trees</li>
<li>combine them together</li>
</ol>
</li>
<li><p><strong>Fully CNN</strong></p>
<ul>
<li>还是没看懂</li>
</ul>
</li>
<li><p>大多还是<code>conv</code>与<code>deconv</code>的方式来做（ppt 79页）</p>
</li>
<li><p><strong>Refinement</strong></p>
</li>
<li><p><strong>Upsampling</strong></p>
</li>
</ol>
<h3 id="Instance-Segmentation"><a href="#Instance-Segmentation" class="headerlink" title="Instance Segmentation"></a><strong>Instance Segmentation</strong></h3><ol>
<li>distinguish different instance of same kind of objects.</li>
</ol>
<p>与 <code>RNN</code> 很相似（30分处没看懂）。</p>
<ol start="2">
<li><p>Caseades：similar to <code>Faster-RCNN</code></p>
</li>
<li><p>与<code>Object Detection</code>很相似。</p>
</li>
</ol>
<h3 id="Attention-Models"><a href="#Attention-Models" class="headerlink" title="Attention Models"></a><strong>Attention Models</strong></h3><ol>
<li><p>comes from <code>Machine Translation</code> first</p>
</li>
<li><p>soft attention vs hard attention，which one is better？（49:17）</p>
</li>
<li><p>和后面的<code>Spatial Transformer</code>都没看懂</p>
</li>
</ol>
<h2 id="Lecture-14"><a href="#Lecture-14" class="headerlink" title="Lecture 14"></a><strong>Lecture 14</strong></h2><ol>
<li><p><code>Optical Flow</code></p>
</li>
<li><p>有个 tracker，视频课程中是15 frame</p>
</li>
<li><p><code>Spatio-Temporal ConvNets</code></p>
<ul>
<li>3D VGGNet，</li>
<li>把 <code>Optical Flow</code> 当作原始数据跑，效果会更好。</li>
</ul>
</li>
<li><p><code>Long-Time Spatio-Temporal ConvNets</code></p>
<p> 在网络的某些地方使用<code>RNN</code>。</p>
</li>
<li><p>22：35处：summary so far</p>
</li>
<li><p>29:25: summary</p>
</li>
</ol>
<h3 id="Unsupervised-Learning"><a href="#Unsupervised-Learning" class="headerlink" title="Unsupervised Learning"></a><strong>Unsupervised Learning</strong></h3><h4 id="Autoencoders"><a href="#Autoencoders" class="headerlink" title="Autoencoders"></a><strong>Autoencoders</strong></h4><ol>
<li><code>Encoder</code>与<code>Decoder</code>有时会<code>share weights</code>（大概39分钟处），经常把<code>Encoder</code>作为<code>Supervised Learning</code>网络的初始化部分，但实际中的效果并不太好。</li>
</ol>

      
    </div>
    <footer>
      
        
  
  <div class="categories">
    <a href="/categories/Computer-Vision/">Computer Vision</a>
  </div>

        
  
  <div class="tags">
    <a href="/tags/Deep-Learning/">Deep Learning</a>, <a href="/tags/CS231n/">CS231n</a>, <a href="/tags/CNN/">CNN</a>
  </div>

        
  <div class="addthis addthis_toolbox addthis_default_style">
    
      <a class="addthis_button_facebook_like" fb:like:layout="button_count"></a>
    
    
      <a class="addthis_button_tweet"></a>
    
    
      <a class="addthis_button_google_plusone" g:plusone:size="medium"></a>
    
    
      <a class="addthis_button_pinterest_pinit" pi:pinit:layout="horizontal"></a>
    
    <a class="addthis_counter addthis_pill_style"></a>
  </div>
  <script type="text/javascript" src="//s7.addthis.com/js/300/addthis_widget.js"></script>

      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>


<section id="comment">
  <h1 class="title">Comments</h1>

  
  <div id="disqus_thread">
    <noscript>Please enable JavaScript to view the <a href="//disqus.com/?ref_noscript" target="_blank" rel="noopener">comments powered by Disqus.</a></noscript>
  </div>
  
</section>

</div></div>
    <aside id="sidebar" class="alignright">
  <div class="search">
  <form action="//google.com/search" method="get" accept-charset="utf-8">
    <input type="search" name="q" results="0" placeholder="Search">
    <input type="hidden" name="q" value="site:watsonyanghx.github.io">
  </form>
</div>

  
<div class="widget tag">
  <h3 class="title">Categories</h3>
  <ul class="entry">
  
    <li><a href="/categories/Algorithm/">Algorithm</a><small>2</small></li>
  
    <li><a href="/categories/C/">C++</a><small>5</small></li>
  
    <li><a href="/categories/Computer-Vision/">Computer Vision</a><small>3</small></li>
  
    <li><a href="/categories/Deep-Learning/">Deep Learning</a><small>3</small></li>
  
    <li><a href="/categories/Interview/">Interview</a><small>1</small></li>
  
    <li><a href="/categories/Machine-Learning/">Machine Learning</a><small>8</small></li>
  
    <li><a href="/categories/NLP/">NLP</a><small>1</small></li>
  
    <li><a href="/categories/OJ/">OJ</a><small>48</small></li>
  
    <li><a href="/categories/Paper/">Paper</a><small>5</small></li>
  
    <li><a href="/categories/Reinforcement-Learning/">Reinforcement Learning</a><small>1</small></li>
  
    <li><a href="/categories/Technique-Summaries/">Technique Summaries</a><small>2</small></li>
  
    <li><a href="/categories/Tools/">Tools</a><small>1</small></li>
  
    <li><a href="/categories/WhatEver/">WhatEver</a><small>1</small></li>
  
  </ul>
</div>


  
<div class="widget tagcloud">
  <h3 class="title">Tag Cloud</h3>
  <div class="entry">
    <a href="/tags/Algorithm/" style="font-size: 18.18px;">Algorithm</a> <a href="/tags/BFS/" style="font-size: 11.82px;">BFS</a> <a href="/tags/Backtracking/" style="font-size: 12.73px;">Backtracking</a> <a href="/tags/Binary-Tree/" style="font-size: 14.55px;">Binary Tree</a> <a href="/tags/C/" style="font-size: 10px;">C++</a> <a href="/tags/CNN/" style="font-size: 15.45px;">CNN</a> <a href="/tags/CS231n/" style="font-size: 11.82px;">CS231n</a> <a href="/tags/Combinatorial-number/" style="font-size: 10.91px;">Combinatorial number</a> <a href="/tags/DFS/" style="font-size: 14.55px;">DFS</a> <a href="/tags/DP/" style="font-size: 19.09px;">DP</a> <a href="/tags/Data-structure/" style="font-size: 10.91px;">Data structure</a> <a href="/tags/DeconvNet/" style="font-size: 10px;">DeconvNet</a> <a href="/tags/Deep-Learning/" style="font-size: 17.27px;">Deep Learning</a> <a href="/tags/Disjoint-Set/" style="font-size: 11.82px;">Disjoint Set</a> <a href="/tags/Divide-and-Conquer/" style="font-size: 13.64px;">Divide and Conquer</a> <a href="/tags/GAN/" style="font-size: 10.91px;">GAN</a> <a href="/tags/Graph/" style="font-size: 13.64px;">Graph</a> <a href="/tags/Greedy/" style="font-size: 11.82px;">Greedy</a> <a href="/tags/Hexo/" style="font-size: 10px;">Hexo</a> <a href="/tags/HihoCoder/" style="font-size: 16.36px;">HihoCoder</a> <a href="/tags/IPv6/" style="font-size: 10px;">IPv6</a> <a href="/tags/LintCode/" style="font-size: 20px;">LintCode</a> <a href="/tags/Machine-Learning/" style="font-size: 10.91px;">Machine Learning</a> <a href="/tags/Matrix/" style="font-size: 10px;">Matrix</a> <a href="/tags/Metaprogramming/" style="font-size: 10px;">Metaprogramming</a> <a href="/tags/NLP/" style="font-size: 10px;">NLP</a> <a href="/tags/Normalization/" style="font-size: 10px;">Normalization</a> <a href="/tags/Notes/" style="font-size: 10px;">Notes</a> <a href="/tags/Numpy/" style="font-size: 10px;">Numpy</a> <a href="/tags/POJ/" style="font-size: 10.91px;">POJ</a> <a href="/tags/Permutation/" style="font-size: 10px;">Permutation</a> <a href="/tags/Python/" style="font-size: 10px;">Python</a> <a href="/tags/Queue/" style="font-size: 10.91px;">Queue</a> <a href="/tags/RL/" style="font-size: 10px;">RL</a> <a href="/tags/Recursion/" style="font-size: 12.73px;">Recursion</a> <a href="/tags/ResNet/" style="font-size: 10px;">ResNet</a> <a href="/tags/Shadowsocks/" style="font-size: 10px;">Shadowsocks</a> <a href="/tags/String/" style="font-size: 11.82px;">String</a> <a href="/tags/Template/" style="font-size: 10.91px;">Template</a> <a href="/tags/VPS/" style="font-size: 10px;">VPS</a> <a href="/tags/WhatEver/" style="font-size: 10px;">WhatEver</a>
  </div>
</div>

</aside>
    <div class="clearfix"></div>
  </div>
  <footer id="footer" class="inner"><div class="alignleft">
  
  &copy; 2020 Watson Yang
  
</div>
<div class="clearfix"></div></footer>
  <script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>
<script src="/js/jquery.imagesloaded.min.js"></script>
<script src="/js/gallery.js"></script>


<script type="text/javascript">
var disqus_shortname = 'light-disqus';

(function(){
  var dsq = document.createElement('script');
  dsq.type = 'text/javascript';
  dsq.async = true;
  dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
  (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
}());
</script>



<link rel="stylesheet" href="/fancybox/jquery.fancybox.css" media="screen" type="text/css">
<script src="/fancybox/jquery.fancybox.pack.js"></script>
<script type="text/javascript">
(function($){
  $('.fancybox').fancybox();
})(jQuery);
</script>


<div id="totop" style="position:fixed;bottom:35px;right:20px;cursor: pointer;">
  <a title="返回顶部"><i class="fa fa-angle-double-up fa-3x" aria-hidden="true"></i></a>
</div>
<script src="/js/totop.js"></script>

</body>
</html>
