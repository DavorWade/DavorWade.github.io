<!DOCTYPE HTML>
<html>

<head>
  <meta name="google-site-verification" content="6-Grm86DHfS67XSHMuvfO8cIcQcOWPpjGvOBMxbA3Fo" />
  <meta charset="utf-8">
  
  <title>Chapter 7 Regularization for Deep Learning | WatsonYang&#39;s Blog</title>
  
  <meta name="author" content="Watson Yang">
  
  <meta name="description" content="My secret private space.">
  
  
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

  
  <meta property="og:title" content="Chapter 7 Regularization for Deep Learning" />
  <meta property="og:site_name" content="WatsonYang&#39;s Blog" />

  
  <meta property="og:image" content="" />
  

  <link href="/favicon.png" rel="icon">
  <link rel="alternate" href="/atom.xml"
    title="WatsonYang&#39;s Blog" type="application/atom+xml">
  <link rel="stylesheet" href="/css/style.css" media="screen" type="text/css">
  <!--[if lt IE 9]><script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script><![endif]-->
  


  <a href="https://github.com/watsonyanghx" target="_blank">
    <img style="position: absolute; top: 0; left: 0; border: 0;"
      src="https://s3.amazonaws.com/github/ribbons/forkme_left_gray_6d6d6d.png" alt="Fork me on GitHub">
  </a>
  <script src="https://use.fontawesome.com/4eb261e456.js"></script>

  <script>
    MathJax = {
      tex: {
        inlineMath: [
          ['$', '$'],
          ['\\(', '\\)']
        ]
      },
      svg: {
        fontCache: 'global'
      }
    };
  </script>
  <script type="text/javascript" id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js">
  </script>
<meta name="generator" content="Hexo 4.2.0"></head>

<body>
  <header id="header" class="inner"><div class="alignleft">
  <h1><a href="/">WatsonYang&#39;s Blog</a></h1>
  <h2><a href="/">Enrich yourself.</a></h2>
</div>
<nav id="main-nav" class="alignright">
  <ul>
    
      <li><a href="/">Home</a></li>
    
      <li><a href="/archives">Archives</a></li>
    
      <li><a href="/about">About</a></li>
    

    <li> <a href="/atom.xml">RSS</a> </li>
  </ul>
  <div class="clearfix"></div>
</nav>
<div class="clearfix"></div>
</header>
  <div id="content" class="inner">
    <div id="main-col" class="alignleft"><div id="wrapper"><article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2017-02-25T14:55:40.000Z"><a href="/2017/02/25/Chapter-7-Regularization-for-Deep-Learning/">2017-02-25</a></time>
      
      
  
    <h1 class="title">Chapter 7 Regularization for Deep Learning</h1>
  

    </header>
    <div class="entry">
      
        <p>首先，介绍了<code>regularization</code>的定义：</p>
<blockquote>
<p>any modification we make to a learning algorithm that is intended to reduce its generalization error but not its training error.</p>
</blockquote>
<p>也就是说<code>regularization</code>的<font color='red'>目的</font>是：通过<strong>控制模型复杂度</strong>的方式，尽量减轻模型<code>overfitting</code>的程度，提高<code>generalization</code>能力。</p>
<ul>
<li><p><code>regularization</code>的大致可以分为以下几类：</p>
<ul>
<li><p>作用对象/实现方式</p>
<ul>
<li>put extra constraints on a <strong>machine learning model</strong>，such as adding restrictions on the <strong>parameter</strong> values。</li>
<li>add extra terms in the <strong>objective function</strong> that can be thought of as corresponding to a <strong>soft constraint</strong> on the parameter values。</li>
</ul>
</li>
<li><p>原因 <a id="more"></a></p>
<ul>
<li>encode specific kinds of <strong>prior knowledge</strong></li>
<li>express a generic preference for a <strong>simpler</strong> model class in order to <strong>promote generalization</strong></li>
</ul>
</li>
</ul>
</li>
<li><p>An <strong>effective regularizer</strong> is one that makes a profitable trade, reducing variance significantly while not overly increasing the bias.</p>
</li>
<li><p>controlling the <strong>complexity of the model</strong> is not a simple matter of finding the model of the right size, with the right number of parameters. </p>
</li>
<li><p>Instead, we might find—and indeed <strong>in practical</strong> deep learning scenarios, we almost always do find—that <strong>the best fitting</strong> model (in the sense of minimizing generalization error) is a large model that has been regularized appropriately.</p>
</li>
</ul>
<h1 id="7-1-Parameter-Norm-Penalties"><a href="#7-1-Parameter-Norm-Penalties" class="headerlink" title="7.1 Parameter Norm Penalties"></a>7.1 Parameter Norm Penalties</h1><ul>
<li><p>最常用的方法是在<code>objective function</code>上加上一个 parameter norm penalty，这种方法的<strong>理论基础</strong>是 limiting the capacity of models。</p>
<p>此时，<code>objective function</code>的一般形式如下：</p>
<center>
<img src="https://www.tuchuang001.com/images/2017/02/27/QQ20170227111134.png" alt="Objective function with parameter norm penalty" border="0" />
</center>

<p>其中，<code>α</code>控制着两者的强度，对于<code>neural networks</code>，针对每个层，可以选择不同的值，但是很难确定哪一个值是正确的（it can be expensive to search for the correct value of multiple hyperparameters），所以一般都选择同一个值；<code>Ω(θ)</code>可以选择不同的函数，产生的效果也会不一样。</p>
</li>
<li><p>通常<strong>不对</strong><code>bias</code>进行正则化，一是会加大计算量，同时效果也不显著；二是可能会产生<code>underfitting</code>。</p>
</li>
</ul>
<p>下面对不同的 Parameter Norm Penalties 进行介绍，主要是 L2 与 L1。</p>
<h2 id="7-1-1-L2-Parameter-Regularization"><a href="#7-1-1-L2-Parameter-Regularization" class="headerlink" title="7.1.1 L2 Parameter Regularization"></a>7.1.1 L2 Parameter Regularization</h2><p>就是通常所说的<code>weight decay</code>，此时<code>objective function</code>的形式如下：</p>
<center>
<img src="https://www.tuchuang001.com/images/2017/02/27/QQ20170227113937.png" alt="Objective function with L2" border="0" />
</center>

<p>这是最常用的一种正则化，它对参数更新的每一步的<strong>作用</strong>是：</p>
<blockquote>
<p>the addition of the weight decay term has modified the learning rule to multiplicatively <strong>shrink the weight vector by a constant factor</strong> on <strong>each step</strong></p>
</blockquote>
<p>但是，在<strong>整个训练过程</strong>中，它又是如何发挥作用的呢（But what happens over the entire course of training?）？</p>
<p>为了回答这个问题，紧接着是一系列的数学公式推导……</p>
<p>最后的答案是：</p>
<blockquote>
<p>We can see that L2 regularization causes the learning algorithm to “perceive” the input X as having <strong>higher variance</strong>, which makes it shrink the weights on features whose covariance with the output target is low compared to this added variance</p>
</blockquote>
<h2 id="7-1-2-L1-Regularization"><a href="#7-1-2-L1-Regularization" class="headerlink" title="7.1.2 L1 Regularization"></a>7.1.2 L1 Regularization</h2><p>此时<code>objective function</code>的形式如下：</p>
<center>
<img src="https://www.tuchuang001.com/images/2017/02/27/QQ20170227114024.png" alt="Objective function with L1" border="0" />
</center>

<p>这一小节并没有看太难懂，只是知道它的<strong>主要作用</strong>是：</p>
<ul>
<li><p>稀疏化作用</p>
</li>
<li><p>正因为它的稀疏性特性，常被用来做 feature selection（<code>CS231n</code>课程中也有提到这一点）</p>
</li>
</ul>
<p>但是相对于 L2，使用并不频繁。</p>
<h1 id="7-2-Norm-Penalties-as-Constrained-Optimization"><a href="#7-2-Norm-Penalties-as-Constrained-Optimization" class="headerlink" title="7.2 Norm Penalties as Constrained Optimization"></a>7.2 Norm Penalties as Constrained Optimization</h1><p><font color='red'>没看懂 -_-</font> </p>
<ul>
<li><p>这里讨论的不再是对<code>objective function</code>进行惩罚，而是直接使用 explicit constraints。</p>
</li>
<li><p>使用 explicit constraints 的好处：</p>
<ul>
<li><p>Another reason to use explicit constraints and reprojection rather than enforcing constraints with penalties is that penalties can cause non-convex optimization procedures to get stuck in local minima corresponding to small θ.</p>
</li>
<li><p>当使用的<code>leraning rate</code>比较大的时候，可能会陷入 a positive feedback loop，而 explicit constraints 可以避免这种情况。</p>
</li>
</ul>
</li>
</ul>
<h1 id="7-3-Regularization-and-Under-Constrained-Problems"><a href="#7-3-Regularization-and-Under-Constrained-Problems" class="headerlink" title="7.3 Regularization and Under-Constrained Problems"></a>7.3 Regularization and Under-Constrained Problems</h1><p><font color='red'>没看懂 -_-</font> </p>
<h1 id="7-4-Dataset-Augmentation"><a href="#7-4-Dataset-Augmentation" class="headerlink" title="7.4 Dataset Augmentation"></a>7.4 Dataset Augmentation</h1><ul>
<li><p>提升模型性能的<strong>最好方法</strong>是使用更大的数据集对其进行训练，但是很多时候数据集都是有限的。为了解决这个问题，可以对已有的数据集进行操作（比如镜像，裁剪，patch等），然后加到已有的数据集中去。</p>
</li>
<li><p>这种方法可以很好的应用于 <strong>classification 类型</strong>的任务（不仅仅是 classification，还包括 recognition、detection 等），但是对其它的一些任务却很难使用（比如对于 density estimation 类型的任务，除非已经知道了原有数据的分布，不过既然知道了，也不再需要进行 Dataset Augmentation 了）。</p>
<p><font color='red'>注意</font>：</p>
<ul>
<li><p>对数据集（特别是<font color='red'>图像</font>）进行 Dataset Augmentation（比如：transformations、rotating、scaling）的时候，一定要保证不能改变数据的<code>label</code>（比如应该避免<code>b</code>转化为<code>d</code>等类似操作）。</p>
</li>
<li><p>对图像进行的时候，不能只 shift 图像，因为这样根本没有帮助。</p>
</li>
</ul>
</li>
<li><p>对数据集添加噪音（Injecting noise）也是一种 Dataset Augmentation，加入噪音的对象可以分为：</p>
<ul>
<li><p>input。</p>
</li>
<li><p>hidden units。<code>Dropout</code>就是一种。对 hidden units 加入噪音可以看作是对不同抽象层级的一种 Dataset Augmentation。</p>
</li>
<li><p>weights。下一小节讲的内容主要就是这个。</p>
</li>
<li><p>output units。下一小节提了一下这个，不过没看懂。</p>
</li>
</ul>
</li>
<li><p>为什么要加入噪音？</p>
<p>原因是因为<code>neural networks</code>对 noise 的鲁棒性不是很好，而提高<code>neural networks</code>鲁棒性的一种方式就是用加入噪音的数据对其进行训练。</p>
</li>
<li><p>注意区分 Dataset Augmentation 与 pre-processing 的区别：</p>
<ul>
<li><p>operations that are <font color='red'>generally applicable</font> (such as adding Gaussian noise to the input) are<br>considered part of the machine learning algorithm。</p>
</li>
<li><p>while operations that are <font color='red'>specific to one application domain</font> (such as randomly cropping an image) are considered to be separate pre-processing steps。</p>
</li>
</ul>
</li>
</ul>
<h1 id="7-5-Noise-Robustness"><a href="#7-5-Noise-Robustness" class="headerlink" title="7.5 Noise Robustness"></a>7.5 Noise Robustness</h1><ul>
<li><p><code>noise injection</code>对<code>neural networks</code>的性能的提升比仅仅只是 simply shrinking the parameters 要更强有力。特别是对<code>hidden units</code>加入 noise 的时候。</p>
</li>
<li><p>Adding noise to the weights 在<code>RNN</code>的<code>regularization</code>中有很好的效果（但是记得<code>CS231n</code>课程中提到<code>RNN</code>不需要用？）。这一段在新版中被删掉了，不知道为什么。 </p>
</li>
<li><p>然后用<code>MLP</code>举了个对 adding noise to the weights 的例子，但是<font color='red'>没有看懂</font>想表达什么意思。。。</p>
</li>
</ul>
<h1 id="7-5-1-Injecting-Noise-at-the-Output-Targets"><a href="#7-5-1-Injecting-Noise-at-the-Output-Targets" class="headerlink" title="7.5.1 Injecting Noise at the Output Targets"></a>7.5.1 Injecting Noise at the Output Targets</h1><p><font color='red'>这一小节没有看懂 -_-</font></p>
<h1 id="7-6-Semi-Supervised-Learning"><a href="#7-6-Semi-Supervised-Learning" class="headerlink" title="7.6 Semi-Supervised Learning"></a>7.6 Semi-Supervised Learning</h1><ul>
<li><p>In the paradigm of semi-supervised learning, <strong>both</strong> unlabeled examples from P(x) and labeled examples from P (x, y) are used to estimate P (y | x) or predict y from x.</p>
</li>
<li><p>In the context of deep learning, semi-supervised learning usually refers to <strong>learning a representation</strong> h = f (x). The <strong>goal</strong> is to learn a representation so that examples from the same class have similar representations.</p>
</li>
</ul>
<h1 id="7-7-Multi-Task-Learning"><a href="#7-7-Multi-Task-Learning" class="headerlink" title="7.7 Multi-Task Learning"></a>7.7 Multi-Task Learning</h1><ul>
<li><p>多个不同的，但<strong>相关</strong>的任务<strong>共享</strong>网络结构，以期达到整体提高的效果。其中，多任务共享的网络相当于一个<strong>特征提取器</strong>，多任务信息互相补充，让公用特征提取的更好。</p>
</li>
<li><p>比如下面就是一个 Multi-Task Learning 的例子。</p>
<center>
<img src="https://www.tuchuang001.com/images/2017/03/17/QQ20170317002802.png" alt="Multi-task learning" border="0" />
</center>

<p>Multi-Task Learning 可以分为两个阶段，首先是可以共用的特征的学习，然后进行 task-specific 的任务。</p>
<p>对于上图，这里<code>X</code>是共享的 input，通过这个 input 学习到可以<strong>共用</strong>的 feature representation（上图是<code>h(shared)</code>），然后再在<code>h(shared)</code>的基础上做 task-specific 的任务（学习 task-specific 的 feature representation，然后完成相应的预测）。</p>
</li>
</ul>
<h1 id="7-8-Early-Stopping"><a href="#7-8-Early-Stopping" class="headerlink" title="7.8 Early Stopping"></a>7.8 Early Stopping</h1><ul>
<li><p>Definition：</p>
<blockquote>
<p>obtain a model with better validation set error (and thus, hopefully better test set error) by returning to the parameter setting at the point in time with the lowest validation set error. Every time the error on the validation set improves, we store a copy of the model parameters.</p>
</blockquote>
</li>
<li><p>Method：</p>
<p>Every time the error on the validation set improves, we store a copy of the model parameters. When the training algorithm terminates, we return these parameters, rather than the latest parameters. The algorithm terminates when no parameters have improved over the best recorded validation error for some pre-specified number of iterations.</p>
</li>
<li><p>Advantages：</p>
<ul>
<li>effectiveness</li>
<li>simplicity</li>
<li>requires almost no change in the underlying training procedure, the objective function, or the set of allowable parameter values</li>
<li>may be used either alone or in conjunction with other regularization strategies. </li>
<li>reduces the computational cost of the training procedure</li>
</ul>
</li>
<li><p>Disadvantages：</p>
<ul>
<li>the need to maintain a copy of the best parameters</li>
<li>requires a validation set</li>
</ul>
</li>
<li><p><font color='red'>注意：</font> 它与<code>cross validation</code>是不同的，它只有一个固定的<code>validation dataset</code>，用来判断<code>neural networks</code>的 generalization 能力。而<code>cross validation</code>通常是把整个<code>training dataset</code>分成<code>k</code>份，然后每次选其中一份作<code>validation dataset</code>，其余的作<code>training dataset</code>，且需要训练<code>k</code>个 epoch。</p>
</li>
<li><p>How to think of it？</p>
<p>One way to think of early stopping is as a very efficient <strong>hyperparameter selection</strong> algorithm. In this view, <strong>the number of training steps</strong> is just another hyperparameter.</p>
<center>
<img src="https://www.tuchuang001.com/images/2017/03/17/QQ20170317005754.png" alt="Training epochs" border="0" />
</center>
</li>
<li><p>会有一部分数据没有直接被用来训练 model，所以为了更好的利用这一部分数据，可以在确定 training steps 后，用所有的 training examples 再训练一次。此时有2种策略：</p>
<ul>
<li><p>随机初始化所有的<code>parameters</code>，然后训练 training steps 步数。不过这会带来一个问题：由于 training examples 增多了，所以这么多的训练步数可能无法充分的更新参数，致使网络无法充分的学习到数据的features。</p>
</li>
<li><p>按照之前存储下来的<code>parameters</code>初始化所有的<code>parameters</code>。但是此时由于没有 validation dataset，导致不知道应该在何时停止训练。</p>
</li>
</ul>
</li>
<li><p>How early stopping acts as a regularizer</p>
<p>主要就是说<code>early stopping</code>的作用和<code>L2</code>的作用相似，且比<code>L2</code>要好（因为它既起到了<code>L2</code>使<code>weight</code>比较小的作用，同时也起到保证 generalization 比较好作用），具体解释为什么和<code>L2</code>作用相似的部分没有太理解。</p>
</li>
</ul>
<h1 id="7-9-Parameter-Tying-and-Parameter-Sharing"><a href="#7-9-Parameter-Tying-and-Parameter-Sharing" class="headerlink" title="7.9 Parameter Tying and Parameter Sharing"></a>7.9 Parameter Tying and Parameter Sharing</h1><p>具体讲解可以参考书本，这里不知道该怎么阐述。<code>CNN</code>中<code>filter</code>的<code>parameter sharing</code>就是这种思想的体现。</p>
<p>有一个<font color='red'>问题</font>：为什么 supervised paradigm 要与 unsupervised paradigm 相一致？而不是反过来。</p>
<h1 id="7-10-Sparse-Representations"><a href="#7-10-Sparse-Representations" class="headerlink" title="7.10 Sparse Representations"></a>7.10 Sparse Representations</h1><ul>
<li><p><font color='red'>有什么好处？</font></p>
</li>
<li><p><font color='red'>太笨了，没看懂 -_-</font> </p>
</li>
</ul>
<h1 id="7-11-Bagging-and-Other-Ensemble-Methods"><a href="#7-11-Bagging-and-Other-Ensemble-Methods" class="headerlink" title="7.11 Bagging and Other Ensemble Methods"></a>7.11 Bagging and Other Ensemble Methods</h1><ul>
<li><p>The <strong>idea</strong> is to train several different models separately, then have all of the models <strong>vote</strong> on the output for test examples. This is an example of a general strategy in machine learning called <code>model averaging</code>. Techniques employing this strategy are known as <code>ensemble methods</code>。</p>
</li>
<li><p>书中有个例子很好的解释了 bagging 是如何工作的，以及它为什么效果会单一的模型要好。</p>
<center>
<img src="https://www.tuchuang001.com/images/2017/03/14/QQ20170314182509.png" alt="A cartoon depiction of how bagging works" border="0" />
</center>
</li>
<li><p>只看这些的话，对理解或使用 Bagging 肯定是不够的，可以看看别人写的博客或找本书看看。</p>
</li>
<li><p>注意区分以下概念：</p>
<ul>
<li><p>bagging（bootstrap aggregating）：</p>
</li>
<li><p>boosting：</p>
</li>
<li><p>stack：</p>
</li>
</ul>
</li>
</ul>
<h1 id="7-12-Dropout"><a href="#7-12-Dropout" class="headerlink" title="7.12 Dropout"></a>7.12 Dropout</h1><ul>
<li>感觉原论文讲的已经非常好了，比较容易理解，其它讲的都是试图去解释它为什么会起作用，而不是它是怎么实现的、怎么起作用的。</li>
</ul>
<h1 id="7-13-Adversarial-Training"><a href="#7-13-Adversarial-Training" class="headerlink" title="7.13 Adversarial Training"></a>7.13 Adversarial Training</h1><ul>
<li><p>可以参考这篇 <a href="http://www.kdnuggets.com/2015/07/deep-learning-adversarial-examples-misconceptions.html" target="_blank" rel="noopener">Deep Learning Adversarial Examples</a>，也是 Goodfellow 写的，更为详细和容易理解些。</p>
</li>
<li><p>前面已经讲到，<code>neural networks</code>对 noise 的鲁棒性不好，所以即使是准确率达到 100% 的模型也有可能出现非常匪夷所思的错误。如下图的例子。</p>
<center>
<img src="https://www.tuchuang001.com/images/2017/03/17/QQ20170317015741.png" alt="A demonstration of adversarial example generation applied to GoogLeNet on ImageNet. " border="0" />
</center>
</li>
<li><p>注意它与<code>GAN</code>是<strong>没有</strong>任何关系的。</p>
</li>
</ul>
<h1 id="7-14-Tangent-Distance-Tangent-Prop-and-Manifold-Tangent-Classifier"><a href="#7-14-Tangent-Distance-Tangent-Prop-and-Manifold-Tangent-Classifier" class="headerlink" title="7.14 Tangent Distance, Tangent Prop, and Manifold Tangent Classifier"></a>7.14 Tangent Distance, Tangent Prop, and Manifold Tangent Classifier</h1><p><font color='red'>太笨了，没看懂 -_-</font> </p>

      
    </div>
    <footer>
      
        
  
  <div class="categories">
    <a href="/categories/Machine-Learning/">Machine Learning</a>
  </div>

        
  
  <div class="tags">
    <a href="/tags/Deep-Learning/">Deep Learning</a>
  </div>

        
  <div class="addthis addthis_toolbox addthis_default_style">
    
      <a class="addthis_button_facebook_like" fb:like:layout="button_count"></a>
    
    
      <a class="addthis_button_tweet"></a>
    
    
      <a class="addthis_button_google_plusone" g:plusone:size="medium"></a>
    
    
      <a class="addthis_button_pinterest_pinit" pi:pinit:layout="horizontal"></a>
    
    <a class="addthis_counter addthis_pill_style"></a>
  </div>
  <script type="text/javascript" src="//s7.addthis.com/js/300/addthis_widget.js"></script>

      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>


<section id="comment">
  <h1 class="title">Comments</h1>

  
  <div id="disqus_thread">
    <noscript>Please enable JavaScript to view the <a href="//disqus.com/?ref_noscript" target="_blank" rel="noopener">comments powered by Disqus.</a></noscript>
  </div>
  
</section>

</div></div>
    <aside id="sidebar" class="alignright">
  <div class="search">
  <form action="//google.com/search" method="get" accept-charset="utf-8">
    <input type="search" name="q" results="0" placeholder="Search">
    <input type="hidden" name="q" value="site:watsonyanghx.github.io">
  </form>
</div>

  
<div class="widget tag">
  <h3 class="title">Categories</h3>
  <ul class="entry">
  
    <li><a href="/categories/Algorithm/">Algorithm</a><small>2</small></li>
  
    <li><a href="/categories/Computer-Vision/">Computer Vision</a><small>3</small></li>
  
    <li><a href="/categories/Deep-Learning/">Deep Learning</a><small>3</small></li>
  
    <li><a href="/categories/Interview/">Interview</a><small>1</small></li>
  
    <li><a href="/categories/Machine-Learning/">Machine Learning</a><small>8</small></li>
  
    <li><a href="/categories/NLP/">NLP</a><small>1</small></li>
  
    <li><a href="/categories/OJ/">OJ</a><small>48</small></li>
  
    <li><a href="/categories/Paper/">Paper</a><small>5</small></li>
  
    <li><a href="/categories/Reinforcement-Learning/">Reinforcement Learning</a><small>1</small></li>
  
    <li><a href="/categories/Technique-Summaries/">Technique Summaries</a><small>2</small></li>
  
    <li><a href="/categories/Tools/">Tools</a><small>1</small></li>
  
    <li><a href="/categories/WhatEver/">WhatEver</a><small>1</small></li>
  
  </ul>
</div>


  
<div class="widget tagcloud">
  <h3 class="title">Tag Cloud</h3>
  <div class="entry">
    <a href="/tags/Algorithm/" style="font-size: 18.18px;">Algorithm</a> <a href="/tags/BFS/" style="font-size: 11.82px;">BFS</a> <a href="/tags/Backtracking/" style="font-size: 12.73px;">Backtracking</a> <a href="/tags/Binary-Tree/" style="font-size: 14.55px;">Binary Tree</a> <a href="/tags/C/" style="font-size: 10px;">C++</a> <a href="/tags/CNN/" style="font-size: 15.45px;">CNN</a> <a href="/tags/CS231n/" style="font-size: 11.82px;">CS231n</a> <a href="/tags/Combinatorial-number/" style="font-size: 10.91px;">Combinatorial number</a> <a href="/tags/DFS/" style="font-size: 14.55px;">DFS</a> <a href="/tags/DP/" style="font-size: 19.09px;">DP</a> <a href="/tags/Data-structure/" style="font-size: 10.91px;">Data structure</a> <a href="/tags/DeconvNet/" style="font-size: 10px;">DeconvNet</a> <a href="/tags/Deep-Learning/" style="font-size: 17.27px;">Deep Learning</a> <a href="/tags/Disjoint-Set/" style="font-size: 11.82px;">Disjoint Set</a> <a href="/tags/Divide-and-Conquer/" style="font-size: 13.64px;">Divide and Conquer</a> <a href="/tags/GAN/" style="font-size: 10.91px;">GAN</a> <a href="/tags/Graph/" style="font-size: 13.64px;">Graph</a> <a href="/tags/Greedy/" style="font-size: 11.82px;">Greedy</a> <a href="/tags/Hexo/" style="font-size: 10px;">Hexo</a> <a href="/tags/HihoCoder/" style="font-size: 16.36px;">HihoCoder</a> <a href="/tags/IPv6/" style="font-size: 10px;">IPv6</a> <a href="/tags/LintCode/" style="font-size: 20px;">LintCode</a> <a href="/tags/Machine-Learning/" style="font-size: 10.91px;">Machine Learning</a> <a href="/tags/Matrix/" style="font-size: 10px;">Matrix</a> <a href="/tags/NLP/" style="font-size: 10px;">NLP</a> <a href="/tags/Normalization/" style="font-size: 10px;">Normalization</a> <a href="/tags/Notes/" style="font-size: 10px;">Notes</a> <a href="/tags/Numpy/" style="font-size: 10px;">Numpy</a> <a href="/tags/POJ/" style="font-size: 10.91px;">POJ</a> <a href="/tags/Permutation/" style="font-size: 10px;">Permutation</a> <a href="/tags/Python/" style="font-size: 10px;">Python</a> <a href="/tags/Queue/" style="font-size: 10.91px;">Queue</a> <a href="/tags/RL/" style="font-size: 10px;">RL</a> <a href="/tags/Recursion/" style="font-size: 12.73px;">Recursion</a> <a href="/tags/ResNet/" style="font-size: 10px;">ResNet</a> <a href="/tags/Shadowsocks/" style="font-size: 10px;">Shadowsocks</a> <a href="/tags/String/" style="font-size: 11.82px;">String</a> <a href="/tags/VPS/" style="font-size: 10px;">VPS</a> <a href="/tags/WhatEver/" style="font-size: 10px;">WhatEver</a>
  </div>
</div>

</aside>
    <div class="clearfix"></div>
  </div>
  <footer id="footer" class="inner"><div class="alignleft">
  
  &copy; 2020 Watson Yang
  
</div>
<div class="clearfix"></div></footer>
  <script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>
<script src="/js/jquery.imagesloaded.min.js"></script>
<script src="/js/gallery.js"></script>


<script type="text/javascript">
var disqus_shortname = 'light-disqus';

(function(){
  var dsq = document.createElement('script');
  dsq.type = 'text/javascript';
  dsq.async = true;
  dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
  (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
}());
</script>



<link rel="stylesheet" href="/fancybox/jquery.fancybox.css" media="screen" type="text/css">
<script src="/fancybox/jquery.fancybox.pack.js"></script>
<script type="text/javascript">
(function($){
  $('.fancybox').fancybox();
})(jQuery);
</script>


<div id="totop" style="position:fixed;bottom:35px;right:20px;cursor: pointer;">
  <a title="返回顶部"><i class="fa fa-angle-double-up fa-3x" aria-hidden="true"></i></a>
</div>
<script src="/js/totop.js"></script>

</body>
</html>
