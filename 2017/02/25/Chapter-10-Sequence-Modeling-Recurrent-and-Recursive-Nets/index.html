<!DOCTYPE HTML>
<html>

<head>
  <meta name="google-site-verification" content="6-Grm86DHfS67XSHMuvfO8cIcQcOWPpjGvOBMxbA3Fo" />
  <meta charset="utf-8">
  
  <title>Chapter 10 Sequence Modeling: Recurrent and Recursive Nets | WatsonYang&#39;s Blog</title>
  
  <meta name="author" content="Watson Yang">
  
  <meta name="description" content="My secret private space.">
  
  
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

  
  <meta property="og:title" content="Chapter 10 Sequence Modeling: Recurrent and Recursive Nets" />
  <meta property="og:site_name" content="WatsonYang&#39;s Blog" />

  
  <meta property="og:image" content="" />
  

  <link href="/favicon.png" rel="icon">
  <link rel="alternate" href="/atom.xml"
    title="WatsonYang&#39;s Blog" type="application/atom+xml">
  <link rel="stylesheet" href="/css/style.css" media="screen" type="text/css">
  <!--[if lt IE 9]><script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script><![endif]-->
  


  <a href="https://github.com/watsonyanghx" target="_blank">
    <img style="position: absolute; top: 0; left: 0; border: 0;"
      src="https://s3.amazonaws.com/github/ribbons/forkme_left_gray_6d6d6d.png" alt="Fork me on GitHub">
  </a>
  <script src="https://use.fontawesome.com/4eb261e456.js"></script>

  <script>
    MathJax = {
      tex: {
        inlineMath: [
          ['$', '$'],
          ['\\(', '\\)']
        ]
      },
      svg: {
        fontCache: 'global'
      }
    };
  </script>
  <script type="text/javascript" id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js">
  </script>
<meta name="generator" content="Hexo 4.2.0"></head>

<body>
  <header id="header" class="inner"><div class="alignleft">
  <h1><a href="/">WatsonYang&#39;s Blog</a></h1>
  <h2><a href="/">Enrich yourself.</a></h2>
</div>
<nav id="main-nav" class="alignright">
  <ul>
    
      <li><a href="/">Home</a></li>
    
      <li><a href="/archives">Archives</a></li>
    
      <li><a href="/about">About</a></li>
    

    <li> <a href="/atom.xml">RSS</a> </li>
  </ul>
  <div class="clearfix"></div>
</nav>
<div class="clearfix"></div>
</header>
  <div id="content" class="inner">
    <div id="main-col" class="alignleft"><div id="wrapper"><article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2017-02-25T15:05:53.000Z"><a href="/2017/02/25/Chapter-10-Sequence-Modeling-Recurrent-and-Recursive-Nets/">2017-02-25</a></time>
      
      
  
    <h1 class="title">Chapter 10 Sequence Modeling: Recurrent and Recursive Nets</h1>
  

    </header>
    <div class="entry">
      
        <p>最近比较浮躁，书读不进去，总结也写不下去。虽然感觉有很多事情要做，却有种分不清主次的感觉。啊啊啊啊啊啊啊啊，什么时候才是个头啊！！！！！！</p>
<p>这一 Chapter 的很多内容都没有仔细的去看，所以这个总结里的内容也很少，会在后面慢慢的补充上来 -_- </p>
<p>首先，介绍了<code>RNN</code>的定义：</p>
<blockquote>
<p>Recurrent neural networks or RNNs (Rumelhart et al., 1986a) are a family of neural networks for processing sequential data. specialized for processing a sequence of values x<sup>(1)</sup>, . . . , x<sup>(τ)</sup>.</p>
</blockquote>
<p>也就是在每一步都有新的输入，所以从某种角度它也算是一种<code>Feedforward Networks</code>。</p>
<p>另外，这里介绍了<code>RNN</code>的<code>parameter sharing</code>机制：Each member of the output is produced using the <strong>same update rule</strong> applied to the <strong>previous outputs</strong>. This recurrent formulation results in the sharing of parameters through a very deep <strong>computational graph</strong>.</p>
<p>Sharing Parameters is key to RNNs。书中举得 “I went to Nepal in 1999 ” and “In 1999, I went to Nepal ”的例子就是为了说明这个重要性。</p>
<a id="more"></a>

<p><font color='red'>注意</font> ：</p>
<ul>
<li><p>上面指出了是在一个<code>computational graph</code>上进行的<code>parameter sharing</code>，在整个过程中相同的 units 都是只对同一个 parameter（可能有不同种类的parameters，比如：<code>W</code>、<code>U</code>等）进行更新。</p>
</li>
<li><p><code>CNN</code>中的<code>parameter sharing</code>是通过<code>kernel</code>实现的，与<code>RNN</code>的是不同的。</p>
</li>
</ul>
<h2 id="10-1-Unfolding-Computational-Graphs"><a href="#10-1-Unfolding-Computational-Graphs" class="headerlink" title="10.1 Unfolding Computational Graphs"></a>10.1 Unfolding Computational Graphs</h2><ul>
<li><p>这里主要讲<code>RNN</code>的定义，以及如何把<code>RNN</code>的<strong>递归定义</strong>展开成 chain 形式。</p>
</li>
<li><p>并不需要过去<strong>所有</strong>的信息，只需要<strong>足够</strong>的信息就可以了。</p>
</li>
</ul>
<h2 id="10-2-Recurrent-Neural-Networks"><a href="#10-2-Recurrent-Neural-Networks" class="headerlink" title="10.2 Recurrent Neural Networks"></a>10.2 Recurrent Neural Networks</h2><ul>
<li><p>Important <strong>design patterns</strong> for recurrent neural networks</p>
<ul>
<li><p>produce an output at each time step and have recurrent connections between <strong>hidden units</strong></p>
<center>
<img src="https://www.tuchuang001.com/images/2017/03/18/QQ20170318173519.png" alt="Hidden Units --> Hidden Units" border="0" />
</center>

<ul>
<li><p>can choose to put any information it wants about the past into its hidden representation h and transmit h to the future. </p>
</li>
<li><p>书中的例子都是按照这个来讲的。同时，注意整个过程都是保持对<strong>同一个</strong><code>W</code>、<code>U</code>进行更新。</p>
</li>
</ul>
</li>
<li><p>produce an output at each time step and have recurrent connections <strong>only</strong> from the <strong>output at one time step to the hidden units at the next time step</strong></p>
<center>
<img src="https://www.tuchuang001.com/images/2017/03/18/QQ20170318173349.png" alt="Output --> Hidden Units" border="0" />
</center>

<ul>
<li><p><strong>Less powerful</strong> (can express a smaller set of functions) than those in the family represented by figure 10.3. </p>
<p>Because this network lacks hidden-to-hidden recurrence, it requires that the output units capture all of the information about the past that the network will use to predict the future. Because the output units are explicitly trained to match the training set targets, they are unlikely to capture the necessary information about the past history of the input</p>
</li>
<li><p>Training can thus be <strong>parallelized</strong>, with the gradient for each step t computed in isolation. There is no need to compute the output for the previous time step first, because the training set provides the ideal value of that output.</p>
</li>
</ul>
</li>
<li><p>recurrent connections between <strong>hidden units</strong>, that read an entire sequence and then produce <strong>a single output</strong></p>
<center>
<img src="https://www.tuchuang001.com/images/2017/03/18/QQ20170318173830.png" alt="Single output" border="0" />
</center>

<ul>
<li>Such a network can be used to <strong>summarize a sequence</strong> and produce a <strong>fixed-size representation</strong> used as input for further processing. There might be a target right at the end (as depicted here) or the gradient on the output O<sup>(t)</sup> can be obtained by back-propagating from further downstream modules.</li>
</ul>
</li>
</ul>
</li>
</ul>
<ul>
<li><strong>any function</strong> computable by a <strong>Turing machine</strong> can be computed by such a recurrent network of a <strong>finite size</strong>. </li>
</ul>
<h3 id="10-2-1-Teacher-Forcing-and-Networks-with-Output-Recurrence"><a href="#10-2-1-Teacher-Forcing-and-Networks-with-Output-Recurrence" class="headerlink" title="10.2.1 Teacher Forcing and Networks with Output Recurrence"></a>10.2.1 Teacher Forcing and Networks with Output Recurrence</h3><ul>
<li><p>Models that have recurrent connections <strong>from their outputs leading back into the model</strong> may be trained with <strong>teacher forcing</strong>. </p>
</li>
<li><p><strong>Teacher forcing</strong> is a procedure that emerges from the maximum likelihood criterion, in which during <strong>training</strong> the model receives the <strong>ground truth output y(t) as input</strong> at time t + 1.</p>
<center>
<img src="https://www.tuchuang001.com/images/2017/03/18/QQ20170318175455.png" alt="Teacher Forcing" border="0" />
</center>

<ul>
<li><p>(Left) At train time, we feed the correct output y<sup>(t)</sup> drawn from the train set as input to h<sup>(t+1)</sup>. </p>
</li>
<li><p>(Right) When the model is deployed, the true output is generally not known. In this case, we approximate the correct output y<sup>(t)</sup> with the model’s output O<sup>(t)</sup>, and feed the output back into the model.</p>
</li>
</ul>
</li>
<li><p>The disadvantage of strict teacher forcing arises if the network is going to be later used in an open-loop mode, with the network outputs (or samples from the output distribution) fed back as input. </p>
</li>
</ul>
<h3 id="10-2-2-Computing-the-Gradient-in-a-Recurrent-Neural-Network"><a href="#10-2-2-Computing-the-Gradient-in-a-Recurrent-Neural-Network" class="headerlink" title="10.2.2 Computing the Gradient in a Recurrent Neural Network"></a>10.2.2 Computing the Gradient in a Recurrent Neural Network</h3><ul>
<li>主要讲计算梯度。</li>
</ul>
<h3 id="10-2-3-Recurrent-Networks-as-Directed-Graphical-Models"><a href="#10-2-3-Recurrent-Networks-as-Directed-Graphical-Models" class="headerlink" title="10.2.3 Recurrent Networks as Directed Graphical Models"></a>10.2.3 Recurrent Networks as Directed Graphical Models</h3><ul>
<li><p>主要是把<code>RNN</code>作为一种<code>Graphical Models</code>来讲解。</p>
</li>
<li><p>没弄懂这么做有什么好处，易于理解？</p>
</li>
</ul>
<h3 id="10-2-4-Modeling-Sequences-Conditioned-on-Context-with-RNNs"><a href="#10-2-4-Modeling-Sequences-Conditioned-on-Context-with-RNNs" class="headerlink" title="10.2.4 Modeling Sequences Conditioned on Context with RNNs"></a>10.2.4 Modeling Sequences Conditioned on Context with RNNs</h3><ul>
<li><p>把一个 fixed-size vector 转化为 sequence 的方法：</p>
<ul>
<li><p>as an extra input at each time step</p>
</li>
<li><p>as the initial state h<sup>(0)</sup></p>
</li>
<li><p>both</p>
</li>
</ul>
</li>
</ul>
<h2 id="10-3-Bidirectional-RNNs"><a href="#10-3-Bidirectional-RNNs" class="headerlink" title="10.3 Bidirectional RNNs"></a>10.3 Bidirectional RNNs</h2><ul>
<li><p>Prediction of y which may depend on <strong>whole input sequence</strong>。 </p>
</li>
<li><p>Bidirectional RNNs combine an RNN that moves forward through time beginning from the start of the sequence with another RNN that moves backward through time beginning from the end of the sequence.</p>
<center>
<img src="https://www.tuchuang001.com/images/2017/03/18/QQ20170318184133.png" alt="Bidirectional RNNs" border="0" />
</center>

<ul>
<li><p>Computation of a typical bidirectional recurrent neural network, meant to learn to map input sequences x to target sequences y, with loss L<sup>(t)</sup> at each step t. </p>
</li>
<li><p>The <strong>h</strong> recurrence propagates information <strong>forward</strong> in time (towards the right) while the <strong>g</strong> recurrence propagates information <strong>backward</strong> in time (towards the left). </p>
</li>
<li><p>Thus at each point t, the output units o<sup>(t)</sup> can benefit from a relevant summary of the <strong>past in its h<sup>(t)</sup> input</strong> and from a relevant summary of the <strong>future in its g<sup>(t)</sup> input</strong>.</p>
</li>
</ul>
</li>
<li><p>This allows the output units o<sup>(t)</sup> to compute a representation that depends on <strong>both the past and the future</strong> but is most sensitive to the input values around time t, without having to specify a fixed-size window around t (as one would have to do with a feedforward network, a convolutional network, or a regular RNN with a fixed-size look-ahead buffer).</p>
</li>
<li><p>也可以用于更高维的数据，如：2-D 图像。（This idea can be naturally extended to 2-dimensional input, such as images, by having four RNNs, each one going in one of the four directions: up, down, left, right.）</p>
</li>
</ul>
<h2 id="10-4-Encoder-Decoder-Sequence-to-Sequence-Architectures"><a href="#10-4-Encoder-Decoder-Sequence-to-Sequence-Architectures" class="headerlink" title="10.4 Encoder-Decoder Sequence-to-Sequence Architectures"></a>10.4 Encoder-Decoder Sequence-to-Sequence Architectures</h2>  <center>
  <img src="https://www.tuchuang001.com/images/2017/03/14/QQ20170312174433.png" alt="Sequence-to-sequence architecture" border="0" />
  </center>

<ul>
<li><p>作用：map an input sequence to an output sequence which is not necessarily of the same length.</p>
</li>
<li><p>主要分为 2 部分：</p>
<ul>
<li><p>Encoder：把 inputs 转化为 final hidden state （书中称作 context C）的 function。注意，这里的最终输出是一个 <strong>fixed-length vector</strong>，所以有缺点，这也是接下来的 attention 机制要改进的地方。</p>
</li>
<li><p>Decoder：conditioned on that fixed-length vector to generate the output sequence</p>
</li>
</ul>
</li>
<li><p>There is no constraint that the encoder must have the same size of hidden layer as the decoder</p>
</li>
<li><p>Limitation：when the context C output by the encoder RNN has a dimension that is too small to properly summarize a long sequence</p>
<ul>
<li><p>解决：attention mechanism，make C a variable-length sequence rather than a fixed-size vector</p>
</li>
<li><p><a href="https://www.zhihu.com/question/36591394" target="_blank" rel="noopener">Attention based model 是什么，它解决了什么问题？ - 知乎</a></p>
</li>
</ul>
</li>
<li><p>非常推荐看一下<strong>参考文献 1</strong> 中这一节的内容。</p>
</li>
</ul>
<h2 id="10-5-Deep-Recurrent-Networks"><a href="#10-5-Deep-Recurrent-Networks" class="headerlink" title="10.5 Deep Recurrent Networks"></a>10.5 Deep Recurrent Networks</h2><ul>
<li><p>Need enough depth in order to perform the required mappings。</p>
</li>
<li><p>讲了如何构建深层<code>RNN</code>，如下图所示，具体可以见书中内容（<font color='red'>没看懂想要表达什么意思 -_-</font>）。</p>
<center>
<img src="https://www.tuchuang001.com/images/2017/03/18/QQ20170318235210.png" alt="Decomposing the state of an RNN into multiple layers" border="0" />
</center>

<ul>
<li><p>(a) The hidden recurrent state can be broken down into groups organized hierarchically. </p>
</li>
<li><p>(b) Deeper computation (e.g., an MLP) can be introduced in the input-to-hidden, hidden-to-hidden and hidden-to-output parts. This may lengthen the shortest path linking different time steps. </p>
</li>
<li><p>(c) The path-lengthening effect can be mitigated by introducing skip connections.</p>
</li>
</ul>
</li>
</ul>
<h2 id="10-6-Recursive-Neural-Networks"><a href="#10-6-Recursive-Neural-Networks" class="headerlink" title="10.6 Recursive Neural Networks"></a>10.6 Recursive Neural Networks</h2><ul>
<li><p>与<code>RNN</code>有很大的区别，<code>RNN</code>主要还是<strong>链式结构</strong>（在时间维度上），而<code>Recursive Neural Networks</code>却是 tree 结构的。</p>
</li>
<li><p>相比较<code>RNN</code>的一大<strong>优势</strong>是，可以和把 tree 的深度变成<code>O(lg n)</code>。</p>
</li>
<li><p>至于如何构建 tree，可能需要更多的 domain knowledge。</p>
</li>
</ul>
<h2 id="10-7-The-Challenge-of-Long-Term-Dependet’r’re’er’rncies"><a href="#10-7-The-Challenge-of-Long-Term-Dependet’r’re’er’rncies" class="headerlink" title="10.7 The Challenge of Long-Term Dependet’r’re’er’rncies"></a>10.7 The Challenge of Long-Term Dependet’r’re’er’rncies</h2><ul>
<li><p>主要讲了<strong>梯度爆炸</strong>和<strong>梯度消失</strong>的问题。</p>
</li>
<li><p>感觉目前解决这个问题的还是<code>LSTM</code>，其它的说了也没有什么帮助。</p>
</li>
</ul>
<h2 id="10-8-Echo-State-Networks"><a href="#10-8-Echo-State-Networks" class="headerlink" title="10.8 Echo State Networks"></a>10.8 Echo State Networks</h2><ul>
<li><p>可以看 <a href="https://en.wikipedia.org/wiki/Echo_state_network" target="_blank" rel="noopener">wikipedia</a> 和 <a href="http://www.scholarpedia.org/article/Echo_state_network" target="_blank" rel="noopener">这个</a>，比书里讲的容易理解些。</p>
</li>
<li><p>这个还是有很多需要理解的东西的，不过奈何理解能力不够，没有看懂。</p>
</li>
</ul>
<h2 id="10-9-Leaky-Units-and-Other-Strategies-for-Multiple-Time-Scales"><a href="#10-9-Leaky-Units-and-Other-Strategies-for-Multiple-Time-Scales" class="headerlink" title="10.9 Leaky Units and Other Strategies for Multiple Time Scales"></a>10.9 Leaky Units and Other Strategies for Multiple Time Scales</h2><ul>
<li><p><strong>目的</strong>还是为了解决 long-term dependencies，从而能够更多、更好的利用过去的信息。</p>
</li>
<li><p>不过还是感觉没有什么帮助，<code>LSTM</code>挑大梁。</p>
</li>
<li><p>这里讲的方法都遵循了一个思想：设计一个可以处理不同 time scales 的模型，让模型的一部分在细粒度的 time scales 下进行操作，以期捕获<strong>细节</strong>；其它的部分在粗粒度的 time scales 下进行操作，以期能够更高效地把<strong>较远的过去信息</strong>用到现在的操作中。（One way to deal with long-term dependencies is to design a model that operates at <strong>multiple time scales</strong>, so that some parts of the model operate at <strong>fine-grained time scales</strong> and can handle <strong>small details</strong>, while other parts operate at <strong>coarse time scales</strong> and transfer information from the distant past to the present more efficiently.）</p>
</li>
<li><p>除了Skip Connections，<font color='red'>其它都没有看太懂 -_-</font> 。</p>
</li>
</ul>
<h3 id="10-9-1-Adding-Skip-Connections-through-Time"><a href="#10-9-1-Adding-Skip-Connections-through-Time" class="headerlink" title="10.9.1 Adding Skip Connections through Time"></a>10.9.1 Adding Skip Connections through Time</h3><ul>
<li>与现在<code>CNN</code>中的 Skip Connections 差不多，就是直接把过去的某个时间 t 的信息直接连到当前时间。（One way to obtain coarse time scales is to add direct connections from variables in the <strong>distant</strong> past to variables in the present.）</li>
</ul>
<h3 id="10-9-2-Leaky-Units-and-a-Spectrum-of-Different-Time-Scales"><a href="#10-9-2-Leaky-Units-and-a-Spectrum-of-Different-Time-Scales" class="headerlink" title="10.9.2 Leaky Units and a Spectrum of Different Time Scales"></a>10.9.2 Leaky Units and a Spectrum of Different Time Scales</h3><ul>
<li><p>Another way to obtain paths on which the product of derivatives is close to one is to have units with linear self-connections and a weight near one on these connections.</p>
</li>
<li><p>参数需要人工的指定。</p>
</li>
</ul>
<h3 id="10-9-3-Removing-Connections"><a href="#10-9-3-Removing-Connections" class="headerlink" title="10.9.3 Removing Connections"></a>10.9.3 Removing Connections</h3><ul>
<li>注意与 Skip Connections 的不同。</li>
</ul>
<h2 id="10-10-The-Long-Short-Term-Memory-and-Other-Gated-RNNs"><a href="#10-10-The-Long-Short-Term-Memory-and-Other-Gated-RNNs" class="headerlink" title="10.10 The Long Short-Term Memory and Other Gated RNNs"></a>10.10 The Long Short-Term Memory and Other Gated RNNs</h2><ul>
<li><p>主要介绍为了<strong>解决</strong> long-term dependencies 的问题，而被提出来的几种<strong>网络结构</strong>。</p>
</li>
<li><p>主要<strong>思路</strong>：有时候把<strong>用过</strong>的信息<strong>丢掉</strong>也许会更好。</p>
</li>
<li><p>让<code>neural networks</code><strong>自己决定</strong>丢掉哪些内容，掉丢多少，什么时候丢掉：<code>gate function</code>。</p>
</li>
</ul>
<h3 id="10-10-1-LSTM"><a href="#10-10-1-LSTM" class="headerlink" title="10.10.1 LSTM"></a>10.10.1 LSTM</h3><ul>
<li>没有仔细看这小节的内容。可以看一下<strong>参考文献 4</strong>，写的很通俗易懂。</li>
</ul>
<h3 id="10-10-2-Other-Gated-RNNs"><a href="#10-10-2-Other-Gated-RNNs" class="headerlink" title="10.10.2 Other Gated RNNs"></a>10.10.2 Other Gated RNNs</h3><ul>
<li>没有仔细看。</li>
</ul>
<h2 id="10-11-Optimization-for-Long-Term-Dependencies"><a href="#10-11-Optimization-for-Long-Term-Dependencies" class="headerlink" title="10.11 Optimization for Long-Term Dependencies"></a>10.11 Optimization for Long-Term Dependencies</h2><ul>
<li>主要介绍为了<strong>解决</strong> long-term dependencies 而产生的 exploding gradients 与 vanishing gradients。</li>
</ul>
<h3 id="10-11-1-Clipping-Gradients"><a href="#10-11-1-Clipping-Gradients" class="headerlink" title="10.11.1 Clipping Gradients"></a>10.11.1 Clipping Gradients</h3><ul>
<li>为了解决 exploding gradients。</li>
</ul>
<h3 id="10-11-2-Regularizing-to-Encourage-Information-Flow"><a href="#10-11-2-Regularizing-to-Encourage-Information-Flow" class="headerlink" title="10.11.2 Regularizing to Encourage Information Flow"></a>10.11.2 Regularizing to Encourage Information Flow</h3><ul>
<li>为了解决 vanishing gradients。</li>
</ul>
<h2 id="10-12-Explicit-Memory"><a href="#10-12-Explicit-Memory" class="headerlink" title="10.12 Explicit Memory"></a>10.12 Explicit Memory</h2><ul>
<li>没看懂</li>
</ul>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ol>
<li><p><a href="http://datamining.uos.ac.kr/wp-content/uploads/2016/09/DeepLearningBook_10.3_10.8.pdf" target="_blank" rel="noopener">DeepLearningBook_10.3_10.9</a></p>
</li>
<li><p><a href="http://www.cedar.buffalo.edu/~srihari/CSE676/SequenceModeling.pdf" target="_blank" rel="noopener">SequenceModeling.ppt</a></p>
</li>
<li><p><a href="http://www.control.lth.se/media/Education/DoctorateProgram/2016/Deep%20Learning/dl_rnn.pdf" target="_blank" rel="noopener">Deep Learning - Study Circle Sequence Modeling: Recurrent and Recursive Nets</a></p>
</li>
<li><p><a href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/" target="_blank" rel="noopener">Understanding LSTM Networks</a></p>
</li>
</ol>

      
    </div>
    <footer>
      
        
  
  <div class="categories">
    <a href="/categories/Machine-Learning/">Machine Learning</a>
  </div>

        
  
  <div class="tags">
    <a href="/tags/Deep-Learning/">Deep Learning</a>
  </div>

        
  <div class="addthis addthis_toolbox addthis_default_style">
    
      <a class="addthis_button_facebook_like" fb:like:layout="button_count"></a>
    
    
      <a class="addthis_button_tweet"></a>
    
    
      <a class="addthis_button_google_plusone" g:plusone:size="medium"></a>
    
    
      <a class="addthis_button_pinterest_pinit" pi:pinit:layout="horizontal"></a>
    
    <a class="addthis_counter addthis_pill_style"></a>
  </div>
  <script type="text/javascript" src="//s7.addthis.com/js/300/addthis_widget.js"></script>

      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>


<section id="comment">
  <h1 class="title">Comments</h1>

  
  <div id="disqus_thread">
    <noscript>Please enable JavaScript to view the <a href="//disqus.com/?ref_noscript" target="_blank" rel="noopener">comments powered by Disqus.</a></noscript>
  </div>
  
</section>

</div></div>
    <aside id="sidebar" class="alignright">
  <div class="search">
  <form action="//google.com/search" method="get" accept-charset="utf-8">
    <input type="search" name="q" results="0" placeholder="Search">
    <input type="hidden" name="q" value="site:watsonyanghx.github.io">
  </form>
</div>

  
<div class="widget tag">
  <h3 class="title">Categories</h3>
  <ul class="entry">
  
    <li><a href="/categories/Algorithm/">Algorithm</a><small>2</small></li>
  
    <li><a href="/categories/C/">C++</a><small>1</small></li>
  
    <li><a href="/categories/Computer-Vision/">Computer Vision</a><small>3</small></li>
  
    <li><a href="/categories/Deep-Learning/">Deep Learning</a><small>3</small></li>
  
    <li><a href="/categories/Interview/">Interview</a><small>1</small></li>
  
    <li><a href="/categories/Machine-Learning/">Machine Learning</a><small>8</small></li>
  
    <li><a href="/categories/NLP/">NLP</a><small>1</small></li>
  
    <li><a href="/categories/OJ/">OJ</a><small>48</small></li>
  
    <li><a href="/categories/Paper/">Paper</a><small>5</small></li>
  
    <li><a href="/categories/Reinforcement-Learning/">Reinforcement Learning</a><small>1</small></li>
  
    <li><a href="/categories/Technique-Summaries/">Technique Summaries</a><small>2</small></li>
  
    <li><a href="/categories/Tools/">Tools</a><small>1</small></li>
  
    <li><a href="/categories/WhatEver/">WhatEver</a><small>1</small></li>
  
  </ul>
</div>


  
<div class="widget tagcloud">
  <h3 class="title">Tag Cloud</h3>
  <div class="entry">
    <a href="/tags/Algorithm/" style="font-size: 18.18px;">Algorithm</a> <a href="/tags/BFS/" style="font-size: 11.82px;">BFS</a> <a href="/tags/Backtracking/" style="font-size: 12.73px;">Backtracking</a> <a href="/tags/Binary-Tree/" style="font-size: 14.55px;">Binary Tree</a> <a href="/tags/C/" style="font-size: 10px;">C++</a> <a href="/tags/CNN/" style="font-size: 15.45px;">CNN</a> <a href="/tags/CS231n/" style="font-size: 11.82px;">CS231n</a> <a href="/tags/Combinatorial-number/" style="font-size: 10.91px;">Combinatorial number</a> <a href="/tags/DFS/" style="font-size: 14.55px;">DFS</a> <a href="/tags/DP/" style="font-size: 19.09px;">DP</a> <a href="/tags/Data-structure/" style="font-size: 10.91px;">Data structure</a> <a href="/tags/DeconvNet/" style="font-size: 10px;">DeconvNet</a> <a href="/tags/Deep-Learning/" style="font-size: 17.27px;">Deep Learning</a> <a href="/tags/Disjoint-Set/" style="font-size: 11.82px;">Disjoint Set</a> <a href="/tags/Divide-and-Conquer/" style="font-size: 13.64px;">Divide and Conquer</a> <a href="/tags/GAN/" style="font-size: 10.91px;">GAN</a> <a href="/tags/Graph/" style="font-size: 13.64px;">Graph</a> <a href="/tags/Greedy/" style="font-size: 11.82px;">Greedy</a> <a href="/tags/Hexo/" style="font-size: 10px;">Hexo</a> <a href="/tags/HihoCoder/" style="font-size: 16.36px;">HihoCoder</a> <a href="/tags/IPv6/" style="font-size: 10px;">IPv6</a> <a href="/tags/LintCode/" style="font-size: 20px;">LintCode</a> <a href="/tags/Machine-Learning/" style="font-size: 10.91px;">Machine Learning</a> <a href="/tags/Matrix/" style="font-size: 10px;">Matrix</a> <a href="/tags/NLP/" style="font-size: 10px;">NLP</a> <a href="/tags/Normalization/" style="font-size: 10px;">Normalization</a> <a href="/tags/Notes/" style="font-size: 10px;">Notes</a> <a href="/tags/Numpy/" style="font-size: 10px;">Numpy</a> <a href="/tags/POJ/" style="font-size: 10.91px;">POJ</a> <a href="/tags/Permutation/" style="font-size: 10px;">Permutation</a> <a href="/tags/Python/" style="font-size: 10px;">Python</a> <a href="/tags/Queue/" style="font-size: 10.91px;">Queue</a> <a href="/tags/RL/" style="font-size: 10px;">RL</a> <a href="/tags/Recursion/" style="font-size: 12.73px;">Recursion</a> <a href="/tags/ResNet/" style="font-size: 10px;">ResNet</a> <a href="/tags/Shadowsocks/" style="font-size: 10px;">Shadowsocks</a> <a href="/tags/String/" style="font-size: 11.82px;">String</a> <a href="/tags/VPS/" style="font-size: 10px;">VPS</a> <a href="/tags/WhatEver/" style="font-size: 10px;">WhatEver</a>
  </div>
</div>

</aside>
    <div class="clearfix"></div>
  </div>
  <footer id="footer" class="inner"><div class="alignleft">
  
  &copy; 2020 Watson Yang
  
</div>
<div class="clearfix"></div></footer>
  <script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>
<script src="/js/jquery.imagesloaded.min.js"></script>
<script src="/js/gallery.js"></script>


<script type="text/javascript">
var disqus_shortname = 'light-disqus';

(function(){
  var dsq = document.createElement('script');
  dsq.type = 'text/javascript';
  dsq.async = true;
  dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
  (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
}());
</script>



<link rel="stylesheet" href="/fancybox/jquery.fancybox.css" media="screen" type="text/css">
<script src="/fancybox/jquery.fancybox.pack.js"></script>
<script type="text/javascript">
(function($){
  $('.fancybox').fancybox();
})(jQuery);
</script>


<div id="totop" style="position:fixed;bottom:35px;right:20px;cursor: pointer;">
  <a title="返回顶部"><i class="fa fa-angle-double-up fa-3x" aria-hidden="true"></i></a>
</div>
<script src="/js/totop.js"></script>

</body>
</html>
