<!DOCTYPE HTML>
<html>

<head>
  <meta name="google-site-verification" content="6-Grm86DHfS67XSHMuvfO8cIcQcOWPpjGvOBMxbA3Fo" />
  <meta charset="utf-8">
  
  <title>NOTES | WatsonYang&#39;s Blog</title>
  
  <meta name="author" content="Watson Yang">
  
  <meta name="description" content="My secret private space.">
  
  
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

  
  <meta property="og:title" content="NOTES" />
  <meta property="og:site_name" content="WatsonYang&#39;s Blog" />

  
  <meta property="og:image" content="" />
  

  <link href="/favicon.png" rel="icon">
  <link rel="alternate" href="/atom.xml"
    title="WatsonYang&#39;s Blog" type="application/atom+xml">
  <link rel="stylesheet" href="/css/style.css" media="screen" type="text/css">
  <!--[if lt IE 9]><script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script><![endif]-->
  


  <a href="https://github.com/watsonyanghx" target="_blank">
    <img style="position: absolute; top: 0; left: 0; border: 0;"
      src="https://s3.amazonaws.com/github/ribbons/forkme_left_gray_6d6d6d.png" alt="Fork me on GitHub">
  </a>
  <script src="https://use.fontawesome.com/4eb261e456.js"></script>

  <script>
    MathJax = {
      tex: {
        inlineMath: [
          ['$', '$'],
          ['\\(', '\\)']
        ]
      },
      svg: {
        fontCache: 'global'
      }
    };
  </script>
  <script type="text/javascript" id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js">
  </script>
<meta name="generator" content="Hexo 4.2.0"></head>

<body>
  <header id="header" class="inner"><div class="alignleft">
  <h1><a href="/">WatsonYang&#39;s Blog</a></h1>
  <h2><a href="/">Enrich yourself.</a></h2>
</div>
<nav id="main-nav" class="alignright">
  <ul>
    
      <li><a href="/">Home</a></li>
    
      <li><a href="/archives">Archives</a></li>
    
      <li><a href="/about">About</a></li>
    

    <li> <a href="/atom.xml">RSS</a> </li>
  </ul>
  <div class="clearfix"></div>
</nav>
<div class="clearfix"></div>
</header>
  <div id="content" class="inner">
    <div id="main-col" class="alignleft"><div id="wrapper"><article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2020-04-25T08:31:17.000Z"><a href="/2020/04/25/NOTES/">2020-04-25</a></time>
      
      
  
    <h1 class="title">NOTES</h1>
  

    </header>
    <div class="entry">
      
        <h3 id="基于生成对抗网络的网页显著度图预测"><a href="#基于生成对抗网络的网页显著度图预测" class="headerlink" title="基于生成对抗网络的网页显著度图预测"></a>基于生成对抗网络的网页显著度图预测</h3><ul>
<li><p>使用 GAN(生成对抗网络)生成给定网页的显著度图，从而预测网页的哪些区域更会受到用户的关注。</p>
</li>
<li><p>把网页快照和显著度图当作不同域别的图像，并使用 GAN 进行转换。设计多种不同结构的网络模型，加入自注意力机制与谱归一化，引入网页特征用于指导显著度图的生成，同时试用多种损失函数，并使用 TTUR 的方式对网络进行训练。</p>
</li>
<li><p>使用的网络结构：</p>
<ul>
<li><p>resnet</p>
</li>
<li><p>u-net</p>
<a id="more"></a>
</li>
<li><p>预训练的 VGG 网络提取特征（不对 VGG 进行微调/微调最后几层）</p>
</li>
<li><p>加入 attention</p>
</li>
</ul>
</li>
<li><p>使用的网络结构：</p>
<ul>
<li><p>Patch GAN</p>
</li>
<li><p>hinge</p>
</li>
<li><p>pixle wise 的 cross entropy</p>
</li>
<li><p>风格损失的</p>
</li>
</ul>
</li>
<li><p>效果：</p>
<ul>
<li><p>超过已有的用来预测 webpage saliency 的方法，但是超出的不是太多</p>
</li>
<li><p>和在图像上的方法差不多（有的指标相对较高，有的指标相对较低）</p>
</li>
</ul>
</li>
<li><p>指标</p>
<ul>
<li><p><a href="https://sites.google.com/site/saliencyevaluation/evaluation-measures" target="_blank" rel="noopener">Evaluation measures - SaliencyEvaluation</a></p>
</li>
<li><p><a href="https://arxiv.org/abs/1604.03605v2" target="_blank" rel="noopener">What do different evaluation metrics tell us about saliency models?</a></p>
</li>
</ul>
</li>
</ul>
<h3 id="细粒度的，跨域别的根据文本描述生成图像的研究"><a href="#细粒度的，跨域别的根据文本描述生成图像的研究" class="headerlink" title="细粒度的，跨域别的根据文本描述生成图像的研究"></a>细粒度的，跨域别的根据文本描述生成图像的研究</h3><ul>
<li><p>改进现有工作的不足： 只能生成同一个域别的、粗粒度的图像。</p>
</li>
<li><p>设计网络结构， 并获取更为细粒度的文本描述，然后据此生成与给定的文本描述内容相关的、更加细粒度的图像；设计图像的生成方式，使得单一的一个模型能够同时生成不同域别的图像。</p>
</li>
<li><p>前两篇工作都是自己想一个 idea，然后实现，基本可以说是 creative 的工作。而这项工作是一个 incremental 的工作，目的是为了改进已有的工作。想法是受当时一篇paper的启发：spectral normalization ，它用一个单一的网络结构生成了</p>
</li>
<li><p>现在用于这项工作的数据集有3个：flower、bird、coco。前两个是基本的数据集，后一个难度较大，所以一般都是在前两个数据集上训练模型，到最后在coco数据集上跑一下模型，验证模型比之前的模型好。</p>
</li>
<li><p>第一部分工作</p>
<p>  但是前两个数据集的文本描述都是：this flower，this bird，没有引入类别的信息，比如玫瑰，向日葵等细粒度的类别。所以所做工作的第一部分就是细粒度的生成图像。</p>
<ul>
<li><p>所做工作</p>
</li>
<li><p>对数据集做了一些处理，使得每个文本描述中都有类别信息。</p>
</li>
<li><p>使用 condition batch normalization 来引入细粒度的类别信息；同时，把 text 编码成 embedding 后 和 随机变量 在 channel dimension 上 concat在一起，引入文本的语义信息。</p>
</li>
<li><p>使用 spectral normalization</p>
</li>
<li><p>改进网络结构，resnet / U-Net / U-Net 形式的 resnet</p>
</li>
</ul>
</li>
<li><p>第二部分工作</p>
<ul>
<li><p>现有的方法都是一个模型只能生成一个 domain 的图像。</p>
</li>
<li><p>跨域别的生成：一个模型能够生成多种不同域的图像，即使用一个单一的网络结构同时生成 flower 和 bird。</p>
</li>
<li><p>当时出了一个类似的工作，叫做 StarGAN，但是那个不是真正的跨域别的生成，只是改变了一个属性。</p>
</li>
</ul>
</li>
<li><p>related</p>
<ul>
<li><p><a href="https://github.com/paarthneekhara/text-to-image" target="_blank" rel="noopener">paarthneekhara/text-to-image: Text to image synthesis using thought vectors</a></p>
</li>
<li><p><a href="https://github.com/taoxugit/AttnGAN" target="_blank" rel="noopener">AttnGAN</a></p>
</li>
<li><p><a href="https://github.com/hanzhanggit/StackGAN-Pytorch" target="_blank" rel="noopener">StackGAN-Pytorch</a></p>
</li>
</ul>
</li>
</ul>
<h3 id="基于对偶学习的图片和文本描述的相互转换"><a href="#基于对偶学习的图片和文本描述的相互转换" class="headerlink" title="基于对偶学习的图片和文本描述的相互转换"></a>基于对偶学习的图片和文本描述的相互转换</h3><ul>
<li><p>基于对偶学习的思想，以无监督的方式同时进行图像描述(Image Caption)和图像生成(Text to Image)任务。</p>
</li>
<li><p>把图像描述作为主任务(Primal Task)，图像生成作为对偶任务(Dual Task)； 对于主任务， 使用 CNN + LSTM + Attention，生成图像的文本描述，并使用强化学习中的 REINFORCE 算法训练网络； 对于对偶任务， 把主任务生成的图像的文本描述作为生成器的输入，使用 GAN 生成与文本描述语义相关的图片。</p>
</li>
<li><p>因为要计算 <code>BLEU</code> 等指标，所以必须要把生成的字符 decode 成对应的 word，这个操作是不可导的，所以需要用到 <code>PG</code> 算法来进行训练。这里用到的是 <code>REINFOR</code> 算法。</p>
</li>
<li><p>独立构造数据集，并训练了一个 language model，用来计算文本是否是由人类的置信度。</p>
</li>
</ul>
<h3 id="基于生成对抗网络的图像描述生成"><a href="#基于生成对抗网络的图像描述生成" class="headerlink" title="基于生成对抗网络的图像描述生成"></a>基于生成对抗网络的图像描述生成</h3><ul>
<li><p>Generator</p>
<ul>
<li><p>目标：</p>
</li>
<li><p>能够尽可能的描述图片内容。</p>
</li>
<li><p>生成的 caption 尽量的像人类书写的。</p>
</li>
<li><p>目标函数包括 2 部分（两者的加权和）：</p>
</li>
<li><p>生成的 caption 和 真实的 caption 的 <code>BLEU</code> 值。</p>
</li>
<li><p><code>Discriminator</code> 给出的置信度（文本是否是人类书写）。</p>
</li>
</ul>
</li>
<li><p>Discriminator</p>
<ul>
<li><p>目标：</p>
</li>
<li><p>判断输入文本是否是有人类书写的，并给出置信度。</p>
</li>
<li><p>目标函数：</p>
</li>
<li><p>和传统的 GAN 类似。</p>
</li>
</ul>
</li>
<li><p>和抢发的 ICCV 那篇 paper 的不同：</p>
<ul>
<li><p><code>Discriminator</code> 的输入只是 caption，那篇 paper 的输入是 caption 和 image。</p>
</li>
<li><p><code>Discriminator</code> 的结构用的是 <code>CNN</code> ，那篇 paper 用的是 <code>LSTM + CNN</code>， <code>LSTM</code> 用于对 caption 进行编码，<code>CNN</code> 用于提取 image 的特征。</p>
</li>
<li><p><code>Discriminator</code> 只是用来给出一个文本是人类书写的置信度；那篇文章是把 <code>Discriminator</code> 的输出直接作为生成的 caption 和 image 的语义相关度的置信度（做法参考了 text to image 的那篇 paper）。</p>
</li>
</ul>
</li>
<li><p>抢发的 ICCV 那篇 paper 考虑了更多：</p>
<ul>
<li><p>进行了 paragraph 的生成。</p>
</li>
<li><p>early feedback 的生成方式：evaluate an expected future reward as defined below when the sentence is partially generated。</p>
</li>
</ul>
</li>
</ul>
<h3 id="基于卷积神经网络的短文本聚类算法研究与应用"><a href="#基于卷积神经网络的短文本聚类算法研究与应用" class="headerlink" title="基于卷积神经网络的短文本聚类算法研究与应用"></a>基于卷积神经网络的短文本聚类算法研究与应用</h3><ul>
<li><p>目标：使用 <code>CNN</code> 抽取文本的语义特征， 用来改善短文本聚类的效果。</p>
</li>
<li><p>主要工作：使用有标文本数据训练 <code>CNN</code>， 用于计算文本之间的语义相似度； 使用训练好得到的 <code>CNN</code> 模型计算无标的短文本之间的相似度作为它们之间的距离，根据此距离进行聚类； 最后， 与已有的不同的聚类算法进行对比分析，撰写论文。</p>
</li>
<li><p><code>CNN</code> 的做法是从 <code>文本x</code> 中取 k 个单词，从 <code>文本y</code> 中取 k 个单词，然后对两者组成的矩阵进行一维卷积，得到一个标量，把这个标量按照顺序放到一起，就组成了一个类似于图片的二维矩阵，然后就可以进行类似图片的卷积神经网络了。</p>
</li>
<li><p>效果：和基于 <code>TF-IDF</code> 的 <code>K Means</code> 差不多。</p>
<ul>
<li><p>原因：</p>
</li>
<li><p>对短文本进行 0 padd，可能会产生比较大的影响。</p>
</li>
<li><p>依赖于训练好的 <code>CNN</code> 模型。</p>
</li>
<li><p><code>CNN</code> 是在一个语义匹配的数据集上训练的（因为是有监督的，普通的聚类文本即使有类别label，也没有表示句子匹配程度的lable），聚类实在另外一个数据集上进行，可能会对效果有一定的影响。</p>
</li>
</ul>
</li>
<li><p>数据集</p>
<p>  （1）用卷积神经网络计算不同文本之间语义相似度的时候用的数据集有：MSRP（Microsoft Research Paraphrase Corpus）数据集与SICK（Sentences Involving Compositional Knowledge）数据集。</p>
<p>  MSRP数据集来源于新闻，总共有5801个句子对。其中4076个句子对用于卷积神经网络模型的训练，1725个句子对用于模型准确度的测试。每一个句子对都有一个标签来表征它们的语义相关性：0表示两个句子不相关，1则表示两个句子相关。</p>
<p>  SICK数据集是对图片和视频的文字描述，总共有9927个句子对。其中4500对用于卷积神经网络模型的训练，500对用于模型训练情况的验证，而剩下的4927对用于模型准确度的测试。由于原数据集用了5种标签来表征句子对的语义相关性，与本论文提出的卷积神经网络模型不匹配，故本论文对此数据集进行了处理，使其标签只有两种情况。</p>
<p>  （2）文本聚类算法建模阶段用的是StackOverflow数据集[47]。这是一个由Kaggle整理并开源的短文本数据集，总共有20000个句子，每个句子都有一个标签来表示它所属的类别。总共有20个类别，标号与类别的关系如表4.1所示。</p>
</li>
</ul>
<h3 id="图像中文描述-SCST"><a href="#图像中文描述-SCST" class="headerlink" title="图像中文描述/SCST"></a>图像中文描述/SCST</h3><ul>
<li><p>传统的方法就是取概率最大的那个作为输出，这是一种贪心的做法，但是贪心并不能保证是最好的。Image Caption 可以看做是一个 序列决策（Decision Making） 的问题，这样就可以用强化学习的方法来解决。</p>
<ul>
<li><p>把 <code>生成caption的算法/模型</code> 作为 agent</p>
</li>
<li><p>把 <code>输入的图片特征</code> 和 <code>当前生成的所有 word</code> 作为 agent 所处的状态</p>
</li>
<li><p>生成下一个 word 作为 action</p>
</li>
<li><p>reward 可以自己选择（比如生成的 caption 的 BLEU 值）</p>
<p>用强化学习的好处是：强化</p>
</li>
</ul>
</li>
<li><p><code>Policy Gradient</code> 算法是一种基于策略的强化学习算法，属于无模型的强化学习算法的一种，它直接对策略进行建模。相对于基于值函数的方法，它更为高效，因为不需要再进行 Policy Iteration 这一步。拿这个 Image Caption 模型来说的话就是，这里直接把 <code>网络的输出的概率</code> 作为 这一个时刻 <code>take action 的概率</code>，通过更新网络的参数就可以改变输出的概率的大小，也就是 采取某个 action 的大小，最后当网络收敛后就可以得到一个比较好的策略。</p>
<p>  这里可以讲一下 <code>PG</code> 的不足：因为改变一个参数，会对整个策略产生影响，而reward 是最后才获得的，这就会产生一个问题：比如，在某个决策序列中的某个 action 可能是有益的，但是最终的 reward 却很低，训练的时候可能就会抑制这个 action。但是 reward 是统一的，网络参数的更新也是统一的，这就可能会使得网络训练不容易收敛。正确的做法是：好的 action 应该大多数被鼓励，坏的 action 应该大多数情况下被抑制（不是一直，类似于下象棋，有时候也需要不好的 action）。所以有了几种解决办法：</p>
<ul>
<li><p>给每一个 action 单独的分配 reward。做法是：当前时刻采取 action 后，之后的时刻通过 蒙特卡洛采样 的方法采取 action。多次采样就可以获得多个 reward ，取平均，作为它的期望，然后把得到的 reward 的期望作为这一个时刻的 reward。</p>
</li>
<li><p>算法层面的：TYPO 算法</p>
</li>
</ul>
</li>
<li><p>比赛用到的</p>
<ul>
<li><p>算法：这个比赛利用到的是 IBM 发表的 SCST，他把原始的通过 greedy 方式得到的 caption 作为 baseline，然后把 <code>当前得到的 caption 的 BLEU</code> 与 <code>通过 greedy 方式获得的 caption 的 BLEU</code> 的差值 作为 reward。</p>
</li>
<li><p>特征：</p>
</li>
<li><p>不对图片进行裁剪，直接使用 <code>Resnet-101</code> 最后一个卷积层提取的特征。不对 ResNet 进行网络更新。</p>
</li>
<li><p>对图片进行裁剪，直接使用 <code>Resnet-101</code> 最后一个卷积层提取的特征，但是对 ResNet 进行更新。</p>
</li>
<li><p>维度都设置为 512</p>
</li>
<li><p>image embedding 层数（1层，2层）</p>
</li>
</ul>
</li>
<li><p>D</p>
<p>  在 time step t ，生成了一个 vocabulary size 大小的向量，向量中的每个元素表示每个 word 是 target 输出的概率，这里就把这个概率当做是 take action 的概率。<code>tf.multinomial</code> 就会根据这个概率进行采样，概率越大，越容易被采样到。</p>
<p>  这样就把 <code>每个时刻 word 的生成</code> 转换成了 <code>每个时刻的 take action</code>，就可以当做是 RL 来做了。</p>
<p>  在每个时刻，根据概率的大小，来决定下一个时刻生成的单词（也就是：根据概率的大小，来决定下一个时刻 take 哪个 action，概率越大，对应的那个 action 就越容易被 take。因为policy 的定义就是：在某个状态下采取 action 的概率）。</p>
<p>  更新模型的参数，就会改变每个时刻、生成某个 word 的概率（也就是改变采取某个 action 的概率），这就是直接对 policy 进行建模，也就是 <code>Policy Gradient</code>。</p>
<p>  这时候 reward 的作用是：根据reward，手动的改变梯度的 <strong>大小</strong> 和 <strong>方向</strong>（作用类似 ground truth）。reward 大就增加它的梯度（改变梯度方向，使得 take 这个 action 的概率变大），否则就降低它的梯度（改变梯度方向，使得 take 这个 action 的概率变小）。</p>
</li>
<li><p>从中也可以看出，在每一个 time step 采取 action 后，并不知道这个 action 的好坏，而是在最后通过计算 BLEU 之后才知道（BLEU 越高，这次采取的 <code>action 序列</code> 越好，但是并不能说某一个单独的 action 好）。</p>
</li>
</ul>

      
    </div>
    <footer>
      
        
        
        
  <div class="addthis addthis_toolbox addthis_default_style">
    
      <a class="addthis_button_facebook_like" fb:like:layout="button_count"></a>
    
    
      <a class="addthis_button_tweet"></a>
    
    
      <a class="addthis_button_google_plusone" g:plusone:size="medium"></a>
    
    
      <a class="addthis_button_pinterest_pinit" pi:pinit:layout="horizontal"></a>
    
    <a class="addthis_counter addthis_pill_style"></a>
  </div>
  <script type="text/javascript" src="//s7.addthis.com/js/300/addthis_widget.js"></script>

      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>


<section id="comment">
  <h1 class="title">Comments</h1>

  
  <div id="disqus_thread">
    <noscript>Please enable JavaScript to view the <a href="//disqus.com/?ref_noscript" target="_blank" rel="noopener">comments powered by Disqus.</a></noscript>
  </div>
  
</section>

</div></div>
    <aside id="sidebar" class="alignright">
  <div class="search">
  <form action="//google.com/search" method="get" accept-charset="utf-8">
    <input type="search" name="q" results="0" placeholder="Search">
    <input type="hidden" name="q" value="site:watsonyanghx.github.io">
  </form>
</div>

  
<div class="widget tag">
  <h3 class="title">Categories</h3>
  <ul class="entry">
  
    <li><a href="/categories/Algorithm/">Algorithm</a><small>2</small></li>
  
    <li><a href="/categories/Computer-Vision/">Computer Vision</a><small>3</small></li>
  
    <li><a href="/categories/Deep-Learning/">Deep Learning</a><small>3</small></li>
  
    <li><a href="/categories/Interview/">Interview</a><small>1</small></li>
  
    <li><a href="/categories/Machine-Learning/">Machine Learning</a><small>8</small></li>
  
    <li><a href="/categories/NLP/">NLP</a><small>1</small></li>
  
    <li><a href="/categories/OJ/">OJ</a><small>48</small></li>
  
    <li><a href="/categories/Paper/">Paper</a><small>5</small></li>
  
    <li><a href="/categories/Reinforcement-Learning/">Reinforcement Learning</a><small>1</small></li>
  
    <li><a href="/categories/Technique-Summaries/">Technique Summaries</a><small>2</small></li>
  
    <li><a href="/categories/Tools/">Tools</a><small>1</small></li>
  
    <li><a href="/categories/WhatEver/">WhatEver</a><small>1</small></li>
  
  </ul>
</div>


  
<div class="widget tagcloud">
  <h3 class="title">Tag Cloud</h3>
  <div class="entry">
    <a href="/tags/Algorithm/" style="font-size: 18.18px;">Algorithm</a> <a href="/tags/BFS/" style="font-size: 11.82px;">BFS</a> <a href="/tags/Backtracking/" style="font-size: 12.73px;">Backtracking</a> <a href="/tags/Binary-Tree/" style="font-size: 14.55px;">Binary Tree</a> <a href="/tags/C/" style="font-size: 10px;">C++</a> <a href="/tags/CNN/" style="font-size: 15.45px;">CNN</a> <a href="/tags/CS231n/" style="font-size: 11.82px;">CS231n</a> <a href="/tags/Combinatorial-number/" style="font-size: 10.91px;">Combinatorial number</a> <a href="/tags/DFS/" style="font-size: 14.55px;">DFS</a> <a href="/tags/DP/" style="font-size: 19.09px;">DP</a> <a href="/tags/Data-structure/" style="font-size: 10.91px;">Data structure</a> <a href="/tags/DeconvNet/" style="font-size: 10px;">DeconvNet</a> <a href="/tags/Deep-Learning/" style="font-size: 17.27px;">Deep Learning</a> <a href="/tags/Disjoint-Set/" style="font-size: 11.82px;">Disjoint Set</a> <a href="/tags/Divide-and-Conquer/" style="font-size: 13.64px;">Divide and Conquer</a> <a href="/tags/GAN/" style="font-size: 10.91px;">GAN</a> <a href="/tags/Graph/" style="font-size: 13.64px;">Graph</a> <a href="/tags/Greedy/" style="font-size: 11.82px;">Greedy</a> <a href="/tags/Hexo/" style="font-size: 10px;">Hexo</a> <a href="/tags/HihoCoder/" style="font-size: 16.36px;">HihoCoder</a> <a href="/tags/IPv6/" style="font-size: 10px;">IPv6</a> <a href="/tags/LintCode/" style="font-size: 20px;">LintCode</a> <a href="/tags/Machine-Learning/" style="font-size: 10.91px;">Machine Learning</a> <a href="/tags/Matrix/" style="font-size: 10px;">Matrix</a> <a href="/tags/NLP/" style="font-size: 10px;">NLP</a> <a href="/tags/Normalization/" style="font-size: 10px;">Normalization</a> <a href="/tags/Notes/" style="font-size: 10px;">Notes</a> <a href="/tags/Numpy/" style="font-size: 10px;">Numpy</a> <a href="/tags/POJ/" style="font-size: 10.91px;">POJ</a> <a href="/tags/Permutation/" style="font-size: 10px;">Permutation</a> <a href="/tags/Python/" style="font-size: 10px;">Python</a> <a href="/tags/Queue/" style="font-size: 10.91px;">Queue</a> <a href="/tags/RL/" style="font-size: 10px;">RL</a> <a href="/tags/Recursion/" style="font-size: 12.73px;">Recursion</a> <a href="/tags/ResNet/" style="font-size: 10px;">ResNet</a> <a href="/tags/Shadowsocks/" style="font-size: 10px;">Shadowsocks</a> <a href="/tags/String/" style="font-size: 11.82px;">String</a> <a href="/tags/VPS/" style="font-size: 10px;">VPS</a> <a href="/tags/WhatEver/" style="font-size: 10px;">WhatEver</a>
  </div>
</div>

</aside>
    <div class="clearfix"></div>
  </div>
  <footer id="footer" class="inner"><div class="alignleft">
  
  &copy; 2020 Watson Yang
  
</div>
<div class="clearfix"></div></footer>
  <script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>
<script src="/js/jquery.imagesloaded.min.js"></script>
<script src="/js/gallery.js"></script>


<script type="text/javascript">
var disqus_shortname = 'light-disqus';

(function(){
  var dsq = document.createElement('script');
  dsq.type = 'text/javascript';
  dsq.async = true;
  dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
  (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
}());
</script>



<link rel="stylesheet" href="/fancybox/jquery.fancybox.css" media="screen" type="text/css">
<script src="/fancybox/jquery.fancybox.pack.js"></script>
<script type="text/javascript">
(function($){
  $('.fancybox').fancybox();
})(jQuery);
</script>


<div id="totop" style="position:fixed;bottom:35px;right:20px;cursor: pointer;">
  <a title="返回顶部"><i class="fa fa-angle-double-up fa-3x" aria-hidden="true"></i></a>
</div>
<script src="/js/totop.js"></script>

</body>
</html>
