<!DOCTYPE HTML>
<html>
<head>
  <meta name="google-site-verification" content="6-Grm86DHfS67XSHMuvfO8cIcQcOWPpjGvOBMxbA3Fo" />
  <meta charset="utf-8">
  
  <title>Optimization Method in DL | WatsonYang&#39;s Blog</title>
  <meta name="author" content="Watson Yang">
  
  <meta name="description" content="My secret private space.">
  
  
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

  <meta property="og:title" content="Optimization Method in DL"/>
  <meta property="og:site_name" content="WatsonYang&#39;s Blog"/>

  
    <meta property="og:image" content=""/>
  

  <link href="/favicon.png" rel="icon">
  <link rel="alternate" href="/atom.xml" title="WatsonYang&#39;s Blog" type="application/atom+xml">
  <link rel="stylesheet" href="/css/style.css" media="screen" type="text/css">
  <!--[if lt IE 9]><script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script><![endif]-->
  

  
  <a href="https://github.com/watsonyanghx" target="_blank">
    <img style="position: absolute; top: 0; left: 0; border: 0;" src="https://s3.amazonaws.com/github/ribbons/forkme_left_gray_6d6d6d.png" alt="Fork me on GitHub">
  </a>
  <script src="https://use.fontawesome.com/4eb261e456.js"></script>
<meta name="generator" content="Hexo 4.2.0"></head>


<body>
  <header id="header" class="inner"><div class="alignleft">
  <h1><a href="/">WatsonYang&#39;s Blog</a></h1>
  <h2><a href="/">Enrich yourself.</a></h2>
</div>
<nav id="main-nav" class="alignright">
  <ul>
    
      <li><a href="/">Home</a></li>
    
      <li><a href="/archives">Archives</a></li>
    
      <li><a href="/about">About</a></li>
    

    <li> <a href="/atom.xml">RSS</a> </li>
  </ul>
  <div class="clearfix"></div>
</nav>
<div class="clearfix"></div>
</header>
  <div id="content" class="inner">
    <div id="main-col" class="alignleft"><div id="wrapper"><article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2020-04-25T05:44:32.000Z"><a href="/2020/04/25/Optimization-Method-in-DL/">2020-04-25</a></time>
      
      
  
    <h1 class="title">Optimization Method in DL</h1>
  

    </header>
    <div class="entry">
      
        <h1 id="SGD"><a href="#SGD" class="headerlink" title="SGD"></a>SGD</h1><p>$$<br>\theta_{t+1,i} = \theta_{t,i} - \eta·g_{t,i}<br>$$</p>
<ul>
<li><p>缺点：</p>
<ul>
<li><p>SGD 最大的缺点是下降速度慢。</p>
</li>
<li><p>可能会在沟壑（“盆地”）的两边持续震荡，停留在一个局部最优点，可能无法从中出来，也就无法继续优化。（所以需要细心的初始化参数）。</p>
</li>
<li><p>遇到“鞍点/平原”，可能停止更新（梯度都是 $ 0 $）。</p>
<a id="more"></a>
</li>
<li><p>受初始化影响比较大（初始化不好，可能会遇到“盆地/鞍点/平原”）。</p>
</li>
</ul>
</li>
</ul>
<h1 id="SGD-Momentum"><a href="#SGD-Momentum" class="headerlink" title="SGD-Momentum"></a>SGD-Momentum</h1><ul>
<li><p>在鞍点处的梯度为 $ 0 $，但不是极小点，<code>SGD</code> 此时停留在这里，网络不再学习。为了解决这个问题，想到：可以保留之前一步（不是累计）的更新方向，再用当前的更新方向对其进行微调，这就出现了 <code>SGD-Momentum</code> （动量可以理解为之前的梯度的加权累计）。</p>
</li>
<li><p>受到物理学中的动量的启发：之前的梯度可以作为当前梯度的参考。所以，参数的更新不再只是当前的梯度，而是 当前的梯度 对 之前累积的梯度 进行微调之后得到的梯度。更新公式如下：</p>
</li>
</ul>
<p>$$<br>\begin{align<em>}<br>\begin{split}<br>m_t &amp;= \gamma \cdot m_{t-1} + \nabla_\theta J( \theta) \<br>\theta_{t+1} &amp;= \theta_{t} - \eta \cdot m_t<br>\end{split}<br>\end{align</em>}<br>$$</p>
<p>其中 $γ$ 为动量参数，$η$ 为学习率。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">momentum</span><span class="params">(x_start, step, g, discount = <span class="number">0.7</span>)</span>:</span>   </span><br><span class="line">    x = np.array(x_start, dtype=<span class="string">'float64'</span>)</span><br><span class="line">    pre_grad = np.zeros_like(x)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">50</span>):</span><br><span class="line">        grad = g(x)</span><br><span class="line">        pre_grad = pre_grad * discount + grad</span><br><span class="line">        x -= pre_grad * step</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">print</span> <span class="string">'[ Epoch &#123;0&#125; ] grad = &#123;1&#125;, x = &#123;2&#125;'</span>.format(i, grad, x)</span><br><span class="line">        <span class="keyword">if</span> abs(sum(grad)) &lt; <span class="number">1e-6</span>:</span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line">    <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>


<ul>
<li><p>优点：</p>
<ul>
<li><p>跳出 “盆地/平原/鞍点”。</p>
</li>
<li><p>加速收敛</p>
<ul>
<li>如果参数的某些维度的梯度方向变化不大，那么动量将一直在同一个方向上累积，可以加速更新。</li>
</ul>
</li>
<li><p>减小震荡</p>
<ul>
<li>如果参数的某些维度的梯度方向变化较大，那么之前累计的动量可以（适当的）中和这种变化，从而减少梯度方向变化较大的维度上的更新幅度。</li>
</ul>
</li>
</ul>
</li>
</ul>
<ul>
<li><p>注意：</p>
<ul>
<li><p>lr 越小越稳定。太大了很难收敛到最小值上，但是太小的话收敛就太慢了。</p>
</li>
<li><p>动量参数不能太小，0.9 以上表现比较好，但是又不能太大，太大了无法停留在最小值处。</p>
</li>
</ul>
</li>
</ul>
<h1 id="NAG-Nesterov-Accelerated-Gradient"><a href="#NAG-Nesterov-Accelerated-Gradient" class="headerlink" title="NAG (Nesterov Accelerated Gradient)"></a>NAG (Nesterov Accelerated Gradient)</h1><ul>
<li><code>SGD-Momentum</code> 中，之前积累的动量并没有直接改变当前的梯度，<code>Nesterov</code> 的改进就是让之前的动量直接影响当前的动量。这样做的好处是：。更新公式如下：</li>
</ul>
<p>$$</p>
<p>\begin{align<em>}<br>\begin{split}<br>m_t &amp;= \gamma \cdot m_{t-1} + \nabla_\theta J(\theta_{t} - \eta \cdot m_{t-1}) \<br>\theta_{t+1} &amp;= \theta_{t} - \eta \cdot m_t<br>\end{split}<br>\end{align</em>}</p>
<p>$$</p>
<ul>
<li><p>步骤</p>
<ul>
<li><p>先用之前积累的动量更新参数。</p>
</li>
<li><p>用 更新过的参数的梯度 和 之前积累的梯度（就是动量），进行 SGD-Momentum</p>
</li>
</ul>
</li>
<li><p>Nesterov 项在梯度更新时做出校正，避免前进太快，同时提高灵敏度。(<a href="https://zhuanlan.zhihu.com/p/21486826" target="_blank" rel="noopener">路遥知马力——Momentum</a>)</p>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">nesterov</span><span class="params">(x_start, step, g, discount = <span class="number">0.7</span>)</span>:</span>   </span><br><span class="line">    x = np.array(x_start, dtype=<span class="string">'float64'</span>)</span><br><span class="line">    pre_grad = np.zeros_like(x)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">50</span>):</span><br><span class="line">        x_future = x - step * discount * pre_grad</span><br><span class="line">        grad = g(x_future)</span><br><span class="line">        pre_grad = pre_grad * <span class="number">0.7</span> + grad </span><br><span class="line">        x -= pre_grad * step</span><br><span class="line"></span><br><span class="line">        <span class="keyword">print</span> <span class="string">'[ Epoch &#123;0&#125; ] grad = &#123;1&#125;, x = &#123;2&#125;'</span>.format(i, grad, x)</span><br><span class="line">        <span class="keyword">if</span> abs(sum(grad)) &lt; <span class="number">1e-6</span>:</span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line">    <span class="keyword">return</span> x</span><br><span class="line">nesterov([<span class="number">150</span>,<span class="number">75</span>], <span class="number">0.012</span>, g)</span><br></pre></td></tr></table></figure>

<ul>
<li><p>优点：更快的收敛速度。</p>
<ul>
<li>原因：利用了目标函数二阶导的信息。</li>
</ul>
</li>
<li><p>结论：在原始形式中，Nesterov Accelerated Gradient（NAG）算法相对于 Momentum 的改进在于，以“向前看”看到的梯度而不是当前位置梯度去更新。经过变换之后的等效形式中，NAG 算法相对于 Momentum 多了一个本次梯度相对上次梯度的变化量，这个变化量本质上是对目标函数二阶导的近似。由于利用了二阶导的信息，NAG 算法才会比 Momentum 具有更快的收敛速度。<a href="https://zhuanlan.zhihu.com/p/22810533" target="_blank" rel="noopener">比Momentum更快：揭开Nesterov Accelerated Gradient的真面目</a></p>
</li>
<li><p>参考</p>
<ul>
<li><a href="https://ckmarkoh.github.io/blog/2016/01/16/optimization-method-momentum/" target="_blank" rel="noopener">Gradient Descent With Momentum - Mark Chang’s Blog</a></li>
</ul>
</li>
</ul>
<h1 id="AdaGrad"><a href="#AdaGrad" class="headerlink" title="AdaGrad"></a>AdaGrad</h1><ul>
<li><p>二阶动量：该维度上，迄今为止所有梯度值的平方和。</p>
</li>
<li><p>并没有使用 Momentum，只是对学习率进行了缩放。在 Adagrad 的更新规则中，学习率 η 会随着每次迭代而根据历史梯度的变化而变化。</p>
</li>
</ul>
<p>$$<br>\begin{align<em>}<br>\begin{split}<br>v_t &amp;= v_{t-1} + g_{t,i}^2 \<br>\theta_{t+1,i} &amp;= \theta_{t,i} - \frac{\eta}{\sqrt{v_t+\epsilon}}·g_{t,i}<br>\end{split}<br>\end{align</em>}<br>$$</p>
<p>$ v_t \in {\Bbb R}^{d \times d} $ 是一个对角矩阵，每个对角线位置 $i,i$ 的值累加到 $t$ 次迭代的对应参数 $θ_i$ 梯度平方和。$\epsilon$ 是平滑项，防止除零操作，一般取值 $1e−8$ 。为什么分母要进行平方根的原因是去掉平方根操作算法的表现会大打折扣。</p>
<ul>
<li><p>分母作为 Regularizer 项的工作机制如下：</p>
<ul>
<li><p>训练前期，梯度较小，使得 Regularizer 项很大，放大梯度。（对于偶尔更新的参数，我们了解的信息太少，希望能从每个偶然出现的样本身上多学一些，即学习速率大一些。）</p>
</li>
<li><p>训练后期，梯度较大，使得 Regularizer 项很小，缩小梯度。（对于经常更新的参数，我们已经积累了大量关于它的知识，不希望被单个样本影响太大，希望学习速率慢一些。）</p>
</li>
</ul>
</li>
<li><p>另外，由于 Regularizer 是专门针对 Gradient 的，所以有利于解决 Gradient Vanish/Expoloding 问题。所以在深度神经网络中使用会非常不错。</p>
</li>
<li><p>当然，<code>AdaGrad</code> 本身有不少缺陷：</p>
<ul>
<li><p>初始化 $W$ 影响初始化梯度，初始化 $W$ 过大，会导致初始梯度被惩罚得很小。此时可以人工加大 $η$ 的值，但过大的 $η$ 会使得 Regularizer 过于敏感，调节幅度很大。</p>
</li>
<li><p>训练到中后期，递推路径上累加的梯度平方和越大越多，迅速使得 Gradinet 被惩罚逼近 $0$ ，提前结束训练。</p>
</li>
</ul>
</li>
</ul>
<h1 id="RMSprop"><a href="#RMSprop" class="headerlink" title="RMSprop"></a>RMSprop</h1><p>与 AdaGrad 只有一处不同：不是简单的计算之前不同迭代步骤的梯度平方和，而是计算：上一个时刻的加权平均和 和 这个时刻的梯度平方和 的 加权和。（这一点和 Adadelta 很像：不累积全部历史梯度，而只关注过去一段时间窗口的下降梯度。(<a href="https://zhuanlan.zhihu.com/p/32230623" target="_blank" rel="noopener">Adam那么棒，为什么还对SGD念念不忘 (1) —— 一个框架看懂优化算法</a>）</p>
<p>$$<br>\begin{align<em>}<br>\begin{split}<br>v_t &amp;= \gamma \cdot v_{t-1} + (1 -\gamma) \cdot g_{t,i}^2 \<br>\theta_{t+1,i} &amp;= \theta_{t,i} - \frac{\eta}{\sqrt{v_t+\epsilon}}·g_{t,i}<br>\end{split}<br>\end{align</em>}<br>$$</p>
<p>所以，当 $ g{t,i}^2 &lt; v{t-1} $ 的时候，$ v_t $ 将变小，这样就不会出现 AdaGrad 中 $ v_t $ 持续变大，导致后来无法学习的现象。</p>
<ul>
<li><p><a href="https://zhuanlan.zhihu.com/p/34230849" target="_blank" rel="noopener">【优化算法】一文搞懂RMSProp优化算法</a></p>
</li>
<li><p><a href="https://stats.stackexchange.com/questions/181585/adadelta-idea-1-vs-rmsprop" target="_blank" rel="noopener">optimization - Adadelta idea 1 vs RMSprop - Cross Validated</a></p>
</li>
</ul>
<h1 id="Adadelta"><a href="#Adadelta" class="headerlink" title="Adadelta"></a>Adadelta</h1><p>AdaDelta 解決了 AdaGrad 會發生的兩個問題：</p>
<p>(1) Learning Rate 只會隨著時間而一直遞減下去</p>
<p>(2) \triangle x 與 x 的單位不同</p>
<p>更新公式：</p>
<p>$$<br>\begin{align<em>}<br>\begin{split}<br>\theta_{t+1} &amp;= \theta_t + \triangle\theta_t \<br>&amp;= \theta_t - \frac{RMS[\triangle \theta]<em>{t-1}}{RMS[g]_t} \cdot g_t \<br>RMS[g]_t &amp;= \sqrt{E[g^2]_t + \epsilon} \<br>RMS[\triangle \theta]_t &amp;= \sqrt{E[\triangle \theta^2]_t + \epsilon} \<br>E[g^2]_t &amp;= \gamma \cdot E[g^2]</em>{t-1} + (1-\gamma) \cdot g^2_t \<br>E[\triangle\theta_t^2]<em>t &amp;= \gamma \cdot E[\triangle\theta_t^2]</em>{t-1} + (1-\gamma) \cdot \triangle\theta_t^2<br>\end{split}<br>\end{align</em>}<br>$$</p>
<ul>
<li><p>特点</p>
<ul>
<li><p>Adadelta 只累加固定大小的项，并且也不直接存储这些项，仅仅是近似计算对应的平均值</p>
</li>
<li><p>不需要设置初始学习率</p>
</li>
<li><p>训练初中期，加速效果不错，很快</p>
</li>
<li><p>训练后期，反复在局部最小值附近抖动</p>
</li>
</ul>
</li>
<li><p>参考</p>
<ul>
<li><p><a href="https://ckmarkoh.github.io/blog/2016/02/08/optimization-method-adadelta/" target="_blank" rel="noopener">AdaDelta - Mark Chang’s Blog</a></p>
</li>
<li><p><a href="http://cpmarkchang.logdown.com/posts/467674-optimization-method-adadelta" target="_blank" rel="noopener">Optimization Method — AdaDelta « MARK CHANG’S BLOG</a></p>
</li>
<li><p><a href="https://blog.csdn.net/joshuaxx316/article/details/52062291" target="_blank" rel="noopener">梯度下降算法中的Adagrad和Adadelta</a></p>
</li>
<li><p><a href="https://arxiv.org/abs/1212.5701v1" target="_blank" rel="noopener">[1212.5701v1] ADADELTA: An Adaptive Learning Rate Method</a></p>
</li>
</ul>
</li>
</ul>
<h1 id="Adam-Adaptive-Moments"><a href="#Adam-Adaptive-Moments" class="headerlink" title="Adam (Adaptive Moments)"></a>Adam (Adaptive Moments)</h1><p>引入 <code>momentum</code> 。</p>
<p>$$<br>\begin{align<em>}<br>\begin{split}<br>m_t &amp;= beta_1 * m_{t-1} + (1 - beta_1) * g \<br>m_t &amp;= m_t / (1-beta1^t) \<br>v_t &amp;= beta_2 * v_{t-1} + (1 - beta_2) * g^2 \<br>v_t &amp;= v_t / (1-beta2^t) \<br>x_{t+1} &amp;= x_t - lr_t * m_t / (\sqrt{v_t} + \epsilon) \<br>\end{split}<br>\end{align</em>}<br>$$</p>
<p>在初期 $m_t = 0, v_t=0$ ，这个估计是有问题的。因此需要进行误差修正。</p>
<h1 id="其它"><a href="#其它" class="headerlink" title="其它"></a>其它</h1><p>Adadelta, RMSProp, Adam 由于使用的是过去二阶动量的加权平均和，所以不能保证学习率单调递减，如果数据变化比较大，会出现学习率一会大，一会小的现象，导致学习很不稳定。这也是 Adam 被批评的一个原因。（不过很多文献中指出：Adam 最后的步长往往不是过大而是过小了。事实上，[3] 中的实验也证明了 Adam 训练末期过小的步长是导致泛化性能差的重要原因。Adam 究竟还有什么问题 —— 深度学习优化算法概览(二)）</p>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ul>
<li><p><a href="https://cs231n.github.io/neural-networks-3/#update" target="_blank" rel="noopener">CS231n Convolutional Neural Networks for Visual Recognition</a></p>
</li>
<li><p><a href="http://shuokay.com/2016/06/11/optimization/" target="_blank" rel="noopener">卷积神经网络中的优化算法比较 | Memo</a></p>
</li>
<li><p><a href="https://zhuanlan.zhihu.com/p/32230623" target="_blank" rel="noopener">Adam那么棒，为什么还对SGD念念不忘 (1) —— 一个框架看懂优化算法</a></p>
</li>
<li><p><a href="https://zhuanlan.zhihu.com/p/37269222" target="_blank" rel="noopener">Adam 究竟还有什么问题 —— 深度学习优化算法概览(二)</a></p>
</li>
<li><p><a href="https://alanlee.fun/2017/10/08/gradient-descent-methods/#Challenges" target="_blank" rel="noopener">梯度下降优化算法概述 · Lee’s Space Station</a></p>
</li>
<li><p><a href="https://zhuanlan.zhihu.com/p/22252270" target="_blank" rel="noopener">深度学习最全优化方法总结比较（SGD，Adagrad，Adadelta，Adam，Adamax，Nadam）</a></p>
</li>
<li><p><a href="https://ruder.io/optimizing-gradient-descent/index.html" target="_blank" rel="noopener">An overview of gradient descent optimization algorithms</a></p>
</li>
<li><p><a href="https://zhuanlan.zhihu.com/p/41799394" target="_blank" rel="noopener">深度学习中5种最优化算法的可视化与理解</a></p>
</li>
<li><p><a href="">adadelta算法 - Google 搜索</a></p>
</li>
<li><p><a href="https://zhpmatrix.github.io/2017/04/12/deep-learning-optimizer-tricks/" target="_blank" rel="noopener">[Optimization]Deep Learning中的梯度下降优化算法</a></p>
</li>
</ul>
<h1 id="Related"><a href="#Related" class="headerlink" title="Related"></a>Related</h1><ul>
<li><p><a href="https://watsonyanghx.github.io/2018/06/07/Initialization-in-DL/">Initialization in DL</a></p>
</li>
<li><p><a href="https://watsonyanghx.github.io/2018/06/07/Normalization-in-DL/">Normalization in DL</a></p>
</li>
</ul>

      
    </div>
    <footer>
      
        
  
  <div class="categories">
    <a href="/categories/Deep-Learning/">Deep Learning</a>, <a href="/categories/Deep-Learning/Machine-Learning/">Machine Learning</a>
  </div>

        
        
  <div class="addthis addthis_toolbox addthis_default_style">
    
      <a class="addthis_button_facebook_like" fb:like:layout="button_count"></a>
    
    
      <a class="addthis_button_tweet"></a>
    
    
      <a class="addthis_button_google_plusone" g:plusone:size="medium"></a>
    
    
      <a class="addthis_button_pinterest_pinit" pi:pinit:layout="horizontal"></a>
    
    <a class="addthis_counter addthis_pill_style"></a>
  </div>
  <script type="text/javascript" src="//s7.addthis.com/js/300/addthis_widget.js"></script>

      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>


<section id="comment">
  <h1 class="title">Comments</h1>

  
  <div id="disqus_thread">
    <noscript>Please enable JavaScript to view the <a href="//disqus.com/?ref_noscript" target="_blank" rel="noopener">comments powered by Disqus.</a></noscript>
  </div>
  
</section>

</div></div>
    <aside id="sidebar" class="alignright">
  <div class="search">
  <form action="//google.com/search" method="get" accept-charset="utf-8">
    <input type="search" name="q" results="0" placeholder="Search">
    <input type="hidden" name="q" value="site:watsonyanghx.github.io">
  </form>
</div>

  
<div class="widget tag">
  <h3 class="title">Categories</h3>
  <ul class="entry">
  
    <li><a href="/categories/Algorithm/">Algorithm</a><small>2</small></li>
  
    <li><a href="/categories/Computer-Vision/">Computer Vision</a><small>3</small></li>
  
    <li><a href="/categories/Deep-Learning/">Deep Learning</a><small>3</small></li>
  
    <li><a href="/categories/Interview/">Interview</a><small>1</small></li>
  
    <li><a href="/categories/Machine-Learning/">Machine Learning</a><small>8</small></li>
  
    <li><a href="/categories/Deep-Learning/Machine-Learning/">Machine Learning</a><small>1</small></li>
  
    <li><a href="/categories/NLP/">NLP</a><small>1</small></li>
  
    <li><a href="/categories/OJ/">OJ</a><small>48</small></li>
  
    <li><a href="/categories/Paper/">Paper</a><small>5</small></li>
  
    <li><a href="/categories/Reinforcement-Learning/">Reinforcement Learning</a><small>1</small></li>
  
    <li><a href="/categories/Technique-Summaries/">Technique Summaries</a><small>2</small></li>
  
    <li><a href="/categories/Tools/">Tools</a><small>1</small></li>
  
    <li><a href="/categories/WhatEver/">WhatEver</a><small>1</small></li>
  
  </ul>
</div>


  
<div class="widget tagcloud">
  <h3 class="title">Tag Cloud</h3>
  <div class="entry">
    <a href="/tags/Algorithm/" style="font-size: 18.18px;">Algorithm</a> <a href="/tags/BFS/" style="font-size: 11.82px;">BFS</a> <a href="/tags/Backtracking/" style="font-size: 12.73px;">Backtracking</a> <a href="/tags/Binary-Tree/" style="font-size: 14.55px;">Binary Tree</a> <a href="/tags/C/" style="font-size: 10px;">C++</a> <a href="/tags/CNN/" style="font-size: 15.45px;">CNN</a> <a href="/tags/CS231n/" style="font-size: 11.82px;">CS231n</a> <a href="/tags/Combinatorial-number/" style="font-size: 10.91px;">Combinatorial number</a> <a href="/tags/DFS/" style="font-size: 14.55px;">DFS</a> <a href="/tags/DP/" style="font-size: 19.09px;">DP</a> <a href="/tags/Data-structure/" style="font-size: 10.91px;">Data structure</a> <a href="/tags/DeconvNet/" style="font-size: 10px;">DeconvNet</a> <a href="/tags/Deep-Learning/" style="font-size: 17.27px;">Deep Learning</a> <a href="/tags/Disjoint-Set/" style="font-size: 11.82px;">Disjoint Set</a> <a href="/tags/Divide-and-Conquer/" style="font-size: 13.64px;">Divide and Conquer</a> <a href="/tags/GAN/" style="font-size: 10.91px;">GAN</a> <a href="/tags/Graph/" style="font-size: 13.64px;">Graph</a> <a href="/tags/Greedy/" style="font-size: 11.82px;">Greedy</a> <a href="/tags/Hexo/" style="font-size: 10px;">Hexo</a> <a href="/tags/HihoCoder/" style="font-size: 16.36px;">HihoCoder</a> <a href="/tags/IPv6/" style="font-size: 10px;">IPv6</a> <a href="/tags/LintCode/" style="font-size: 20px;">LintCode</a> <a href="/tags/Machine-Learning/" style="font-size: 10.91px;">Machine Learning</a> <a href="/tags/Matrix/" style="font-size: 10px;">Matrix</a> <a href="/tags/NLP/" style="font-size: 10px;">NLP</a> <a href="/tags/Normalization/" style="font-size: 10px;">Normalization</a> <a href="/tags/Notes/" style="font-size: 10px;">Notes</a> <a href="/tags/Numpy/" style="font-size: 10px;">Numpy</a> <a href="/tags/POJ/" style="font-size: 10.91px;">POJ</a> <a href="/tags/Permutation/" style="font-size: 10px;">Permutation</a> <a href="/tags/Python/" style="font-size: 10px;">Python</a> <a href="/tags/Queue/" style="font-size: 10.91px;">Queue</a> <a href="/tags/RL/" style="font-size: 10px;">RL</a> <a href="/tags/Recursion/" style="font-size: 12.73px;">Recursion</a> <a href="/tags/ResNet/" style="font-size: 10px;">ResNet</a> <a href="/tags/Shadowsocks/" style="font-size: 10px;">Shadowsocks</a> <a href="/tags/String/" style="font-size: 11.82px;">String</a> <a href="/tags/VPS/" style="font-size: 10px;">VPS</a> <a href="/tags/WhatEver/" style="font-size: 10px;">WhatEver</a>
  </div>
</div>

</aside>
    <div class="clearfix"></div>
  </div>
  <footer id="footer" class="inner"><div class="alignleft">
  
  &copy; 2020 Watson Yang
  
</div>
<div class="clearfix"></div></footer>
  <script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>
<script src="/js/jquery.imagesloaded.min.js"></script>
<script src="/js/gallery.js"></script>


<script type="text/javascript">
var disqus_shortname = 'light-disqus';

(function(){
  var dsq = document.createElement('script');
  dsq.type = 'text/javascript';
  dsq.async = true;
  dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
  (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
}());
</script>



<link rel="stylesheet" href="/fancybox/jquery.fancybox.css" media="screen" type="text/css">
<script src="/fancybox/jquery.fancybox.pack.js"></script>
<script type="text/javascript">
(function($){
  $('.fancybox').fancybox();
})(jQuery);
</script>


<div id="totop" style="position:fixed;bottom:35px;right:20px;cursor: pointer;">
  <a title="返回顶部"><i class="fa fa-angle-double-up fa-3x" aria-hidden="true"></i></a>
</div>
<script src="/js/totop.js"></script>

</body>
</html>
