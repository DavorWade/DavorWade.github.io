<!DOCTYPE HTML>
<html>

<head>
  <meta name="google-site-verification" content="6-Grm86DHfS67XSHMuvfO8cIcQcOWPpjGvOBMxbA3Fo" />
  <meta charset="utf-8">
  
  <title>NLP | WatsonYang&#39;s Blog</title>
  
  <meta name="author" content="Watson Yang">
  
  <meta name="description" content="My secret private space.">
  
  
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

  
  <meta property="og:title" content="NLP" />
  <meta property="og:site_name" content="WatsonYang&#39;s Blog" />

  
  <meta property="og:image" content="" />
  

  <link href="/favicon.png" rel="icon">
  <link rel="alternate" href="/atom.xml"
    title="WatsonYang&#39;s Blog" type="application/atom+xml">
  <link rel="stylesheet" href="/css/style.css" media="screen" type="text/css">
  <!--[if lt IE 9]><script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script><![endif]-->
  


  <a href="https://github.com/watsonyanghx" target="_blank">
    <img style="position: absolute; top: 0; left: 0; border: 0;"
      src="https://s3.amazonaws.com/github/ribbons/forkme_left_gray_6d6d6d.png" alt="Fork me on GitHub">
  </a>
  <script src="https://use.fontawesome.com/4eb261e456.js"></script>

  <script>
    MathJax = {
      tex: {
        inlineMath: [
          ['$', '$'],
          ['\\(', '\\)']
        ]
      },
      svg: {
        fontCache: 'global'
      }
    };
  </script>
  <script type="text/javascript" id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js">
  </script>
<meta name="generator" content="Hexo 4.2.0"></head>

<body>
  <header id="header" class="inner"><div class="alignleft">
  <h1><a href="/">WatsonYang&#39;s Blog</a></h1>
  <h2><a href="/">Enrich yourself.</a></h2>
</div>
<nav id="main-nav" class="alignright">
  <ul>
    
      <li><a href="/">Home</a></li>
    
      <li><a href="/archives">Archives</a></li>
    
      <li><a href="/about">About</a></li>
    

    <li> <a href="/atom.xml">RSS</a> </li>
  </ul>
  <div class="clearfix"></div>
</nav>
<div class="clearfix"></div>
</header>
  <div id="content" class="inner">
    <div id="main-col" class="alignleft"><div id="wrapper"><article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2020-04-25T08:28:26.000Z"><a href="/2020/04/25/NLP/">2020-04-25</a></time>
      
      
  
    <h1 class="title">NLP</h1>
  

    </header>
    <div class="entry">
      
        <h2 id="Skip-gram-Naive-Softmax"><a href="#Skip-gram-Naive-Softmax" class="headerlink" title="Skip-gram-Naive-Softmax"></a>Skip-gram-Naive-Softmax</h2><ul>
<li><p>one-hot 缺陷</p>
<ul>
<li><p>维度爆炸</p>
</li>
<li><p>不能表达语义关系：计算机，电脑</p>
</li>
</ul>
</li>
</ul>
<h2 id="Word2Vec"><a href="#Word2Vec" class="headerlink" title="Word2Vec"></a>Word2Vec</h2><ul>
<li>分为 <code>CBOW</code> 与 <code>Skip-gram</code>，两种都是 predictive 的模型。</li>
</ul>
<a id="more"></a>

<ul>
<li><p>将 word 映射成某个空间内的向量之后，我们可以轻松地通过 <code>cos similarity</code> 来计算 word 之间的相似度，并且可以做一些简单的加减运算（可以用这种方式做 <strong>推荐</strong>）。</p>
</li>
<li><p><code>Word2Vec</code> 一个很重要的意义在于，是<strong>无监督方法</strong>，不需要花额外的功夫去构建数据集来 teach 模型，只需要给入一个非常大的文本数据集，就可以得到非常好的效果。</p>
</li>
<li><p>negative sampling</p>
<ul>
<li>不仅告诉模型什么是有关的，同时告诉它什么是无关的（与 <code>SVM</code> 的 <code>hinge loss</code> 很像，也与多模态空间的训练很像，比如：image caption中，text-to-image 中，fine-grained 那篇论文。）</li>
</ul>
</li>
<li><p>在很多代码中，输入并不是 one-hot 形式，也没有所说的 projection 层，而是初始化一个 embedding 矩阵，然后直接训练这个矩阵。这与输入one-hot形式并训练映射层的权重矩阵是一样的。因为输入是one-hot，所以也只是选择了one-hot中 1 所对应的那一行，这与 <code>tf.nn.embedding_lookup()</code> 是一样的操作。</p>
</li>
<li><p>论文中是把训练的网络的权重拿过来做 <code>word embedding</code>，但是实际的训练中都是随机初始化一个 matrix，并把这个 matrix 作为 <code>word embedding</code>，从中 lookup word，然后对这个 matrix 进行训练。两者本质是一样的，可以看做后者是把前者的隐藏层权重拿过来作为一个变量进行训练。</p>
</li>
<li><p>参考</p>
<ul>
<li><p><a href="http://cpmarkchang.logdown.com/posts/773062-neural-network-word2vec-part-1-overview" target="_blank" rel="noopener">類神經網路 — word2vec (part 1 : Overview) « MARK CHANG’S BLOG</a></p>
</li>
<li><p><a href="https://ckmarkoh.github.io/blog/2016/07/12/neural-network-word2vec-part-1-overview/" target="_blank" rel="noopener">Word2vec (Part 1 : Overview) - Mark Chang’s Blog</a></p>
</li>
<li><p><a href="https://zhuanlan.zhihu.com/p/27234078" target="_blank" rel="noopener">理解 Word2Vec 之 Skip-Gram 模型</a></p>
</li>
<li><p><a href="https://zhuanlan.zhihu.com/p/26306795" target="_blank" rel="noopener">[NLP] 秒懂词向量Word2vec的本质</a></p>
</li>
<li><p><a href="https://wugh.github.io/posts/2016/02/cs224d-notes1-word2vec/" target="_blank" rel="noopener">CS224d笔记1——word2vec</a></p>
</li>
<li><p><a href="https://zhuanlan.zhihu.com/p/30302498" target="_blank" rel="noopener">漫谈Word2vec之skip-gram模型</a></p>
</li>
<li><p><a href="https://zhuanlan.zhihu.com/p/21241739" target="_blank" rel="noopener">Efficient Estimation of Word Representations in Vector Space</a></p>
</li>
<li><p><a href="https://www.zybuluo.com/ShawnNg/note/547365" target="_blank" rel="noopener">从头学起深度学习应用于自然语言处理-词向量</a></p>
</li>
<li><p><a href="https://juejin.im/entry/5a6af990f265da3e283a3b42" target="_blank" rel="noopener">Word2Vec、GloVe、Fasttext等背后的思想简介</a></p>
</li>
<li><p><a href="https://www.zhihu.com/question/37489735" target="_blank" rel="noopener">word2vec算出的词向量怎么衡量好坏？ - 知乎</a></p>
</li>
</ul>
</li>
</ul>
<h2 id="GloVe"><a href="#GloVe" class="headerlink" title="GloVe"></a>GloVe</h2><ul>
<li><p><code>GloVe</code> 是 count based 的模型，对 <code>词-词共现矩阵</code> 进行。(但是效果并不一定一直比 <code>Word2Vec</code> 好，要看具体任务。)</p>
</li>
<li><p>优势：</p>
<ul>
<li>更容易并行化</li>
</ul>
</li>
<li><p>参考</p>
<ul>
<li><p><a href="https://zhuanlan.zhihu.com/p/21242347" target="_blank" rel="noopener">GloVe: Global Vectors for Word Representation</a></p>
</li>
<li><p><a href="https://zhuanlan.zhihu.com/p/31023929" target="_blank" rel="noopener">GloVe与word2vec的区别</a></p>
</li>
<li><p><a href="https://zhuanlan.zhihu.com/p/31296926?group_id=916698584980709376" target="_blank" rel="noopener">PaperWeekly 第52期 | 更别致的词向量模型：Simpler GloVe - Part 1</a></p>
</li>
<li><p><a href="https://www.quora.com/How-is-GloVe-different-from-word2vec" target="_blank" rel="noopener">How is GloVe different from word2vec? - Quora</a></p>
</li>
<li><p><a href="https://www.quora.com/What-are-the-differences-between-GloVe-word2vec-and-tf-idf" target="_blank" rel="noopener">What are the differences between GloVe, word2vec and tf-idf? - Quora</a></p>
</li>
<li><p><a href="http://www.shuang0420.com/2017/03/21/NLP%20%E7%AC%94%E8%AE%B0%20-%20%E5%86%8D%E8%B0%88%E8%AF%8D%E5%90%91%E9%87%8F/" target="_blank" rel="noopener">NLP 笔记 - 再谈词向量</a></p>
</li>
</ul>
</li>
</ul>
<h2 id="FastText"><a href="#FastText" class="headerlink" title="FastText"></a>FastText</h2><ul>
<li><p>所谓 fasttext，不过是 word2vec 中 cbow + h-softmax 的灵活使用，体现在两个方面：</p>
<ol>
<li><p>模型的输出层：word2vec 的输出层，对应的是每一个 term，计算某 term 的概率最大；而 fasttext 的输出层对应的是分类的label。不过不管输出层对应的是什么内容，起对应的 vector 都不会被保留和使用；</p>
</li>
<li><p>模型的输入层：word2vec 的输出层，是 context window 内的term；而 fasttext 对应的整个 sentence 的内容，包括 term，也包括 n-gram 的内容；</p>
<p>两者本质的不同，体现在 h-softmax 的使用。</p>
<p>Wordvec 的目的是得到词向量，该词向量 最终是在输入层得到，输出层对应的 h-softmax 也会生成一系列的向量，但最终都被抛弃，不会使用。</p>
<p>fasttext 则充分利用了 h-softmax 的分类功能，遍历分类树的所有叶节点，找到概率最大的 label（一个或者 N 个）。<a href="https://blog.csdn.net/phoeny0201/article/details/52329477" target="_blank" rel="noopener">fasttext</a></p>
</li>
</ol>
</li>
<li><p>优势：</p>
<ul>
<li><p>可以给出训练语料库中没有出现的词的 <code>word embedding</code>(把这个词输入训练得到的网络中计算得到，这点与 <code>Word2Vec</code> 不同，后者是使用训练得到的 word membeddings matrix，不会再用网络计算）。（因为是基于字符级的 <code>n-gram</code> 来训练网络，所以对于之前没有见过的词，可以通过组合 character 的方式得到。也正因如此，训练的时间长，占用的内存大。）</p>
</li>
<li><p>对于出现次数少的词，也能很好的训练得到 <code>word embedding</code>。（还是因为用的是字符级的 <code>n-gram</code> 来训练网络，所以这个词出现的 character 在别的词中也可能会出现，这就增加了训练数据。）</p>
</li>
</ul>
</li>
<li><p>参考</p>
<ul>
<li><p><a href="https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650736673&idx=4&sn=d5cb11250b28912accbc08ddb5d9c97b&chksm=871acc5fb06d45492ee54f3ff42e767bdc668d12c615b8ddc7e0aaeae7748aafe1aa53686176#rd" target="_blank" rel="noopener">专栏 | fastText原理及实践</a></p>
</li>
<li><p><a href="https://blog.csdn.net/sinat_26917383/article/details/54850933" target="_blank" rel="noopener">NLP︱高级词向量表达（二）——FastText（简述、学习笔记）</a></p>
</li>
<li><p><a href="https://www.quora.com/What-is-the-main-difference-between-word2vec-and-fastText" target="_blank" rel="noopener">What is the main difference between word2vec and fastText? - Quora</a></p>
</li>
<li><p><a href="https://www.quora.com/What-is-the-difference-between-fastText-and-GloVe" target="_blank" rel="noopener">What is the difference between fastText and GloVe? - Quora</a></p>
</li>
<li><p><a href="https://blog.csdn.net/thriving_fcl/article/details/53239856" target="_blank" rel="noopener">FastText 文本分类使用心得</a></p>
</li>
</ul>
</li>
</ul>
<h2 id="Sentence-Vector"><a href="#Sentence-Vector" class="headerlink" title="Sentence Vector"></a>Sentence Vector</h2><h3 id="Skip-Thought-Vector"><a href="#Skip-Thought-Vector" class="headerlink" title="Skip Thought Vector"></a>Skip Thought Vector</h3><ul>
<li><p>sentence 编码模型，一个 encoder，两个 decoder。</p>
</li>
<li><p>参考</p>
<ul>
<li><a href="https://medium.com/@sanyamagarwal/my-thoughts-on-skip-thoughts-a3e773605efa" target="_blank" rel="noopener">My thoughts on Skip-Thoughts</a></li>
</ul>
</li>
</ul>
<h3 id="Doc2Vec"><a href="#Doc2Vec" class="headerlink" title="Doc2Vec"></a>Doc2Vec</h3><ul>
<li><p>参考</p>
<ul>
<li><p><a href=""></a></p>
</li>
<li><p><a href=""></a></p>
</li>
</ul>
</li>
</ul>
<h3 id="Visual-Semantic-Embedding"><a href="#Visual-Semantic-Embedding" class="headerlink" title="Visual Semantic Embedding"></a>Visual Semantic Embedding</h3><ul>
<li><p>参考</p>
<ul>
<li><a href="https://zhuanlan.zhihu.com/p/25910749" target="_blank" rel="noopener">图文互搜论文综述</a></li>
</ul>
</li>
</ul>
<h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3><ul>
<li><p><a href="https://github.com/DSKSD/DeepNLP-models-Pytorch" target="_blank" rel="noopener">DeepNLP-models-Pytorch</a></p>
</li>
<li><p><a href="https://github.com/oxford-cs-deepnlp-2017/lectures" target="_blank" rel="noopener">oxford-cs-deepnlp-2017/lectures: Oxford Deep NLP 2017 course</a></p>
</li>
</ul>
<h2 id="Basics"><a href="#Basics" class="headerlink" title="Basics"></a>Basics</h2><h3 id="TF-IDF"><a href="#TF-IDF" class="headerlink" title="TF-IDF"></a>TF-IDF</h3><ul>
<li><p>参考</p>
<ul>
<li><p><a href="http://www.ruanyifeng.com/blog/2013/03/tf-idf.html" target="_blank" rel="noopener">TF-IDF与余弦相似性的应用（一）：自动提取关键词 - 阮一峰的网络日志</a></p>
</li>
<li><p><a href="http://www.ruanyifeng.com/blog/2013/03/cosine_similarity.html" target="_blank" rel="noopener">TF-IDF与余弦相似性的应用（二）：找出相似文章 - 阮一峰的网络日志</a></p>
</li>
<li><p><a href="https://deeplearning4j.org/cn/bagofwords-tf-idf" target="_blank" rel="noopener">词袋与TF-IDF</a></p>
</li>
</ul>
</li>
</ul>
<h3 id="词袋模型-向量空间模型"><a href="#词袋模型-向量空间模型" class="headerlink" title="词袋模型 / 向量空间模型"></a>词袋模型 / 向量空间模型</h3><ul>
<li><p><code>词袋模型</code> 用来表示的一个句子，把不同句子的词袋模型表示叠加在一起就得到了 <code>向量空间模型</code>。（参考1）</p>
</li>
<li><p><code>词袋模型</code> 中的值可以是 <code>TF</code> (词频)，<code>TF-IDF</code> 等。</p>
</li>
<li><p>参考</p>
<ul>
<li><p><a href="https://www.letiantian.me/2014-12-12-bag-of-words-model-and-document-term-matrix/" target="_blank" rel="noopener">词袋模型与文档-词矩阵</a></p>
</li>
<li><p><a href="http://cpmarkchang.logdown.com/posts/772665-nlp-vector-space-semantics" target="_blank" rel="noopener">自然語言處理 — Vector Space of Semantics « MARK CHANG’S BLOG</a></p>
</li>
<li><p><a href="https://ckmarkoh.github.io/blog/2016/07/10/nlp-vector-space-semantics/" target="_blank" rel="noopener">Vector Space of Semantics - Mark Chang’s Blog</a></p>
</li>
</ul>
</li>
</ul>
<h3 id="潜在语义分析"><a href="#潜在语义分析" class="headerlink" title="潜在语义分析"></a>潜在语义分析</h3><ul>
<li><p><a href="http://www.flickering.cn/ads/2015/02/%E8%AF%AD%E4%B9%89%E5%88%86%E6%9E%90%E7%9A%84%E4%B8%80%E4%BA%9B%E6%96%B9%E6%B3%95%E4%B8%80/" target="_blank" rel="noopener">语义分析的一些方法(一) | 火光摇曳</a></p>
</li>
<li><p><a href="http://www.52nlp.cn/%e5%a6%82%e4%bd%95%e8%ae%a1%e7%ae%97%e4%b8%a4%e4%b8%aa%e6%96%87%e6%a1%a3%e7%9a%84%e7%9b%b8%e4%bc%bc%e5%ba%a6%e4%ba%8c" target="_blank" rel="noopener">如何计算两个文档的相似度（二）</a></p>
</li>
</ul>
<h3 id="参考-1"><a href="#参考-1" class="headerlink" title="参考"></a>参考</h3><ul>
<li><p><a href="https://zhuanlan.zhihu.com/p/39054002" target="_blank" rel="noopener">CNN也能用于NLP任务，一文简述文本分类任务的7个模型</a></p>
</li>
<li><p><a href="https://towardsdatascience.com/introduction-to-nlp-5bff2b2a7170" target="_blank" rel="noopener">Introduction to NLP – Towards Data Science</a></p>
</li>
<li><p><a href="https://zhuanlan.zhihu.com/p/41850756" target="_blank" rel="noopener">自然语言处理是如何工作的？一步步教你构建 NLP 流水线</a></p>
</li>
</ul>

      
    </div>
    <footer>
      
        
  
  <div class="categories">
    <a href="/categories/NLP/">NLP</a>
  </div>

        
  
  <div class="tags">
    <a href="/tags/NLP/">NLP</a>
  </div>

        
  <div class="addthis addthis_toolbox addthis_default_style">
    
      <a class="addthis_button_facebook_like" fb:like:layout="button_count"></a>
    
    
      <a class="addthis_button_tweet"></a>
    
    
      <a class="addthis_button_google_plusone" g:plusone:size="medium"></a>
    
    
      <a class="addthis_button_pinterest_pinit" pi:pinit:layout="horizontal"></a>
    
    <a class="addthis_counter addthis_pill_style"></a>
  </div>
  <script type="text/javascript" src="//s7.addthis.com/js/300/addthis_widget.js"></script>

      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>


<section id="comment">
  <h1 class="title">Comments</h1>

  
  <div id="disqus_thread">
    <noscript>Please enable JavaScript to view the <a href="//disqus.com/?ref_noscript" target="_blank" rel="noopener">comments powered by Disqus.</a></noscript>
  </div>
  
</section>

</div></div>
    <aside id="sidebar" class="alignright">
  <div class="search">
  <form action="//google.com/search" method="get" accept-charset="utf-8">
    <input type="search" name="q" results="0" placeholder="Search">
    <input type="hidden" name="q" value="site:watsonyanghx.github.io">
  </form>
</div>

  
<div class="widget tag">
  <h3 class="title">Categories</h3>
  <ul class="entry">
  
    <li><a href="/categories/Algorithm/">Algorithm</a><small>2</small></li>
  
    <li><a href="/categories/C/">C++</a><small>6</small></li>
  
    <li><a href="/categories/Computer-Vision/">Computer Vision</a><small>3</small></li>
  
    <li><a href="/categories/Deep-Learning/">Deep Learning</a><small>3</small></li>
  
    <li><a href="/categories/Interview/">Interview</a><small>1</small></li>
  
    <li><a href="/categories/Machine-Learning/">Machine Learning</a><small>8</small></li>
  
    <li><a href="/categories/NLP/">NLP</a><small>1</small></li>
  
    <li><a href="/categories/OJ/">OJ</a><small>48</small></li>
  
    <li><a href="/categories/Paper/">Paper</a><small>5</small></li>
  
    <li><a href="/categories/Reinforcement-Learning/">Reinforcement Learning</a><small>1</small></li>
  
    <li><a href="/categories/Technique-Summaries/">Technique Summaries</a><small>2</small></li>
  
    <li><a href="/categories/Tools/">Tools</a><small>1</small></li>
  
    <li><a href="/categories/WhatEver/">WhatEver</a><small>1</small></li>
  
  </ul>
</div>


  
<div class="widget tagcloud">
  <h3 class="title">Tag Cloud</h3>
  <div class="entry">
    <a href="/tags/Algorithm/" style="font-size: 18.18px;">Algorithm</a> <a href="/tags/BFS/" style="font-size: 11.82px;">BFS</a> <a href="/tags/Backtracking/" style="font-size: 12.73px;">Backtracking</a> <a href="/tags/Binary-Tree/" style="font-size: 14.55px;">Binary Tree</a> <a href="/tags/C/" style="font-size: 10px;">C++</a> <a href="/tags/CNN/" style="font-size: 15.45px;">CNN</a> <a href="/tags/CS231n/" style="font-size: 11.82px;">CS231n</a> <a href="/tags/Combinatorial-number/" style="font-size: 10.91px;">Combinatorial number</a> <a href="/tags/DFS/" style="font-size: 14.55px;">DFS</a> <a href="/tags/DP/" style="font-size: 19.09px;">DP</a> <a href="/tags/Data-structure/" style="font-size: 10.91px;">Data structure</a> <a href="/tags/DeconvNet/" style="font-size: 10px;">DeconvNet</a> <a href="/tags/Deep-Learning/" style="font-size: 17.27px;">Deep Learning</a> <a href="/tags/Disjoint-Set/" style="font-size: 11.82px;">Disjoint Set</a> <a href="/tags/Divide-and-Conquer/" style="font-size: 13.64px;">Divide and Conquer</a> <a href="/tags/GAN/" style="font-size: 10.91px;">GAN</a> <a href="/tags/Graph/" style="font-size: 13.64px;">Graph</a> <a href="/tags/Greedy/" style="font-size: 11.82px;">Greedy</a> <a href="/tags/Hexo/" style="font-size: 10px;">Hexo</a> <a href="/tags/HihoCoder/" style="font-size: 16.36px;">HihoCoder</a> <a href="/tags/IPv6/" style="font-size: 10px;">IPv6</a> <a href="/tags/LintCode/" style="font-size: 20px;">LintCode</a> <a href="/tags/Machine-Learning/" style="font-size: 10.91px;">Machine Learning</a> <a href="/tags/Matrix/" style="font-size: 10px;">Matrix</a> <a href="/tags/Metaprogramming/" style="font-size: 10px;">Metaprogramming</a> <a href="/tags/NLP/" style="font-size: 10px;">NLP</a> <a href="/tags/Normalization/" style="font-size: 10px;">Normalization</a> <a href="/tags/Notes/" style="font-size: 10px;">Notes</a> <a href="/tags/Numpy/" style="font-size: 10px;">Numpy</a> <a href="/tags/POJ/" style="font-size: 10.91px;">POJ</a> <a href="/tags/Permutation/" style="font-size: 10px;">Permutation</a> <a href="/tags/Python/" style="font-size: 10px;">Python</a> <a href="/tags/Queue/" style="font-size: 10.91px;">Queue</a> <a href="/tags/RL/" style="font-size: 10px;">RL</a> <a href="/tags/Recursion/" style="font-size: 12.73px;">Recursion</a> <a href="/tags/ResNet/" style="font-size: 10px;">ResNet</a> <a href="/tags/Shadowsocks/" style="font-size: 10px;">Shadowsocks</a> <a href="/tags/String/" style="font-size: 11.82px;">String</a> <a href="/tags/Template/" style="font-size: 10.91px;">Template</a> <a href="/tags/VPS/" style="font-size: 10px;">VPS</a> <a href="/tags/WhatEver/" style="font-size: 10px;">WhatEver</a>
  </div>
</div>

</aside>
    <div class="clearfix"></div>
  </div>
  <footer id="footer" class="inner"><div class="alignleft">
  
  &copy; 2020 Watson Yang
  
</div>
<div class="clearfix"></div></footer>
  <script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>
<script src="/js/jquery.imagesloaded.min.js"></script>
<script src="/js/gallery.js"></script>


<script type="text/javascript">
var disqus_shortname = 'light-disqus';

(function(){
  var dsq = document.createElement('script');
  dsq.type = 'text/javascript';
  dsq.async = true;
  dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
  (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
}());
</script>



<link rel="stylesheet" href="/fancybox/jquery.fancybox.css" media="screen" type="text/css">
<script src="/fancybox/jquery.fancybox.pack.js"></script>
<script type="text/javascript">
(function($){
  $('.fancybox').fancybox();
})(jQuery);
</script>


<div id="totop" style="position:fixed;bottom:35px;right:20px;cursor: pointer;">
  <a title="返回顶部"><i class="fa fa-angle-double-up fa-3x" aria-hidden="true"></i></a>
</div>
<script src="/js/totop.js"></script>

</body>
</html>
