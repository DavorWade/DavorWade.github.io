<!DOCTYPE HTML>
<html>

<head>
  <meta name="google-site-verification" content="6-Grm86DHfS67XSHMuvfO8cIcQcOWPpjGvOBMxbA3Fo" />
  <meta charset="utf-8">
  
  <title>Page 2 | WatsonYang&#39;s Blog</title>
  
  <meta name="author" content="Watson Yang">
  
  <meta name="description" content="My secret private space.">
  
  
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

  
  <meta property="og:site_name" content="WatsonYang&#39;s Blog" />

  
  <meta property="og:image" content="" />
  

  <link href="/favicon.png" rel="icon">
  <link rel="alternate" href="/atom.xml"
    title="WatsonYang&#39;s Blog" type="application/atom+xml">
  <link rel="stylesheet" href="/css/style.css" media="screen" type="text/css">
  <!--[if lt IE 9]><script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script><![endif]-->
  


  <a href="https://github.com/watsonyanghx" target="_blank">
    <img style="position: absolute; top: 0; left: 0; border: 0;"
      src="https://s3.amazonaws.com/github/ribbons/forkme_left_gray_6d6d6d.png" alt="Fork me on GitHub">
  </a>
  <script src="https://use.fontawesome.com/4eb261e456.js"></script>

  <script>
    MathJax = {
      tex: {
        inlineMath: [
          ['$', '$'],
          ['\\(', '\\)']
        ]
      },
      svg: {
        fontCache: 'global'
      }
    };
  </script>
  <script type="text/javascript" id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js">
  </script>
<meta name="generator" content="Hexo 4.2.0"></head>

<body>
  <header id="header" class="inner"><div class="alignleft">
  <h1><a href="/">WatsonYang&#39;s Blog</a></h1>
  <h2><a href="/">Enrich yourself.</a></h2>
</div>
<nav id="main-nav" class="alignright">
  <ul>
    
      <li><a href="/">Home</a></li>
    
      <li><a href="/archives">Archives</a></li>
    
      <li><a href="/about">About</a></li>
    

    <li> <a href="/atom.xml">RSS</a> </li>
  </ul>
  <div class="clearfix"></div>
</nav>
<div class="clearfix"></div>
</header>
  <div id="content" class="inner">
    <div id="main-col" class="alignleft"><div id="wrapper">
  <article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2017-04-11T17:18:52.000Z"><a href="/2017/04/12/Longest-Common-Substring/">2017-04-12</a></time>
      
      
  
    <h1 class="title"><a href="/2017/04/12/Longest-Common-Substring/">Longest Common Substring</a></h1>
  

    </header>
    <div class="entry">
      
        <blockquote>
<p><a href="https://www.lintcode.com/en/problem/longest-common-substring/" target="_blank" rel="noopener">Longest Common Substring</a></p>
<p><strong>Description:</strong></p>
<p>Given two strings, find the longest common substring. Return the length of it.</p>
<p><strong>Notice:</strong></p>
<p>The characters in substring should occur continuously in original string. This is different with subsequence.</p>
<p><strong>Example:</strong></p>
<p>Given A = “ABCD”, B = “CBCE”, return 2.</p>
</blockquote>
      
    </div>
    <footer>
      
        
          <div class="alignleft">
            <a href="/2017/04/12/Longest-Common-Substring/#more" class="more-link">Read More</a>
          </div>
        
        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>




  <article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2017-03-29T11:41:49.000Z"><a href="/2017/03/29/Learning-Deconvolution-Network-for-Semantic-Segmentation/">2017-03-29</a></time>
      
      
  
    <h1 class="title"><a href="/2017/03/29/Learning-Deconvolution-Network-for-Semantic-Segmentation/">Learning Deconvolution Network for Semantic Segmentation</a></h1>
  

    </header>
    <div class="entry">
      
        <p>这个就是大家所说的<strong>反卷积网络</strong>。</p>
<h2 id="Category-of-Deconvolution"><a href="#Category-of-Deconvolution" class="headerlink" title="Category of Deconvolution"></a>Category of Deconvolution</h2><p><code>Deconvolution</code>在<code>Deep Learning</code>领域大致可以分为下面 2 个种类。</p>
<ul>
<li><p>Transposed Convolution</p>
<p><code>convolution</code>的反转过程，这篇 paper 讲的就是这个。</p>
</li>
<li><p>Convolutional Sparse Coding</p>
<p>它是一种<code>representation learning</code>的一种，<strong>目的</strong>是根据从图像中学到的<code>features</code>完全的<strong>还原</strong>图像。</p>
</li>
</ul>
<h2 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h2>
      
    </div>
    <footer>
      
        
          <div class="alignleft">
            <a href="/2017/03/29/Learning-Deconvolution-Network-for-Semantic-Segmentation/#more" class="more-link">Read More</a>
          </div>
        
        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>




  <article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2017-03-29T09:07:45.000Z"><a href="/2017/03/29/Deep-Residual-Learning-for-Image-Recognition/">2017-03-29</a></time>
      
      
  
    <h1 class="title"><a href="/2017/03/29/Deep-Residual-Learning-for-Image-Recognition/">Deep Residual Learning for Image Recognition</a></h1>
  

    </header>
    <div class="entry">
      
        <p>这个就是大家所说的<code>ResNet</code>——<strong>残差网络</strong>。</p>
<h2 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h2><p>很多实验都证明网络的<strong>深度</strong>对网络的效果有着很重要的影响，理论上网络层数<strong>越多</strong>，得到的效果会<strong>越好</strong>，但是作者做了实验却发现网络的深度与网络在<code>test dataset</code>上的<code>error rate</code>是一个<code>U</code>形的曲线。也就是说当网络达到<strong>一定深度</strong>后，再增加深度，效果会变差，作者把这种现象称为<code>Degradation Problem</code>。</p>
  <center>
  <img src="https://www.tuchuang001.com/images/2017/03/29/QQ20170329202816.png" alt="Degradation Problem" border="0" />
  </center>
      
    </div>
    <footer>
      
        
          <div class="alignleft">
            <a href="/2017/03/29/Deep-Residual-Learning-for-Image-Recognition/#more" class="more-link">Read More</a>
          </div>
        
        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>




  <article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2017-03-10T05:41:29.000Z"><a href="/2017/03/10/Maxout-Networks-Network-in-Network/">2017-03-10</a></time>
      
      
  
    <h1 class="title"><a href="/2017/03/10/Maxout-Networks-Network-in-Network/">Maxout Networks &amp; Network in Network</a></h1>
  

    </header>
    <div class="entry">
      
        <h2 id="Maxout-Networks"><a href="#Maxout-Networks" class="headerlink" title="Maxout Networks"></a>Maxout Networks</h2><h3 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h3><p><code>dropout</code>是一种 regularization 的方法，主要作用是用来减轻过拟合现象。实现方式是：以一定的概率，随机的把<code>hidden units</code>的输出值设成<code>0</code>，也就是随机的让某些<code>hidden units</code>失活，此时就相当于是一种集成模型的训练。</p>
<p>实验证实当网络比较深的时候，使用<code>dropout</code>能够很好的提升网络的 generalization 能力。</p>
<p>作者就想：如果不只是单单的把它作为一种提升模型性能的工具，而是直接应用到网络结构的内部，是不是能够更好的提高网络的性能呢？</p>
<p>注意：<code>dropout</code>只是一种 regularizer，而<code>maxout</code>是一种网络结构。</p>
      
    </div>
    <footer>
      
        
          <div class="alignleft">
            <a href="/2017/03/10/Maxout-Networks-Network-in-Network/#more" class="more-link">Read More</a>
          </div>
        
        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>




  <article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2017-02-25T15:05:53.000Z"><a href="/2017/02/25/Chapter-10-Sequence-Modeling-Recurrent-and-Recursive-Nets/">2017-02-25</a></time>
      
      
  
    <h1 class="title"><a href="/2017/02/25/Chapter-10-Sequence-Modeling-Recurrent-and-Recursive-Nets/">Chapter 10 Sequence Modeling: Recurrent and Recursive Nets</a></h1>
  

    </header>
    <div class="entry">
      
        <p>最近比较浮躁，书读不进去，总结也写不下去。虽然感觉有很多事情要做，却有种分不清主次的感觉。啊啊啊啊啊啊啊啊，什么时候才是个头啊！！！！！！</p>
<p>这一 Chapter 的很多内容都没有仔细的去看，所以这个总结里的内容也很少，会在后面慢慢的补充上来 -_- </p>
<p>首先，介绍了<code>RNN</code>的定义：</p>
<blockquote>
<p>Recurrent neural networks or RNNs (Rumelhart et al., 1986a) are a family of neural networks for processing sequential data. specialized for processing a sequence of values x<sup>(1)</sup>, . . . , x<sup>(τ)</sup>.</p>
</blockquote>
<p>也就是在每一步都有新的输入，所以从某种角度它也算是一种<code>Feedforward Networks</code>。</p>
<p>另外，这里介绍了<code>RNN</code>的<code>parameter sharing</code>机制：Each member of the output is produced using the <strong>same update rule</strong> applied to the <strong>previous outputs</strong>. This recurrent formulation results in the sharing of parameters through a very deep <strong>computational graph</strong>.</p>
<p>Sharing Parameters is key to RNNs。书中举得 “I went to Nepal in 1999 ” and “In 1999, I went to Nepal ”的例子就是为了说明这个重要性。</p>
      
    </div>
    <footer>
      
        
          <div class="alignleft">
            <a href="/2017/02/25/Chapter-10-Sequence-Modeling-Recurrent-and-Recursive-Nets/#more" class="more-link">Read More</a>
          </div>
        
        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>




  <article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2017-02-25T15:04:26.000Z"><a href="/2017/02/25/Chapter-9-Convolutional-Networks/">2017-02-25</a></time>
      
      
  
    <h1 class="title"><a href="/2017/02/25/Chapter-9-Convolutional-Networks/">Chapter 9 Convolutional Networks</a></h1>
  

    </header>
    <div class="entry">
      
        <p>这一章可能是因为比较了解的原因，没有找到感觉需要强调的地方。</p>
<p>不知道为什么最近读这本书，突然产生了怀疑：到底应不应该花时间读这本书，用读这本书的时间去多读点论文是不是会更好？或者多多动手实现一下算法是不是会收获更多？</p>
<p>总感觉时间确实很紧张，有很多事情需要去做。却不知道应该把时间花在哪件事情上会更值得。</p>
      
    </div>
    <footer>
      
        
          <div class="alignleft">
            <a href="/2017/02/25/Chapter-9-Convolutional-Networks/#more" class="more-link">Read More</a>
          </div>
        
        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>




  <article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2017-02-25T15:01:24.000Z"><a href="/2017/02/25/Chapter-8-Optimization-for-Training-Deep-Models/">2017-02-25</a></time>
      
      
  
    <h1 class="title"><a href="/2017/02/25/Chapter-8-Optimization-for-Training-Deep-Models/">Chapter 8 Optimization for Training Deep Models</a></h1>
  

    </header>
    <div class="entry">
      
        <p>非常抱歉，这一张基本上没怎么看，只是大致的浏览了一下。</p>
<p>原因是，这章基本上都是理论的介绍，感觉对做工程的可能没有太大帮助（如果你是研究这个的phD，而且你对这方面也不是很了解，还是很建议你好好读一下的），另外<code>Neural Networks</code>的 optimization 本身就像是个黑盒子。</p>
<p>这一部分感觉目前对我的帮助真的不大，打算是等到以后遇到相关问题后再回来复习吧。</p>
<p>This chapter focuses on <strong>one particular case of optimization</strong>: finding the parameters θ of a neural network that significantly <strong>reduce a cost function J(θ)</strong>, which typically includes a performance measure evaluated on the <strong>entire training set</strong> as well as <strong>additional regularization terms</strong>。</p>
<h1 id="8-1-How-Learning-Differs-from-Pure-Optimization"><a href="#8-1-How-Learning-Differs-from-Pure-Optimization" class="headerlink" title="8.1 How Learning Differs from Pure Optimization"></a>8.1 How Learning Differs from Pure Optimization</h1><p>前面有个小节就是关于<code>optimization</code>的，在那里也说了：<code>ML</code>只是利用 <code>optimization</code> 的方法来进行训练，它的终极目标是<code>test error</code>尽可能的小；而 <code>optimization</code> 是希望完全的拟合数据。</p>
      
    </div>
    <footer>
      
        
          <div class="alignleft">
            <a href="/2017/02/25/Chapter-8-Optimization-for-Training-Deep-Models/#more" class="more-link">Read More</a>
          </div>
        
        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>




  <article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2017-02-25T14:55:40.000Z"><a href="/2017/02/25/Chapter-7-Regularization-for-Deep-Learning/">2017-02-25</a></time>
      
      
  
    <h1 class="title"><a href="/2017/02/25/Chapter-7-Regularization-for-Deep-Learning/">Chapter 7 Regularization for Deep Learning</a></h1>
  

    </header>
    <div class="entry">
      
        <p>首先，介绍了<code>regularization</code>的定义：</p>
<blockquote>
<p>any modification we make to a learning algorithm that is intended to reduce its generalization error but not its training error.</p>
</blockquote>
<p>也就是说<code>regularization</code>的<font color='red'>目的</font>是：通过<strong>控制模型复杂度</strong>的方式，尽量减轻模型<code>overfitting</code>的程度，提高<code>generalization</code>能力。</p>
<ul>
<li><p><code>regularization</code>的大致可以分为以下几类：</p>
<ul>
<li><p>作用对象/实现方式</p>
<ul>
<li>put extra constraints on a <strong>machine learning model</strong>，such as adding restrictions on the <strong>parameter</strong> values。</li>
<li>add extra terms in the <strong>objective function</strong> that can be thought of as corresponding to a <strong>soft constraint</strong> on the parameter values。</li>
</ul>
</li>
<li><p>原因
      
    </div>
    <footer>
      
        
          <div class="alignleft">
            <a href="/2017/02/25/Chapter-7-Regularization-for-Deep-Learning/#more" class="more-link">Read More</a>
          </div>
        
        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>




  <article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2017-02-24T12:26:23.000Z"><a href="/2017/02/24/Chapter-6-Deep-Feedforward-Networks/">2017-02-24</a></time>
      
      
  
    <h1 class="title"><a href="/2017/02/24/Chapter-6-Deep-Feedforward-Networks/">Chapter 6 Deep Feedforward Networks</a></h1>
  

    </header>
    <div class="entry">
      
        <p>首先，介绍了<code>Deep feedforward networks</code>的概念：之所以称为<code>networks</code>，是因为它们是通过把不同的函数组合起来的形式表达的（typically represented by composing together many different functions）；<code>feedforward</code>是因为 no feedback connections in which outputs of the model are fed back into itself（）。</p>
<p>然后，介绍了<code>output layer</code>与<code>hidden layer</code>。training examples 决定了的<code>output layer</code>的行为：输出要接近目标值<code>y</code>。但是它并没有定义<code>hidden layer</code>的行为，不过 learning algorithm 则必须决定如何使用这些<code>hidden layer</code>来完成对函数的逼近。</p>
<p>接着，引入了非线性函数的概念（不是<code>activation function</code>，是指最终学出来的那个函数），指出得到非线性函数的方法有3种：</p>
<ol>
<li><p>使用一个比较 generic 的函数，比如<code>RBF kernel</code>，但最终在 test dataset 上的效果非常不好（generalization to the test set often remains poor）。</p>
</li>
<li><p>使用 domain knowledge 人工指定，但是太费时费力。</p>
</li>
<li><p>算法自己学习，这种方法既可以保证第一种方法的 generic 的性质，又能保证第二种 domain knowledge 的优点（人为的设置一些biased）。</p>
</li>
</ol>
      
    </div>
    <footer>
      
        
          <div class="alignleft">
            <a href="/2017/02/24/Chapter-6-Deep-Feedforward-Networks/#more" class="more-link">Read More</a>
          </div>
        
        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>




  <article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2017-02-22T16:40:27.000Z"><a href="/2017/02/23/Chapter-5-Machine-Learning-Basics/">2017-02-23</a></time>
      
      
  
    <h1 class="title"><a href="/2017/02/23/Chapter-5-Machine-Learning-Basics/">Chapter 5 Machine Learning Basics</a></h1>
  

    </header>
    <div class="entry">
      
        <h1 id="《Deep-Learning》-读书笔记"><a href="#《Deep-Learning》-读书笔记" class="headerlink" title="《Deep Learning》 读书笔记"></a>《Deep Learning》 读书笔记</h1><p>写这个笔记的目的有两个：一是以高层的角度把整个章节的内容联系起来，从而加深自己的理解，同时也可以供日后复习使用；二是在日后的组会中可能会降到的时候，有东西可以讲（好偷懒 -_-）。</p>
<p>因为是刚入门的新手，有很多东西还不了解，或者了解的不透彻，肯定会有错误和疏漏的地方，希望大家不吝赐教哈～～</p>
<h1 id="5-1-Learning-Algorithms"><a href="#5-1-Learning-Algorithms" class="headerlink" title="5.1 Learning Algorithms"></a>5.1 Learning Algorithms</h1><p>首先，提出了经典的<code>ML</code>的定义：</p>
<blockquote>
<p>A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P , if its performance at tasks in T , as measured by P , improves with experience E.</p>
</blockquote>
      
    </div>
    <footer>
      
        
          <div class="alignleft">
            <a href="/2017/02/23/Chapter-5-Machine-Learning-Basics/#more" class="more-link">Read More</a>
          </div>
        
        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>





<nav id="pagination">
  
    <a href="/" class="alignleft prev">Prev</a>
  
  
    <a href="/page/3/" class="alignright next">Next</a>
  
  <div class="clearfix"></div>
</nav></div></div>
    <aside id="sidebar" class="alignright">
  <div class="search">
  <form action="//google.com/search" method="get" accept-charset="utf-8">
    <input type="search" name="q" results="0" placeholder="Search">
    <input type="hidden" name="q" value="site:watsonyanghx.github.io">
  </form>
</div>

  
<div class="widget tag">
  <h3 class="title">Categories</h3>
  <ul class="entry">
  
    <li><a href="/categories/Algorithm/">Algorithm</a><small>2</small></li>
  
    <li><a href="/categories/Computer-Vision/">Computer Vision</a><small>3</small></li>
  
    <li><a href="/categories/Deep-Learning/">Deep Learning</a><small>3</small></li>
  
    <li><a href="/categories/Interview/">Interview</a><small>1</small></li>
  
    <li><a href="/categories/Machine-Learning/">Machine Learning</a><small>8</small></li>
  
    <li><a href="/categories/NLP/">NLP</a><small>1</small></li>
  
    <li><a href="/categories/OJ/">OJ</a><small>48</small></li>
  
    <li><a href="/categories/Paper/">Paper</a><small>5</small></li>
  
    <li><a href="/categories/Reinforcement-Learning/">Reinforcement Learning</a><small>1</small></li>
  
    <li><a href="/categories/Technique-Summaries/">Technique Summaries</a><small>2</small></li>
  
    <li><a href="/categories/Tools/">Tools</a><small>1</small></li>
  
    <li><a href="/categories/WhatEver/">WhatEver</a><small>1</small></li>
  
  </ul>
</div>


  
<div class="widget tagcloud">
  <h3 class="title">Tag Cloud</h3>
  <div class="entry">
    <a href="/tags/Algorithm/" style="font-size: 18.18px;">Algorithm</a> <a href="/tags/BFS/" style="font-size: 11.82px;">BFS</a> <a href="/tags/Backtracking/" style="font-size: 12.73px;">Backtracking</a> <a href="/tags/Binary-Tree/" style="font-size: 14.55px;">Binary Tree</a> <a href="/tags/C/" style="font-size: 10px;">C++</a> <a href="/tags/CNN/" style="font-size: 15.45px;">CNN</a> <a href="/tags/CS231n/" style="font-size: 11.82px;">CS231n</a> <a href="/tags/Combinatorial-number/" style="font-size: 10.91px;">Combinatorial number</a> <a href="/tags/DFS/" style="font-size: 14.55px;">DFS</a> <a href="/tags/DP/" style="font-size: 19.09px;">DP</a> <a href="/tags/Data-structure/" style="font-size: 10.91px;">Data structure</a> <a href="/tags/DeconvNet/" style="font-size: 10px;">DeconvNet</a> <a href="/tags/Deep-Learning/" style="font-size: 17.27px;">Deep Learning</a> <a href="/tags/Disjoint-Set/" style="font-size: 11.82px;">Disjoint Set</a> <a href="/tags/Divide-and-Conquer/" style="font-size: 13.64px;">Divide and Conquer</a> <a href="/tags/GAN/" style="font-size: 10.91px;">GAN</a> <a href="/tags/Graph/" style="font-size: 13.64px;">Graph</a> <a href="/tags/Greedy/" style="font-size: 11.82px;">Greedy</a> <a href="/tags/Hexo/" style="font-size: 10px;">Hexo</a> <a href="/tags/HihoCoder/" style="font-size: 16.36px;">HihoCoder</a> <a href="/tags/IPv6/" style="font-size: 10px;">IPv6</a> <a href="/tags/LintCode/" style="font-size: 20px;">LintCode</a> <a href="/tags/Machine-Learning/" style="font-size: 10.91px;">Machine Learning</a> <a href="/tags/Matrix/" style="font-size: 10px;">Matrix</a> <a href="/tags/NLP/" style="font-size: 10px;">NLP</a> <a href="/tags/Normalization/" style="font-size: 10px;">Normalization</a> <a href="/tags/Notes/" style="font-size: 10px;">Notes</a> <a href="/tags/Numpy/" style="font-size: 10px;">Numpy</a> <a href="/tags/POJ/" style="font-size: 10.91px;">POJ</a> <a href="/tags/Permutation/" style="font-size: 10px;">Permutation</a> <a href="/tags/Python/" style="font-size: 10px;">Python</a> <a href="/tags/Queue/" style="font-size: 10.91px;">Queue</a> <a href="/tags/RL/" style="font-size: 10px;">RL</a> <a href="/tags/Recursion/" style="font-size: 12.73px;">Recursion</a> <a href="/tags/ResNet/" style="font-size: 10px;">ResNet</a> <a href="/tags/Shadowsocks/" style="font-size: 10px;">Shadowsocks</a> <a href="/tags/String/" style="font-size: 11.82px;">String</a> <a href="/tags/VPS/" style="font-size: 10px;">VPS</a> <a href="/tags/WhatEver/" style="font-size: 10px;">WhatEver</a>
  </div>
</div>

</aside>
    <div class="clearfix"></div>
  </div>
  <footer id="footer" class="inner"><div class="alignleft">
  
  &copy; 2020 Watson Yang
  
</div>
<div class="clearfix"></div></footer>
  <script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>
<script src="/js/jquery.imagesloaded.min.js"></script>
<script src="/js/gallery.js"></script>


<script type="text/javascript">
var disqus_shortname = 'light-disqus';

(function(){
  var dsq = document.createElement('script');
  dsq.type = 'text/javascript';
  dsq.async = true;
  dsq.src = '//' + disqus_shortname + '.disqus.com/count.js';
  (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
}());
</script>



<link rel="stylesheet" href="/fancybox/jquery.fancybox.css" media="screen" type="text/css">
<script src="/fancybox/jquery.fancybox.pack.js"></script>
<script type="text/javascript">
(function($){
  $('.fancybox').fancybox();
})(jQuery);
</script>


<div id="totop" style="position:fixed;bottom:35px;right:20px;cursor: pointer;">
  <a title="返回顶部"><i class="fa fa-angle-double-up fa-3x" aria-hidden="true"></i></a>
</div>
<script src="/js/totop.js"></script>

</body>
</html>
